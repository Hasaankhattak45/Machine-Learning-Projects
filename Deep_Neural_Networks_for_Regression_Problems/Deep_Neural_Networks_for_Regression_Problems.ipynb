{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Neural_Networks_for_Regression_Problems.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hasaankhattak45/Machine-Learning-Projects/blob/main/Deep_Neural_Networks_for_Regression_Problems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "qtGTpd6g0r_r"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Networks for Regression problems"
      ]
    },
    {
      "metadata": {
        "id": "WJr9iTt600FD"
      },
      "cell_type": "markdown",
      "source": [
        "Neural networks are well known for classification problems, for example, they are used in handwritten digits classification, but the question is will it be fruitful if we used them for regression problems?\n",
        "\n",
        "In this article I will use a deep neural network to predict house pricing using a dataset from KaggleÂ .\n",
        "\n",
        "You can download the dataset from [Here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n"
      ]
    },
    {
      "metadata": {
        "id": "aTGH2f362N2L"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://cdn-images-1.medium.com/max/2000/1*vUKwarc7rCouMzSt0Ksakw.jpeg)"
      ]
    },
    {
      "metadata": {
        "id": "itv6SMLe6evK"
      },
      "cell_type": "markdown",
      "source": [
        "## Contents :\n",
        "    1- Process the dataset\n",
        "    2- Make the deep neural network\n",
        "    3- Train the DNN\n",
        "    4- Test the DNN\n",
        "    5- Compare the result from the DNN to another ML algorithm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d7RcVlhu920",
        "outputId": "78b013e0-51cb-493c-e7f4-9b7e8c37a5ee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HFZCb922_jfw"
      },
      "cell_type": "markdown",
      "source": [
        "**First of all, we will import the needed dependencies :**"
      ]
    },
    {
      "metadata": {
        "id": "H9QlV0BC_9PY"
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "from xgboost import XGBRegressor"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nvdfBOX87gGV"
      },
      "cell_type": "markdown",
      "source": [
        "## First : Processing the dataset \n",
        "We will not go deep in processing the dataset, all we want to do is getting the dataset ready to be fed into our models .\n",
        "\n",
        "We will get rid of any features with missing values, then we will encode the categorical features, that's it.\n"
      ]
    },
    {
      "metadata": {
        "id": "neKAXXozAmf9"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the dataset :\n",
        "* Load train and test data into pandas DataFrames\n",
        "* Combine train and test data to process them together"
      ]
    },
    {
      "metadata": {
        "id": "QeLRCvK6Au23"
      },
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    #get train data\n",
        "    train_data_path ='/content/drive/MyDrive/Machine Learning ( Projects ) /Deep Neural Networks for Regression Problems/Dataset/train.csv'\n",
        "    train = pd.read_csv(train_data_path)\n",
        "    \n",
        "    #get test data\n",
        "    test_data_path ='/content/drive/MyDrive/Machine Learning ( Projects ) /Deep Neural Networks for Regression Problems/Dataset/test.csv'\n",
        "    test = pd.read_csv(test_data_path)\n",
        "    \n",
        "    return train , test\n",
        "\n",
        "def get_combined_data():\n",
        "  #reading train data\n",
        "  train , test = get_data()\n",
        "\n",
        "  target = train.SalePrice\n",
        "  train.drop(['SalePrice'],axis = 1 , inplace = True)\n",
        "\n",
        "  combined = train.append(test)\n",
        "  combined.reset_index(inplace=True)\n",
        "  combined.drop(['index', 'Id'], inplace=True, axis=1)\n",
        "  return combined, target\n",
        "\n",
        "#Load train and test data into pandas DataFrames\n",
        "train_data, test_data = get_data()\n",
        "\n",
        "#Combine train and test data to process them together\n",
        "combined, target = get_combined_data()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LPcvLGQGjWSr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "73af62b7-2d57-41cf-f4c2-2c7f14772f91"
      },
      "cell_type": "code",
      "source": [
        "combined.describe()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2433.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2896.000000</td>\n",
              "      <td>2918.000000</td>\n",
              "      <td>2918.000000</td>\n",
              "      <td>2918.000000</td>\n",
              "      <td>2918.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2917.000000</td>\n",
              "      <td>2917.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2760.000000</td>\n",
              "      <td>2918.000000</td>\n",
              "      <td>2918.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>57.137718</td>\n",
              "      <td>69.305795</td>\n",
              "      <td>10168.114080</td>\n",
              "      <td>6.089072</td>\n",
              "      <td>5.564577</td>\n",
              "      <td>1971.312778</td>\n",
              "      <td>1984.264474</td>\n",
              "      <td>102.201312</td>\n",
              "      <td>441.423235</td>\n",
              "      <td>49.582248</td>\n",
              "      <td>560.772104</td>\n",
              "      <td>1051.777587</td>\n",
              "      <td>1159.581706</td>\n",
              "      <td>336.483727</td>\n",
              "      <td>4.694416</td>\n",
              "      <td>1500.759849</td>\n",
              "      <td>0.429894</td>\n",
              "      <td>0.061364</td>\n",
              "      <td>1.568003</td>\n",
              "      <td>0.380267</td>\n",
              "      <td>2.860226</td>\n",
              "      <td>1.044536</td>\n",
              "      <td>6.451524</td>\n",
              "      <td>0.597122</td>\n",
              "      <td>1978.113406</td>\n",
              "      <td>1.766621</td>\n",
              "      <td>472.874572</td>\n",
              "      <td>93.709832</td>\n",
              "      <td>47.486811</td>\n",
              "      <td>23.098321</td>\n",
              "      <td>2.602261</td>\n",
              "      <td>16.062350</td>\n",
              "      <td>2.251799</td>\n",
              "      <td>50.825968</td>\n",
              "      <td>6.213087</td>\n",
              "      <td>2007.792737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>42.517628</td>\n",
              "      <td>23.344905</td>\n",
              "      <td>7886.996359</td>\n",
              "      <td>1.409947</td>\n",
              "      <td>1.113131</td>\n",
              "      <td>30.291442</td>\n",
              "      <td>20.894344</td>\n",
              "      <td>179.334253</td>\n",
              "      <td>455.610826</td>\n",
              "      <td>169.205611</td>\n",
              "      <td>439.543659</td>\n",
              "      <td>440.766258</td>\n",
              "      <td>392.362079</td>\n",
              "      <td>428.701456</td>\n",
              "      <td>46.396825</td>\n",
              "      <td>506.051045</td>\n",
              "      <td>0.524736</td>\n",
              "      <td>0.245687</td>\n",
              "      <td>0.552969</td>\n",
              "      <td>0.502872</td>\n",
              "      <td>0.822693</td>\n",
              "      <td>0.214462</td>\n",
              "      <td>1.569379</td>\n",
              "      <td>0.646129</td>\n",
              "      <td>25.574285</td>\n",
              "      <td>0.761624</td>\n",
              "      <td>215.394815</td>\n",
              "      <td>126.526589</td>\n",
              "      <td>67.575493</td>\n",
              "      <td>64.244246</td>\n",
              "      <td>25.188169</td>\n",
              "      <td>56.184365</td>\n",
              "      <td>35.663946</td>\n",
              "      <td>567.402211</td>\n",
              "      <td>2.714762</td>\n",
              "      <td>1.314964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>20.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1300.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1872.000000</td>\n",
              "      <td>1950.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>334.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>334.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1895.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2006.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>20.000000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>7478.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1953.500000</td>\n",
              "      <td>1965.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>220.000000</td>\n",
              "      <td>793.000000</td>\n",
              "      <td>876.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1126.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1960.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>320.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2007.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>50.000000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>9453.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1973.000000</td>\n",
              "      <td>1993.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>368.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>467.000000</td>\n",
              "      <td>989.500000</td>\n",
              "      <td>1082.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1444.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1979.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>480.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2008.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>70.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>11570.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2001.000000</td>\n",
              "      <td>2004.000000</td>\n",
              "      <td>164.000000</td>\n",
              "      <td>733.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>805.500000</td>\n",
              "      <td>1302.000000</td>\n",
              "      <td>1387.500000</td>\n",
              "      <td>704.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1743.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2002.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>576.000000</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>2009.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>190.000000</td>\n",
              "      <td>313.000000</td>\n",
              "      <td>215245.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>2010.000000</td>\n",
              "      <td>2010.000000</td>\n",
              "      <td>1600.000000</td>\n",
              "      <td>5644.000000</td>\n",
              "      <td>1526.000000</td>\n",
              "      <td>2336.000000</td>\n",
              "      <td>6110.000000</td>\n",
              "      <td>5095.000000</td>\n",
              "      <td>2065.000000</td>\n",
              "      <td>1064.000000</td>\n",
              "      <td>5642.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2207.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1488.000000</td>\n",
              "      <td>1424.000000</td>\n",
              "      <td>742.000000</td>\n",
              "      <td>1012.000000</td>\n",
              "      <td>508.000000</td>\n",
              "      <td>576.000000</td>\n",
              "      <td>800.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>2010.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        MSSubClass  LotFrontage  ...       MoSold       YrSold\n",
              "count  2919.000000  2433.000000  ...  2919.000000  2919.000000\n",
              "mean     57.137718    69.305795  ...     6.213087  2007.792737\n",
              "std      42.517628    23.344905  ...     2.714762     1.314964\n",
              "min      20.000000    21.000000  ...     1.000000  2006.000000\n",
              "25%      20.000000    59.000000  ...     4.000000  2007.000000\n",
              "50%      50.000000    68.000000  ...     6.000000  2008.000000\n",
              "75%      70.000000    80.000000  ...     8.000000  2009.000000\n",
              "max     190.000000   313.000000  ...    12.000000  2010.000000\n",
              "\n",
              "[8 rows x 36 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "iL7U515m9BXQ"
      },
      "cell_type": "markdown",
      "source": [
        "let's define a function to get the columns that don't have any missing values "
      ]
    },
    {
      "metadata": {
        "id": "cwHRdGTD9YRD"
      },
      "cell_type": "code",
      "source": [
        "def get_cols_with_no_nans(df,col_type):\n",
        "    '''\n",
        "    Arguments :\n",
        "    df : The dataframe to process\n",
        "    col_type : \n",
        "          num : to only get numerical columns with no nans\n",
        "          no_num : to only get nun-numerical columns with no nans\n",
        "          all : to get any columns with no nans    \n",
        "    '''\n",
        "    if (col_type == 'num'):\n",
        "        predictors = df.select_dtypes(exclude=['object'])\n",
        "    elif (col_type == 'no_num'):\n",
        "        predictors = df.select_dtypes(include=['object'])\n",
        "    elif (col_type == 'all'):\n",
        "        predictors = df\n",
        "    else :\n",
        "        print('Error : choose a type (num, no_num, all)')\n",
        "        return 0\n",
        "    cols_with_no_nans = []\n",
        "    for col in predictors.columns:\n",
        "        if not df[col].isnull().any():\n",
        "            cols_with_no_nans.append(col)\n",
        "    return cols_with_no_nans"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BkdPBeu1n2Se"
      },
      "cell_type": "markdown",
      "source": [
        "Get the columns that do not have any missing values ."
      ]
    },
    {
      "metadata": {
        "id": "MdH1rGZFjNL-"
      },
      "cell_type": "code",
      "source": [
        "num_cols = get_cols_with_no_nans(combined , 'num')\n",
        "cat_cols = get_cols_with_no_nans(combined , 'no_num')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Frf_IY8Onvqm"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see how many columns we got"
      ]
    },
    {
      "metadata": {
        "id": "_X295Nvfl5ZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "883d29d9-558d-45ab-ec45-20880d54e526"
      },
      "cell_type": "code",
      "source": [
        "print ('Number of numerical columns with no nan values :',len(num_cols))\n",
        "print ('Number of nun-numerical columns with no nan values :',len(cat_cols))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of numerical columns with no nan values : 25\n",
            "Number of nun-numerical columns with no nan values : 20\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "11f-e_7dmhRq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "7d25b905-ad48-4820-d0e6-2e1a54427e72"
      },
      "cell_type": "code",
      "source": [
        "combined = combined[num_cols + cat_cols]\n",
        "combined.hist(figsize = (12,10))\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x720 with 25 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "N793AWuKeS7v"
      },
      "cell_type": "markdown",
      "source": [
        "**The correlation between the features**"
      ]
    },
    {
      "metadata": {
        "id": "EJ_lUgcCodbD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "outputId": "de2f7f5c-628c-417d-a499-3b3cefc63e4b"
      },
      "cell_type": "code",
      "source": [
        "train_data = train_data[num_cols + cat_cols]\n",
        "train_data['Target'] = target\n",
        "\n",
        "C_mat = train_data.corr()\n",
        "fig = plt.figure(figsize = (15,15))\n",
        "\n",
        "sb.heatmap(C_mat, vmax = .8, square = True)\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAANLCAYAAAAAV3+5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdebhdZX33//cnYQgQBpknMdYrioohQB4cUXDog62IFlFxjEPR1qFPfxXhUauoT60+1fZRcWiwGm2tqDiQqoCKRNTKEKYEUEQhaJCCDFrGSHK+vz/2OrI9niTn7JO9OCt5v65rX2ete93r/t575yQ53/O911qpKiRJkiRJ09+MB3oCkiRJkqSJMYGTJEmSpI4wgZMkSZKkjjCBkyRJkqSOMIGTJEmSpI4wgZMkSZKkjjCBkyRJkqQBJDkyydVJfprkpHGO75fk3CSXJlme5E+mHNPnwEmSJEnS5CSZCfwEeAawCrgIOK6qrurrswi4tKo+luRRwDeqas5U4lqBkyRJkqTJOxT4aVVdW1W/BU4Djh7Tp4Admu0dgV9ONegWUx1AkiRJkjZD+wC/6NtfBTx2TJ+TgW8meQOwHfD0qQY1gZMkSZLUqvtuuXbaX8e11W4Pew1wfF/ToqpaNMlhjgMWV9UHkjwe+NckB1TVyKDzMoGTJEmSpDGaZG19CdsNwIP79vdt2vq9CjiyGe+HSWYBuwI3Dzovr4GTJEmSpMm7CJib5KFJtgJeCCwZ0+fnwNMAkjwSmAX8aipBrcBJkiRJatfI2gd6BlNWVWuSvB44G5gJfLKqrkzyLmBZVS0B/gY4Nclf07uhycKa4mMAfIyAJEmSpFbdd/M10z4J2XL3uXmg5zAel1BKkiRJUke4hFKSJElSuwa/CeNmzwqcJEmSJHWECZwkSZIkdYQJnCRJkiR1hNfASZIkSWrXiNfADcoKnCRJkiR1hAmcJEmSJHWESyglSZIktap8jMDArMBJkiRJUkeYwEmSJElSR7iEUpIkSVK7vAvlwKzASZIkSVJHmMBJkiRJUke4hFKSJElSu7wL5cCswEmSJElSR5jASZIkSVJHuIRSkiRJUrtG1j7QM+gsK3CSJEmS1BEmcJIkSZLUESZwkiRJktQRXgMnSZIkqV0+RmBgVuAkSZIkqSNM4CRJkiSpI1xCKUmSJKldIy6hHJQVOEmSJEnqCBM4SZIkSeoIl1BKkiRJalV5F8qBWYGTJEmSpI4wgZMkSZKkjnAJpSRJkqR2eRfKgVmBkyRJkqSOMIGTJEmSpI5wCaUkSZKkdnkXyoFZgZMkSZKkjjCBkyRJkqSOcAmlJEmSpHaNrH2gZ9BZVuAkSZIkqSNM4CRJkiSpI0zgJEmSJKkjvAZOkiRJUrt8jMDArMBJkiRJUkeYwEmSJElSR7iEUpIkSVK7RlxCOSgrcJIkSZLUESZwkiRJktQRLqGUJEmS1C7vQjkwK3CSJEmS1BEmcJIkSZLUES6hlCRJktQu70I5MCtwkiRJktQRJnCSJEmS1BEuoZQkSZLUqqq1D/QUOssKnCRJkiR1hAmcJEmSJHWECZwkSZIkdYTXwEmSJElqV/kYgUFZgZMkSZKkjjCBkyRJkqSOcAmlJEmSpHaNuIRyUFbgJEmSJKkjTOAkSZIkqSNcQilJkiSpXd6FcmBW4CRJkiSpI0zgJEmSJKkjXEIpSZIkqV0jax/oGXSWFThJkiRJ6ggrcC2575Zra9gxnn/wXw07BABPYsdW4lw3476hx5gzsuXQYwC0EwVOXf3TVuKc/UfbDj3Gl1ftPfQYANsO/W9mz65r2rlY+0EM/+8NwJmzhv9d/ZqdfjX0GABn3LZHK3FWzljTSpztWvjd7BaVoccAWJN2/oLeTTt/P7dk+J/bDtXO7+ZXt/MtwAjtfA+0UQv6Fb9tIUrPJ1ae3tKfkB4IJnCSpElpI3mTJG3ivAvlwFxCKUmSJEkdYQInSZIkSR3hEkpJkiRJ7RpxCeWgrMBJkiRJUkeYwEmSJElSR5jASZIkSVJHeA2cJEmSpHb5GIGBDb0Cl6SS/Fvf/hZJfpXka83+Hkm+luTyJFcl+UbTPiPJh5JckWRFkouSPHQDsRYned46jh2a5LwkVye5NMknkmybZGGSUzbme5YkSZKkYWijAncXcECSbarqHuAZwA19x98FfKuqPgiQZF7T/gJgb2BeVY0k2bcZa9KS7AF8EXhhVf2waXsesP0g40mSJEnSA6Gta+C+Afxps30c8Lm+Y3sBq0Z3qmp5X/uNVb36alWtqqrbAZLcOdo/yfOSLO4b7+lJliX5SZJnNW2vAz49mrw1451eVTf1TzLJUUkuaCp0324SP5I8JcllzevSJNsn2aup6F3WVAkPG/jTkSRJkjYnIyPT/zVNtZXAnQa8MMksYB5wQd+xjwD/kuTcJG9NsnfT/gXgqCZB+kCSgyYYaw5wKL2E8eNNzAOAiydw7veBx1XVQc2c39y0vwl4XVXNBw4D7gFeBJzdtB0IXDbB+UmSJEnSQFpJ4Jqq2hx61bdvjDl2NvBHwKnA/sClSXarqlXAI4D/DYwA5yR52gTCfaGqRqrqGuDaZsyJ2hc4O8kK4ATg0U37D4B/TPJGYKeqWgNcBLwiycnAY6rqjrGDJTm+qQYu+8RnPjf2sCRJkiRNSpuPEVgCvJ/fXz4JQFXdVlX/XlUvpZcYPblpX11VZ1bVCcB7gOeMntJ3+qyxw42zfyVwyATm+GHglKp6DPCa0bGr6r3Aq4FtgB8k2b+qzmvmeQOwOMnLxnlfi6pqQVUtePXLjptAeEmSJGkz8EAvj3QJ5YR8EnhnVa3ob0zy1CTbNtvbAw8Dfp7k4NHllElm0Ft6eX1z2k1JHtm0P3dMnGObO1g+jF5l72rgFODlSR7bF/fPRq9x67Mj999g5eV9fR9WVSuq6n30Esz9kzwEuKmqTgU+ARw8yIciSZIkSRPV2nPgmiWRHxrn0CHAKUnW0EsoP1FVFyU5Ejg1ydZNvwvpJWIAJwFfA34FLANm943386bvDsBrq+pe4N4kLwTen2R3eksyzwPOGjOXk4EvJrkd+A4w+tiC/5XkiOa8K4EzgRcCJyS5D7gT+IMKnCRJkiRtTENP4Kpq9jhtS4GlzfY/AP8wTp+z+MMEa/TY6cDp47QvXM88fkjvBiRjLW5eVNUZwBnjnPuGcc77dPOSJEmSNAlVax/oKXRWm0soJUmSJElTYAInSZIkSR3R2jVwkiRJkgRM67s8TndW4CRJkiSpI0zgJEmSJKkjTOAkSZIkqSO8Bk6SJElSu8pr4AZlBU6SJEmSOsIKXEuef/BfDT3GFy754NBjAHz5MX/bSpw7thr+t+d9GXqIXpx2wnDWnG1bifOVVXsPPcavZ9TQYwA8c7tbWolzxW27tBLnB1tvPfQYOxQ8YvXwf3N66q93G3oMgD+9755W4hy16x2txPn5zTsOPcbKLYb/fQZwzZbt/Duwe7Xz49A2LbydHVp6NvKvZ7YT546W/p/+bYb/h3PStu38G6BNnwmcJGlS2kjeJEmbOB8jMDCXUEqSJElSR5jASZIkSVJHuIRSkiRJUru8C+XArMBJkiRJUkeYwEmSJElSR7iEUpIkSVK7vAvlwKzASZIkSVJHdDaBS3LnJPouTLL3mLZdk9yX5LUbf3aSJEmStPF1NoGbpIXA3mPajgXOB45b10lJZg5xTpIkSdLmqUam/2ua2qQSuCTzk5yfZHmSryR5UJLnAQuAzya5LMk2TffjgL8B9kmyb98Ydyb5QJLLgccneUmSC5tz/3k0qUvysSTLklyZ5J1tv1dJkiRJm59NKoEDPgOcWFXzgBXAO6rqdGAZ8OKqml9V9yR5MLBXVV0IfAF4Qd8Y2wEXVNWBwK3NsSdW1XxgLfDipt9bq2oBMA94SpJ5bbxBSZIkSZuvTSaBS7IjsFNVfbdp+jTw5HV0fwG9xA3gNH5/GeVa4EvN9tOAQ4CLklzW7P9Rc+z5SS4BLgUeDTxqnDkd31Tplq288/rB3pgkSZIkNTbXxwgcB+yZZLSatneSuVV1DXBvVa1t2gN8uqr+d//JSR4KvAn4H1V1e5LFwKyxQapqEbAI4Ln7HVXDeSuSJElSx/gYgYFtMhW4qvoNcHuSw5qmlwKj1bg7gO0BkjwcmF1V+1TVnKqaA/w949/M5BzgeUl2b87dOclDgB2Au4DfJNkDeOaQ3pYkSZIk/U6XK3DbJlnVt/+PwMuBjyfZFrgWeEVzbHHTfg/wlebV70vA54F39TdW1VVJ3gZ8M8kM4D7gdVV1fpJLgR8DvwB+sFHfmSRJkiSNo7MJXFWtq3r4uHH6fon7r2sbb6zlwCOb7dljjn2eXnI39pyFk5iuJEmSpFEuoRzYJrOEUpIkSZI2dSZwkiRJktQRnV1CKUmSJKmjyiWUg7ICJ0mSJEkdYQInSZIkSR3hEkpJkiRJ7fIulAOzAidJkiRJHWEFriVPYsehx/jyY/526DEA/mzFu1uJc/yCE1qJ89F3zh16jFq9eugxAGrVmlbiHH3mja3E+fCvdh96jH3e8YShxwDY+ze/biXOvA9f2UqcC27dbegx/nPNzUOPAXDSc9v5r3DkrrWtxLn3+3cPPcbD97hl6DEAzr5h+P93Aiz4/UfADs2sFgoOv83wYwDcOKOd7+e5a2a2EuenWwz/D+chS08ZegxtHkzgtFlrI3nTYNpI3jSYNpI3SdImzrtQDswllJIkSZLUESZwkiRJktQRLqGUJEmS1C7vQjkwK3CSJEmS1BEmcJIkSZLUESZwkiRJktQRXgMnSZIkqV0+RmBgrVXgkuyb5Iwk1yT5WZIPJtlqyDHvbL7OSXJFX/uTklyY5MdJrk7ylxsjjiRJkiQNUysJXJIAXwa+WlVzgYcDs4G/m+K4k64gJtkT+HfgtVW1P/BE4FVJnjuVuUiSJEnSsLVVgXsqcG9VfQqgqtYCfw28sqmEPXq0Y5KlSRYk2S7JJ5vjlyY5ujm+MMmSJN8BzkkyO8k5SS5JsmK033q8DlhcVZc0c7kFeDNwQjP+4iTP65vPaBVvsnEkSZIkjWdkZPq/JiDJkc2Kvp8mOWkdfZ6f5KokVyb596l+dG1dA/do4OL+hqr67yQ/B74OPB94R5K9gL2qalmS9wDfqapXJtkJuDDJt5vTDwbmVdVtTRXuuc14uwLnJ1lSVbWeuXx6TNsy4FEbeA/3TjKOJEmSpE1UkpnAR4BnAKuAi5r84Kq+PnOB/w08sapuT7L7VONOh7tQLgVGK17PB05vtv8YOCnJZU2fWcB+zbFvVdVtzXaA9yRZDnwb2AfYYwjzbCuOJEmSpOnvUOCnVXVtVf0WOA0Yu0rvz4GPVNXtAFV181SDtpXAXQUc0t+QZAd6CdlFwK1J5gEvAD4/2gU4pqrmN6/9qupHzbG7+oZ6MbAbcEhVzQduopfsTXguzf6yZnsNzeeSZAYweqOVycYhyfFJliVZdv6d16yvqyRJkrT5eKCXR07g1f+zfPM6fsy72Af4Rd/+qqat38OBhyf5QZLzkxw51Y+urQTuHGDbJC+D35UbP0DvWrS76SVtbwZ2rKrlzTlnA29oboBCkoPWMfaOwM1VdV+SI4CHbGAuHwEWJpnfjLsLvZupvLs5vpL7E7xnA1sOGIeqWlRVC6pqweNmz91Qd0mSJEnTRP/P8s1r0QDDbAHMBQ4HjgNObS4PG1grCVxzndhzgWOTXAP8hN41ZW9pupwOvBD4Qt9p76aXPC1PciX3J1hjfRZYkGQF8DLgxxuYy43AS4BFSa4Gfgl8qKq+23Q5FXhKksuBx3N/tW9ScSRJkiRt0m4AHty3v2/T1m8VsKSq7quq6+jlQVOq7LT2IO+q+gVw1DqO3TR2LlV1D/CacfouBhb37d9CL9Eab9zZzdeVwAF97efRW7NK8wy4tyQ5q6pub+byuL5hTpxoHEmSJEkTsGncB/AiYG6Sh9JL3F4IvGhMn6/Sq7x9qrkR4sOBa6cSdDrcxOQBVVUfrarHjF5YKEmSJEkbUlVrgNfTu/TrR8AXqurKJO9K8uym29n07vdxFXAucEJV3TqVuK1V4CRJkiRpU1JV3wC+Mabt7X3bBfx/zWujMIGTJEmS1K4JPihbf2izX0IpSZIkSV1hAidJkiRJHWECJ0mSJEkd4TVwkiRJktrlNXADswInSZIkSR2R2jQeojftvX7OC4b+Qe9a7RRUr8+9rcRZtOwfhh5ju32ePPQYAB/b7fBW4iyZ8etW4hzGTq3EacPVM1a3E+e+21qJM2/LXVuJs09tOfQY17T0b81sZrYSZ7uW4mxdGXqMu9POb87nrmnnM/tNS7/O3raFH7lumdHOz3V7rR3+9xnADTPbeT8PauHvzRUt/X8DcOrKL7bzBzQF93z2b6d9ErLNi989LT9Hl1BKkialjeRNkrSJK5dQDsollJIkSZLUESZwkiRJktQRLqGUJEmS1C7vQjkwK3CSJEmS1BEmcJIkSZLUES6hlCRJktQuH2U2MCtwkiRJktQRQ03gkuyb5Iwk1yT5WZIPJtlqyDHvbL7OSXJFX/uhSc5LcnWSS5N8Ism2GyHeyUneNNVxJEmSJGlDhpbAJQnwZeCrVTUXeDgwG/i7KY476WWfSfYAvgicWFWPqKqDgLOA7acyF0mSJEkDGBmZ/q9papgVuKcC91bVpwCqai3w18Ark1yY5NGjHZMsTbIgyXZJPtkcvzTJ0c3xhUmWJPkOcE6S2UnOSXJJkhWj/dbjdcCnq+qHow1VdXpV3ZRk5yRfTbI8yflJ5jUxT27msjTJtUne2Dfftyb5SZLvA4/YSJ+XJEmSJK3XMG9i8mjg4v6GqvrvJD8Hvg48H3hHkr2AvapqWZL3AN+pqlcm2Qm4MMm3m9MPBuZV1W1NFe65zXi7AucnWVK1zqshDwA+vY5j7wQurarnJHkq8BlgfnNsf+AIepW6q5N8DJgHvLDpswVwydj3KUmSJEnD8EDdhXIp8FHgHfQSudOb9j8Gnt13TdksYL9m+1tVdVuzHeA9SZ4MjAD7AHsA/zXAXJ4EHANQVd9JskuSHZpjX6+q1cDqJDc3MQ4DvlJVdwMkWTJATEmSJGnzNY2XKE53w1xCeRVwSH9DkxjtB1wE3NosV3wB8PnRLsAxVTW/ee1XVT9qjt3VN9SLgd2AQ6pqPnATvWRvXa4cO5cJWt23vZZJJrxJjk+yLMmyK+/42QDhJUmSJOl+w0zgzgG2TfIygCQzgQ8Ai5vq1eeBNwM7VtXy5pyzgTc0N0AhyUHrGHtH4Oaqui/JEcBDNjCXU4CXJ3nsaEOSP2tubvI9egkhSQ4Hbqmq/17PWOcBz0myTZLtgaPW1bGqFlXVgqpa8OjtH7aBKUqSJEnS+g0tgWuuR3sucGySa4CfAPcCb2m6nE7vWrIv9J32bmBLYHmSK5v98XwWWJBkBfAy4McbmMtNTaz3N48R+BHwP4E7gJOBQ5IsB94LvHwDY11CL/m8HDiTXjVRkiRJkoZuqNfAVdUvWEeFqkmqthjTdg/wmnH6LgYW9+3fAjx+HePObr6upHfzktH2H9K7fm2su4HnjDPOyWP2+8f6O6b4OARJkiRps1VeAzeooT7IW5IkSZK08ZjASZIkSVJHPFCPEZAkSZK0maqRdT2+WRtiBU6SJEmSOsIETpIkSZI6wiWUkiRJkto14l0oB2UFTpIkSZI6wgROkiRJkjrCJZQtmTOy5dBj3JehhwDgo++c20qc7fZ58tBj3HXDeUOPAbDmnH9tJc7T/v77rcT57O3Dv3PUt9fePPQYAO9ds30rcQ54xc6txLn+tDtbifPhtdsNPcbP194x9BgAr1i7Wytxdlm7ppU4e866e+gx7l49/P/TAJ586/mtxHnfnke0EufqmcP/Hti1Zg49BsA1W6xtJc6DWno/v5ox/OV8H/xf7fw/0Bk+yHtgVuAkSZPSRvImSZLGZwInSZIkSR3hEkpJkiRJ7fJB3gOzAidJkiRJHWECJ0mSJEkdYQInSZIkSR3hNXCSJEmS2jXiYwQGZQVOkiRJkjpiWidw6fl+kmf2tR2b5Kwpjrs2yWVJLk9ySZInTOCcTyR5VLO9MsmuSXZK8pdTmYskSZIkTdS0XkJZVZXktcAXk5xLb77vAY4cZLwkW1TVGuCeqprftP1P4O+Bp2xgLq8ep3kn4C+Bjw4yH0mSJGmz5BLKgU3rChxAVV0B/AdwIvB24N+Atya5MMmlSY4GSDInyfeaitrvqmpJDm/alwBXjRNiB+D2vr5fGz2Q5JQkC5vtpUkWjDn3vcDDmmreP2zUNy5JkiRJY0zrClyfdwKXAL8FvgZ8p6pemWQn4MIk3wZuBp5RVfcmmQt8DhhNuA4GDqiq65r9bZJcBswC9gKeOuC8TmrGnT/g+ZIkSZI0YZ1I4KrqriSfB+4Eng8cleRNzeFZwH7AL4FTkswH1gIP7xviwr7kDX5/CeXjgc8kOWBjzzvJ8cDxAMc86FAeN3vuxg4hSZIkdU/VAz2DzupEAtcYaV4Bjqmqq/sPJjkZuAk4kN7S0Hv7Dt+1rkGr6odJdgV2A9bw+8tKZ01lwlW1CFgE8P79XuJ3qSRJkqQpmfbXwI3jbOANSQKQ5KCmfUfgxqoaAV4KzJzIYEn2b/reClwPPCrJ1s3yzKdt4PQ7gO0n/xYkSZIkafK6VIEb9W7g/wHLk8wArgOeRe9OkF9K8jLgLNZTdeP+a+CgV9F7eVWtBX6R5AvAFc24l65vIlV1a5IfJLkCOLOqTpjKG5MkSZI2C96FcmCdSeCq6uS+3deMc/waYF5f04lN+1Jg6Zi+66zOVdWbgTeP03543/acvu0XrX/mkiRJkrRxdHEJpSRJkiRtljpTgZMkSZK0iRjx/n6DsgInSZIkSR1hAidJkiRJHWECJ0mSJEkd4TVwkiRJktpVPkZgUFbgJEmSJKkjTOAkSZIkqSNcQtmSLVuIcV8LMQBq9epW4nxst8OHHmPNOf869BgAWzztpa3EufstF7YSZ/69w1/2cOM2Ow89BsCeO9/aSpyRu7ZpJc7tLcR5CSN8adbw//vYJm38ywkPrXtaiXPDjFmtxNllzcyhx5i99W+HHgPgCbvt30qcrVq6m/mWpJ1ALZjVUg1g+N/NPbuMtPB+Vrfz96YzfIzAwKzASZImpY3kTZIkjc8ETpIkSZI6wl+jSpIkSWpVjXgXykFZgZMkSZKkjjCBkyRJkqSOcAmlJEmSpHZ5F8qBWYGTJEmSpI4wgZMkSZKkjthgApee7yd5Zl/bsUnOmkrgJGuTXJbkiiT/kWSnqYw3ydgLk5wypu2yJKet55zDk3xtHcdWJtl1Y89TkiRJ2iTVyPR/TVMbTOCqqoDXAv+YZFaS2cB7gNcNEjDJ6HV391TV/Ko6ALht0PE2hiSPBGYChyXZ7oGahyRJkiStz4SWUFbVFcB/ACcCbwf+DXhrkguTXJrkaIAkc5J8L8klzesJTfvhTfsS4KpxQvwQ2Kfp+7AkZyW5uDln/6Z9cZKPJTk/ybXNmJ9M8qMki0cHSnJckhVNZe99fe2vSPKTJBcCTxwT/zjgX4FvAkf3nXNkkh8nuQT4s772XZJ8M8mVST4BZCKfoyRJkiRNxWTuQvlO4BLgt8DXgO9U1SubpY8XJvk2cDPwjKq6N8lc4HPAgub8g4EDquq6/kGTzASeBvxL07QIeG1VXZPkscBHgac2xx4EPB54NrCEXiL2auCiJPOb+O8DDgFuB76Z5DnABc38DwF+A5wLXNo3jRcAzwD2B94A/HuSWcCpTeyfAp/v6/8O4PtV9a4kfwq8ahKfoyRJkrR58y6UA5twAldVdyX5PHAn8HzgqCRvag7PAvYDfgmc0iRTa4GH9w1x4ZjkbZskl9GrvP0I+FazPPMJwBeT3xW1tu475z+qqpKsAG6qqhUASa4E5gAPAZZW1a+a9s8CT27O7W///OjckiwAbqmqnye5Afhkkp2b93NdVV3T9Ps34PhmrCfTVOSq6utJbh/vM0ty/Og5z3/QoTxh9tx1fLqSJEmStGGTfQ7cSPMKcExVXd1/MMnJwE3AgfSWZ97bd/iuMWPdU1Xzk2wLnE3vGrjFwK+rav464q/um8fqvvaR5r3cN8n3A73lk/snWdns7wAcA1w0wFi/p6oW0aso8sH9XuKvGSRJkiRNyaCPETgbeEOaMlmSg5r2HYEbq2oEeCm9G4OsV1XdDbwR+BvgbuC6JMc24ybJgZOY14XAU5Ls2izNPA74Lr0llE9prl3bEhgdfwa9auJjqmpOVc2hdw3cccCPgTlJHtaMfVxfnPOAFzVjPJPe0k5JkiRJGqrJVuBGvRv4f8DyJgm6DngWvevVvpTkZcBZ/GHVbVxVdWmS5fSSpBcDH0vyNmBL4DTg8gmOc2OSk+hd4xbg61V1BvyuOvhD4NfAZc0phwE3VNUv+4Y5D3gUvaTseODrSe4Gvgds3/R5J/C5ZunmfwI/n8j8JEmSJAEj0/c2/dPdpBK4qjq5b/c14xy/BpjX13Ri074UWDqm7+wx+0f17R45ztgL+7ZXAges49jn6N08Zez5nwI+NbYdeNyYfmuBPZvdG+nd2GTsWLcCfzzOWJIkSZI0NIMuoZQkSZIktWzQJZSSJEmSNBgfIzAwK3CSJEmS1BEmcJIkSZLUES6hlCRJktSu8i6Ug7ICJ0mSJEkdYQInSZIkSR3hEsqWnLr6p0OPcdacbYceA6BWrWklzpIZdw4/xgkX8cGdVw89zt1vuXDoMQDmXvDhVuL8+YGvHn6Q++CILffccL8p2v0NBw89BsDqsy5qJc6Htp45/CC1huNWD//fm0dusdPQYwAc8Cc3tRNnxh2txPn5OVsNPcZv72vh+ww4YMtdWokzc20rYZjXwue249p2lqWdP6uVMMystBLn1hnD/9wyd+7QY3SKd6EcmBU4bdbaSN40mDaSNw2mjeRNkiSNzwROkiRJkjrCJZSSJEmSWlUj3oVyUFbgJEmSJKkjTOAkSZIkqSNM4CRJkiSpI7wGTpIkSVK7fIzAwKzASZIkSVJHdC6BS/LJJDcnuWID/Q5P8oS+/ZOT3JDksub13qZ9aZIF6xjjWUkuTXJ5kquSvOdHyFgAACAASURBVGZ9Y0mSJEnSMHVxCeVi4BTgMxvodzhwJ/CffW3/VFXvn0iQJFsDi4BDq2pVsz9nkLEkSZIk9XEJ5cA6V4GrqvOA2/rbkryxqZAtT3JakjnAa4G/bipkh01k7CR3JvlAksuBx9JLcG9t4q6uqqs35nuRJEmSpMnoXAK3DicBB1XVPOC1VbUS+Di9Ktn8qvpe0++v+5Y9/s9xxtkOuKCqDmwSxSXA9Uk+l+TFSfo/rw2NJUmSJEkbVReXUI5nOfDZJF8Fvrqefhta9rgW+NLoTlW9OsljgKcDbwKeASyc4FiSJEmSxlMjD/QMOmtTqcD9KfAR4GDgoiSDJqb3VtXa/oaqWlFV/0QveTtmMoMlOT7JsiTLbrvn5gGnJEmSJEk9nU/gmmWND66qc4ETgR2B2cAdwPZTGHd2ksP7muYD109mjKpaVFULqmrBztvsPuhUJEmSJAno4BLKJJ+jd4fJXZOsAt4NvDTJjkCAD1XVr5P8B3B6kqOBNwwSCnhzkn8G7gHu4v7lk5IkSZIG5V0oB9a5BK6qjhun+Z/H6fcTYF5f0/fG9mn6Hd63Pbtv+w7gT9ZxzskTm60kSZIkbTydX0IpSZIkSZuLzlXgJEmSJHVbuYRyYFbgJEmSJKkjTOAkSZIkqSNcQilJkiSpXS6hHJgVOEmSJEnqCBM4SZIkSeoIEzhJkiRJ6givgWvJ2X+07dBjfGXV3kOPAXD0mTe2Eucw9hp6jM/e3s766/n3jrQS588PfHUrcZZe/omhx/jk/LcPPQZAHnlIK3FmXrC8lTiL/+nQVuKMXH310GP860fXDj0GwJbPfkYrcS7/i/NbifNThv//zf4z7hx6DIB5a7dqJc71M9v5N3rPDP/35iMt/W5+h5YuX2qr0nAfw39Dv/l/3xx6jFHb/NlbWos1sJF2/t5tiqzASZImpY3kTZIkjc8ETpIkSZI6wiWUkiRJktrlYwQGZgVOkiRJkjrCBE6SJEmSOsIllJIkSZLa5RLKgVmBkyRJkqSOMIGTJEmSpI6Y9glckgcnOTfJVUmuTPJXkzx/aZIFzfbKJCuSXNa8npBkTpIr1nHujCQfSnJFc95FSR66rrGm/m4lSZKkTV9VTfvXdNWFa+DWAH9TVZck2R64OMm3quqqAcc7oqpuGd1JMme8Tkm2AI4F9gbmVdVIkn2Bu9Y1liRJkiQN07RP4KrqRuDGZvuOJD8C9knyUeAC4AhgJ+BVVfW9JNsAnwIOBH4MbDPRWEkWAn8GzAZmAmcAN1bVSBN/1cZ6X5IkSZI0WdM+gevXVMsOope4AWxRVYcm+RPgHcDTgb8A7q6qRyaZB1wyZphzk6wFVlfVY8cJczC9itttTcXt+0kOA84B/q2qLp3EWJIkSZLG8i6UA+tMApdkNvAl4H9V1X8nAfhyc/hiYE6z/WTgQwBVtTzJ8jFDbWjZ47eq6rbm/FVJHgE8tXmdk+TYqjpngmNJkiRJ0kYz7W9iApBkS3rJ22er6st9h1Y3X9ey8ZLR/mvcqKrVVXVmVZ0AvAd4zkQHSnJ8kmVJlv3bTb/cSNOTJEmStLma9glceqW2fwF+VFX/OIFTzgNe1Jx7ADBvCrEPTrJ3sz2jGev6iZ5fVYuqakFVLXjJHnsPOg1JkiRJArqxhPKJwEuBFUkua9resp7+HwM+1dzs5Ef0llcOanfg1CRbN/sXAqdMYTxJkiRJXgM3sGmfwFXV94GMc+gbfX1uobkGrqruAV64jrHmjNO2Ejig2V4MLO47dhZw1kTHkiRJkqRhmvZLKCVJkiRJPSZwkiRJklpVIzXtXxOR5MgkVyf5aZKT1tPvmCSVZMFUPzsTOEmSJEmapCQzgY8AzwQeBRyX5FHj9Nse+Cvuf5b1lJjASZIkSdLkHQr8tKqurarfAqcBR4/T793A+4B7N0ZQEzhJkiRJ7Rqpaf/qf6Zz8zp+zLvYB/hF3/6qpu13khwMPLiqvr6xPrppfxdKSZIkSWpbVS0CFg16fvMc6X8EFm6sOYEVOEmSJEkaxA3Ag/v2923aRm1P73FlS5OsBB4HLJnqjUyswEmSJElq18gDPYGN4iJgbpKH0kvcXgi8aPRgVf0G2HV0P8lS4E1VtWwqQU3gWvLlVXsPPcavZ7TzRPsP/2r3VuK0EeXba29uIQrcuM3OrcQ5gj1bifPJ+W8feoxXXvauoccAOOuAt7YS59Ktd2klzl1nrWglzsGrh7+A49athh4CgG++5rJW4pw/a1Yrcdr4qWhVzR56DICdWvoBb+tKK3HuayHGvS2trVpNOz9z/HdLP9u04Z9+vldrsd7XWqTNW1WtSfJ64GxgJvDJqroyybuAZVW1ZBhxTeAkSZPSRvImSVIXVNU3gG+MaRv3N91VdfjGiGkCJ0mSJKlVE31Qtv6Qv0aVJEmSpI4wgZMkSZKkjjCBkyRJkqSO8Bo4SZIkSe3yGriBWYGTJEmSpI4wgZMkSZKkjphSApfkzo01kWa845P8uHktS3L4FMY6PMnXmu2FSX6V5LLm9Zkkz05y0gbGmJHkQ0muSLIiyUXNk9ZJsrJpGx3zCYPOVZIkSdqsjHTgNU1Nm2vgkjwLeA3wpKq6JcnBwJIkj62qGzZCiM9X1evHtG3o6egvAPYG5lXVSJJ9gbv6jh9RVbdshLlJkiRJ0gZt9CWUSeYnOT/J8iRfSfKgJLsnubg5fmCSSrJfs/+zJNsCJwInjCZEVXUJ8CngdU2/lUl2bbYXJFnabB+a5IdJLk3yn0keMcF5LkxySrO9uKm0/WeSa5M8r+m2F3BjVY00c1pVVbdvlA9KkiRJkiZpGNfAfQY4sarmASuAd1TVzcCsJDsAhwHLgMOSPAS4uaruBh4NXDxmrGXAozYQ78fAYVV1EPB24D3r6PeCvuWOrxjn+F7Ak4BnAe9t2r4AHNWc84EkB40559zm2AUbmKMkSZKkRo3UtH9NVxt1CWWSHYGdquq7TdOngS822/8JPBF4Mr0k60ggwPemGHZH4NNJ5gIFbLmOfr+3hDLJwjHHv9pU2q5Ksgf0Km5NRe+pzeucJMdW1TnNOetdQpnkeOB4gBfsdChPnD138u9OkiRJkhpt3oXyPHrVt4cAZwAH0qt4jSZwVwGHjDnnEHpVOIA13D/fWX193g2cW1UHAEeNOTYZq/u2M7pRVaur6syqOoFe4vmciQ5YVYuqakFVLTB5kyRJkjRVGzWBq6rfALcnOaxpeikwWo37HvAS4Jqm0nUb8CfA95vj/xd4X5JdoHctHfBc4J+b4yu5P8E7pi/sjsDoTU4WbsS3Q5KDk+zdbM8A5gHXb8wYkiRJ0mbngb7D5GZ8F8ptk6zq2/9H4OXAx5sbk1wLvAKgqlYmCb1KHPQSt31HbwpSVUuaZOkHSbYA9gQOrKpfNf3fCfxLkncDS/ti/l96SyjfBnx9iu9nrN2BU5Ns3exfCJyykWNIkiRJ0oRMKYGrqnVV8B63jv4P7tt+D2NuOFJVH6eX/G1B7w6U70rykur5HvDwccb84Zj2tzXtS2kSvapaDCwec97v2qpq4Zhjs5uvZwFnreO9zBmvXZIkSZKGZdo8B65fVa2ht/xSkiRJ0iZmOt/lcbpr8yYmkiRJkqQpMIGTJEmSpI6YlksoJUmSJG3CpvFdHqc7K3CSJEmS1BEmcJIkSZLUESZwkiRJktQRXgMnSZIkqVXlNXADM4FrybYtPOrimdvdMvwgwD7veEIrcd544hVDj/HeNdsPPQbAnjvf2kqc3d9wcCtx8shDhh7jrAPeOvQYAEde8XetxHn6BUtaiXPfkjNbibPFI/Ybeowz3n/30GMA/MkV/6eVOE/7wAmtxPnF6fcMPcbO+9419BgAn7h231bizCKtxNlnzfB/GNhr7W+HHgPgv2Zt1Uqcfda2s1hs5cy1Q4/xf7764qHH0ObBJZSSpElpI3mTJEnjswInSZIkqV0uoRyYFThJkiRJ6ggTOEmSJEnqCJdQSpIkSWqVd6EcnBU4SZIkSeoIEzhJkiRJ6giXUEqSJElql0soBzatKnBJ9kjy70muTXJxkh8mee44/eYk+YOnPCd5V5KnTyDO/CSV5MiNNXdJkiRJGrZpk8AlCfBV4Lyq+qOqOgR4IbDvmH7rrBpW1dur6tsTCHcc8P3m67hzSTJtPhtJkiRJgmmUwAFPBX5bVR8fbaiq66vqw0kWJlmS5DvAOesaIMniJM9LcmSSL/a1H57ka812gGOBhcAzksxq2uckuTrJZ4ArgAcnOSHJRUmWJ3ln33hfbSqEVyY5fuN+DJIkSdKmrUam/2u6mk4J3KOBS9Zz/GDgeVX1lAmM9W3gsUm2a/ZfAJzWbD8BuK6qfgYsBf6077y5wEer6tHAI5r9Q4H5wCFJntz0e2VTIVwAvDHJLhOYkyRJkiRNyXRK4H5Pko8kuTzJRU3Tt6rqtomcW1VrgLOAo5oll38KnNEcPo77k7nT+P1llNdX1fnN9h83r0vpJZb700vooJe0XQ6cDzy4r33sezg+ybIky7571zUTmbokSZIkrdN0ugvllcAxoztV9bokuwLLmqa7JjneacDrgduAZVV1R5KZTYyjk7wVCLBLku3HiRHg76vqn/sHTXI48HTg8VV1d5KlwKzxJlBVi4BFAP+y70tqkvOXJEmSpN8znSpw3wFmJfmLvrZtpzDed+ktu/xz7q+4PQ1YXlUPrqo5VfUQ4EvAH9zpEjgbeGWS2QBJ9kmyO7AjcHuTvO0PPG4Kc5QkSZI2Ow/09W1eA7cRVFUBzwGekuS6JBcCnwZOXMcpj0iyqu917Jjx1gJfA57ZfIXecsmvjBnnS4xzN8qq+ibw78APk6wATge2p7c0c4skPwLeS28ZpSRJkiQN3XRaQklV3Ujv0QHjWdzXbyWw5Th9vti/U1Wvp7eMcnT/FePEXAIsaXYPGHPsg8AHx4nzzHXMUZIkSZKGZlolcJIkSZI2fdN5ieJ0N22WUEqSJEmS1s8ETpIkSZI6wiWUkiRJktpVeaBn0FlW4CRJkiSpI0zgJEmSJKkjXEIpSZIkqVXehXJw6T0/W8N2xp4vauWDbiPIH79rjxaiwJHvXN5KnDMX7txKnJG77h16jLU33jn0GAAzH7R1K3H+6cxdWolzwkf/x9BjbPHYZw89BsBZB7y1lThPfOwvW4nzgwv2HnqMp/+fvYYeA2Dl+37cSpzb7ti2lTi7PWj4/9684+5ZQ48BcGjNbiXO7TPa+Zlr97XDv7bov2a289P33mvbWSx2Q0vv521/O/x/0wC2edX7p/0FZv/15MOnfRKy53lLp+Xn6BLKTci0/1swDW1KydumZlNK3jY1m1LypsG0kbxpMG0kbxpMW8mbNn0uoZQkSZLUqhrxlw2DsgInSZIkSR1hAidJkiRJHeESSkmSJEmt8i6Ug7MCJ0mSJEkdYQInSZIkSR1hAidJkiRJHeE1cJIkSZJaVeVjBAbVqQpckrVJLut7zVlP34VJTmm2T07ypmZ7cZLrmvN/nOQdE4i7MMneffsrk+w69XckSZIkSRPXtQrcPVU1fyOMc0JVnZ5kFnBVks9U1XXr6b8QuAL45UaILUmSJEkD6VQFbjz91bAkC5IsncTps5qvdzXnvz3JRUmuSLIoPc8DFgCfbap22zTnvCHJJUlWJNl/Y70fSZIkaVNXI9P/NV11LYHbpm/55FemMM4/JLkMWAWcVlU3N+2nVNX/qKoDgG2AZ1XV6cAy4MVVNb+q7mn63lJVBwMfA940hblIkiRJ0oR0LYG7p0mi5lfVc6cwzgnNUsw9gacleULTfkSSC5KsAJ4KPHo9Y3y5+XoxMGe8DkmOT7IsybKz7/7pFKYrSZIkSd27Bm48a7g/EZ21vo5jVdWdzZLLJyW5BPgosKCqfpHk5A2Mt7r5upZ1fI5VtQhYBHDGni+qycxNkiRJ2lTViHehHFTXKnDjWQkc0mwfM5kTk2wBPBb4Gfcna7ckmQ08r6/rHcD2U5umJEmSJE3NppDAvRP4YJJl9KphEzF6DdxyYAXw5ar6NXAqvbtNng1c1Nd/MfDxMTcxkSRJkqRWdWoJZVXNHqfte8DDx2lfTC/xoqpO7mtfuJ7x3wa8bZz2LwFf6mua03dsGXD4huYuSZIkqae8uGhgm0IFTpIkSZI2CyZwkiRJktQRnVpCKUmSJKn7vAvl4KzASZIkSVJHmMBJkiRJUkeYwEmSJElSR3gNnCRJkqRWeQ3c4EzgWvIg7ht6jB9svfXQYwDM+/CV7cTZctehx7j+tDuHHgPg9rvaef77h7ae2Uqcxf906NBj3HXWiqHHALhvyZmtxPn2qy5oJc6RV/xdK3F+tOCvhh7jG7PWDj0GwOO/eHErce68e+dW4qzK8P8v2OI3I0OPAbDVVu38gPcb2nkg1UPWDP/93NrOfwNsVe382fzXzHb+bGYy/Pdz04cvH3qMUXNe1VooPQBcQilJmpQ2kjdJkjQ+K3CSJEmSWlXtFFc3SVbgJEmSJKkjTOAkSZIkqSNcQilJkiSpVd6FcnBW4CRJkiSpI0zgJEmSJKkjXEIpSZIkqVXV0rMEN0WdqMAluXPM/sIkp2zgnN/1SbJbkguSXJrksCQrk6xIclnz9egJzOEtfdtzklwx6PuRJEmSpEF0IoHbCJ4GrKiqg6rqe03bEVU1H3ge8KEJjPGWDXeRJEmSpOHpfAKX5Ki+6tq3k+wx5vh84P8CRzcVt23GDLEDcHtf/68muTjJlUmOb9reC2zTnP/ZpuvMJKc2/b45zriSJEmSxlEj0/81XXUlgRtNni5Lchnwrr5j3wceV1UHAacBb+4/saouA94OfL6q5lfVPc2hc5tlkN8F3tZ3yiur6hBgAfDGJLtU1UnAPc35L276zQU+UlWPBn4NHLNx37IkSZIk/b6u3MTknma5I9C7vo1eggWwL/D5JHsBWwHXTXDMI6rqliQPA85JsrSq7qSXtD236fNgeonareOcf12THAJcDMyZzBuSJEmSpMnqSgVufT4MnFJVjwFeA8yazMlV9TPgJuBRSQ4Hng48vqoOBC5dz3ir+7bXMk4ynOT4JMuSLFty97WTmZYkSZIk/YGuVODWZ0fghmb75ZM9OcnuwEOB64HHAbdX1d1J9m/2R92XZMuqum+iY1fVImARwHl7HluTnZskSZK0KRrxMQID2xQqcCcDX0xyMXDLJM47t7me7lzgpKq6CTgL2CLJj4D3Auf39V8ELO+7iYkkSZIktaoTFbiqmj1mfzGwuNk+AzhjnHP6+/xuu9mfs444q4FnruPYicCJfU0H9B17/4begyRJkiRNVScSOEmSJEmbjnIJ5cA2hSWUkiRJkrRZMIGTJEmSpI5wCaUkSZKkVtWISygHZQVOkiRJkjrCBE6SJEmSOuL/Z+/O4+2q6vv/v94kgQQSRlFBlCggqAwhBBRFREWrlioqikirqBX1a63WH7ZWLIKVVovW+atGS3EqUhEsXyekyCRjAoQERAYBEURlUGQIQ+79/P44++rxcm+SO5yde5LXM4/9uPusvfb6rH3ulM9da6/tFEpJkiRJrapa0z3oX47ASZIkSVKfcASuJd+fOaPnMfZ8YLDnMQAuvnPLVuI8bkbvb2799MBGPY8BMGtmO38rOeSB9VuJM3jNNT2PMf/Bdt6z6Ts+oZU4z3r6ta3EuXrBO1uJ85TFn+x5jMN2PaLnMQBmH3V4K3FuOOh7rcT5zfTe/+x8eGDDnscAmN/OrzV+tV47ge5P7z83mw30PAQAv5nWTpzNBtpZ6OLn03r/xt1w22Y9jzFkbmuRtCaYwEmSxqSN5E2StHZzFcrxcwqlJEmSJPUJEzhJkiRJ6hNOoZQkSZLUqsFyCuV4OQInSZIkSX3CBE6SJEmS+oQJnCRJkiT1Ce+BkyRJktSq8h64cVvlCFySgSRLklyR5LIkzxxLgCRHJ2nnaax/GvddSR5IsklX2WFJPjPGdnZI8p0kP0tyaZKzkuw7+T2WJEmSpJVbnSmUy6tqXlXtBvwj8K+TEThJr0f/DgEWAa8YbwNJZgLfBRZW1XZVtQfwDuBJI9R1NFOSJElST431HriNgd8OvUjyniSLkixNckxX+ZFJrk3yY2DHrvKzk3wiyWLgnUmen+TyJMuSHJ9kg6beaOU3JfnXZkRwcZL5SU5vRsfe2hVnO2A28H46iVy3xzf9uC7JB5r6H07y9q7zh0YNDwUurKrTho5V1ZVVdUJXva8mOR/46hjfS0mSJGmdVDX1t6lqdUaNZiVZAswEtgKeB5DkhcAOwF5AgNOaqYX3Aa8B5jXtXwZc2tXe+lW1oBndug54flVdm+QrwNuSfB44YXg58Inm/Jural6Sjzf1ntX07Urg802d1wDfAM4DdkzymKr6dXNsL2Bn4H5gUZLvAic17X+2qfNq4M+Av2v6vzJPBfapquWrqCdJkiRJEzKWKZQ7AS8CvpIkwAub7XI6Sc5OdBK6ZwOnVtX9VfV74LRh7Z3UfNwRuLGqrm1efxnYdyXlQ4baWwZcXFX3VNXtwINJNm2OHQJ8o6oGgW8Br+o6/4yqurNJuE6hk3xdDjw6ydZJdgN+W1W/GP5GJDk1yZVJTunuz2jJW5LDm5HCxUvuuX6kKpIkSZK02sZ031ZVXZjkUcCWdEbd/rWqvtBdJ8m7VtHMfWPr4iM82Hwc7Nofej09yS50EskzOnkm6wM3AkOLlwwfEB16/U3gIOCx/DHJvIqu5LGqXp5kAfDRrvNHvZ6qWggsBPjHua+dwgOxkiRJUnsGXYVy3MZ0D1ySnYBpwJ3A6cAbk8xujj0uyaOBc4EDk8xKMgf4i1GauwaYm2T75vVfAeespHx1HQIcXVVzm21rYOsk2zbHX5Bk8ySzgAOB85vyk+hMvTyITjIH8F/As5K8tKv9DcfQF0mSJEmaNGO5Bw46o26vr6oB4IdJngJc2Ix03Qv8ZVVdluQk4ArgN3RWgnyEqnogyRuAbzYrOC4CPl9VD45UPoZreg3wkmFlpzblvwYuoTOtchvga1W1uOnPVU3CeWtV3daULU9yAPDvST7RnH8P8KEx9EeSJEmSJsUqE7iqmraSY58EPjlC+bHAsSOU7zfs9ZnA7iPUG618btf+CXQWMRl+7BFL/FfVu7tenjD8eFe9XUYo+ymPTAiHjh09WluSJEmSRuaDvMdvrI8RkCRJkiStISZwkiRJktQnxrQKpSRJkiRN1FR+UPZU5wicJEmSJPUJEzhJkiRJ6hMmcJIkSZLUJ7wHTpIkSVKrBn2MwLg5AidJkiRJfSLlEjCtuGneC3r+Rn/xd1v2OgQAF6y4vZU4T5y+Sc9j3DxwT89jAMzKjFbivHRw01biDLTwR7M712vnZ9OTHm4lDLMHB1uJ872ZA63EOeyhFT2PMX/pR3seA+CGfd7eSpz3L1+/lThzWvh5s0FLf/998mA779lv1mvn+/N+eh9nTkufm20G2olzb0uDNG2EOT+/byFKxyk/P23KD28t3ubAKZ+ELLjl21PyfXQETpI0Jm0kb5KktVtVpvy2OpK8KMk1Sa5P8t4Rjr87yU+SLE1yZpJtJ/remcBJkiRJ0hglmQZ8Fngx8FTgkCRPHVbtcmBBVe0KnAz820TjmsBJkiRJ0tjtBVxfVTdU1UPAN4CXdVeoqrOq6v7m5UXANhMN6iqUkiRJklq1lqxC+TjgF12vbwGevpL6bwK+P9GgJnCSJEmSNEySw4HDu4oWVtXCcbb1l8AC4DkT7ZcJnCRJkiQN0yRrK0vYbgUe3/V6m6bsTyTZHzgSeE5VPTjRfpnASZIkSWrVlH+GwOpZBOyQ5Il0ErfXAK/trpBkd+ALwIuq6jeTEdRFTCRJkiRpjKpqBfA3wOnA1cB/V9VVST6Y5KVNteOA2cA3kyxJctpE405oBC7JvVU1u9l/CfAJ4AV0ltK8v6q+kuQw4IdV9cuVtHMYneU1/2Yi/RnW5reBx1bVM7rKTgC+U1Unj6GdFwEfBDYGHgCuAd5TVTdPVl8lSZIk9Z+q+h7wvWFlR3Xt7z/ZMSdlCmWS5wOfAv6sqn4OfL7r8GHAlcCoCdxkS7IpsAdwb5InVdUN42xnZ+DTwEur6uqm7KXAXODmYXWnN1m4JEmSpJVYS1ahXCMmPIUyyb7AF4EDqupnTdnRSY5IchCd1Va+3gwZzkqyZ5ILklyR5JIkc5qmtk7ygyTXJfm3rvZfmOTCJJcl+WaSoRG/m5Ic05QvS7JTV7deAfw/Os9ieM2wLu+fZHGSa5Mc0LR1UZKndcU8O8kC4B+AfxlK3gCq6rSqOrer3ieSLAbeOdH3UpIkSZJWZqIJ3AbAt4EDq+qnww82UxUXA4dW1TxgADgJeGdV7QbsDyxvqs8DDgZ2AQ5O8vgkjwLeD+xfVfObtt7dFeKOpvxzwBFd5YcAJzbbIcO6NZfOQ/f+HPh8kplNn14NkGQrYKuqWgw8DbhsFe/B+lW1oKo+top6kiRJkjQhE03gHgYuoPNQutWxI3BbVS0CqKrfd007PLOq7q6qB4CfANsCzwCeCpyfZAnw+qZ8yCnNx0vpJGYkeQywA/DjqroWeLiZCjnkv6tqsKquA24AdgL+GzioOf5q4BH3yCXZohlFvDZJd7J40mgXm+TwZrRv8X/decvK3xlJkiRpHVGVKb9NVRNN4AbpJDx7JXnfBNvqfibCAJ378wKcUVXzmu2pVfWmEc4Zqk/Tn82AG5PcRCex6x6FG75qaVXVrcCdSXalMwo4lJRdBcxvKt3ZjCIupLOSzJD7RrugqlrYjM4teO0W24x+5ZIkSZK0GiZ8D1xV3U9nOuKhSUYaibsHGLrP7RpgqyR7AiSZk2RlC6lcBDwryfZN/Y2SPHkVXTqEznMW5lbVXDqLmXTfB/eqJOsl2Q54UtMn6CRtfw9sUlVLm7J/A45M8pSu8zdcRXxJkiRJ6olJWYWyqu5qlts/N8ntww6ffxy1owAAIABJREFUQOdes+XA3nRGuD6dZBad+99GXVqzqm5vHjFwYpINmuL3A9eOVD/JXDpTLC/qauPGJHcneXpTdDNwCZ3HAry1mbIJnWmTnwT+uevcZUneCXwlycbAHc35Hxj93ZAkSZKk3phQAjf0DLhm/xfAE5uXp3WVfwv4Vtdpi+jc29bthGYbOueArv0fAXuOEHtu1/5iYL/m5eNGqDu/2b14Jdfya0Z4P6rqu8B3Rzlnv5HKJUmSJI1ucE13oI9NeAqlJEmSJKkdJnCSJEmS1Ccm5R44SZIkSVpdxdRdpn+qcwROkiRJkvqECZwkSZIk9QmnUEqSJElq1WCt6R70L0fgJEmSJKlPmMBJkiRJUp9wCmVL/ueux/Q8xp8/vLznMQDe+/J2vmyO/N60nsd4w8CWPY8B8MRq53Oz80t+3UqcGS99Qc9j/PAtS3oeA+AlV36olTgrvv2ZVuLs/c1LW4kz+6jDex7jhn3e3vMYAE/68WdbifOZg97YSpyrrn50z2NsuWE7P9O+nRmtxNmg2lkNb9vB3v/+nNnS05F/09L/IGe0NM3utvUGeh7ja//fNj2P0U8GXYVy3ByBkySNSRvJmyRJGpkJnCRJkiT1CadQSpIkSWqVD/IeP0fgJEmSJKlPmMBJkiRJUp8wgZMkSZKkPuE9cJIkSZJa1dITL9ZKjsBJkiRJUp9Y7QQuyRZJljTbr5Lc2vV6/WF135Vkw67XNyVZlmRpknOSbDtZF9DEeiDJJl1lhyUZ01Nzk+yQ5DtJfpbk0iRnJdl3Nc+9Kcmjxtp3SZIkSRqL1U7gqurOqppXVfOAzwMfH3pdVQ8Nq/4uYMNhZc+tql2Bs4H3T6TTwxwCLAJeMd4GkswEvgssrKrtqmoP4B3Ak0ao67RTSZIkaQKKTPltqprQFMokz09yeTO6dnySDZL8LbA1cFaSs0Y47ULgcc35c5P8NMkJSa5N8vUk+yc5P8l1SfZq6j2na7Tv8iRzmvLtgNl0EsJDhsV5fJKzm3Y+0NT/cJK3d/X/6CRHAIcCF1bVaUPHqurKqjqhq95Xk5wPfLUZjfxhkquSfAmm8GdYkiRJ0lpjIgncTOAE4OCq2oXOgihvq6pPAb+kM+L23BHOexHw7a7X2wMfA3ZqttcC+wBHAO9r6hwBvL0Z/Xs2sLwpfw3wDeA8YMckj+lqdy/glcCuwKuSLABOAl7dVefVTdnTgMtWcb1PBfavqkOADwA/rqqnAacCT1jFuZIkSZI0YRNJ4KYBN1bVtc3rLwMru2fsrCS3Ai8GTuwqv7GqllXVIHAVcGZVFbAMmNvUOR/492Z0b9OqWtGUHwJ8ozn3W8Cruto9o5n2uRw4Bdinqi4HHp1k6yS7Ab+tql8M72iSU5NcmeSUruLTmrZorvNrAFX1XeC3I11wksOTLE6y+IJ7r1vJWyNJkiStOwb7YJuq2lyF8rnAtsAS4Jiu8ge79ge7Xg/SPOagqj4M/DUwCzg/yU5JdgF2AM5IchOd0bjuaZQ1LP7Q628CBwEH0xl9g07iOP8PFateDhwGbN51/n2rd5ldAasWVtWCqlrwzNk7jPV0SZIkSfoTE0ngBoC5SbZvXv8VcE6zfw8wZ/gJzcjZu4DXJdl8+PHRJNmuGaX7CJ0FS3aik6wdXVVzm21rYOuuFS5fkGTzJLOAA+mM4kEnaXsNnSTum03ZfwHPSvLSrrDDF2Hpdi6dqZ4keTGw2epeiyRJkiSN10QSuAeANwDfTLKMzojZ55tjC4EfjLSISVXdRmcK5duHH1uJdzVTGpcCDwPfp5OEnTqs3qlNOcAldKZVLgW+VVWLm/hX0Ukub236QjM18gDgrUluSHIhnYVRPjRKf44B9k1yFZ3VL28ew7VIkiRJ67Q1PT2yn6dQjmtJ/Ko6uuvl7iMc/zTw6a7Xc4cdf0fXy527yg/r2r9p6Niw+kMescR/Vb276+UJI3a+U2+XEcp+CrxklPpHD3t9J/DC0dqXJEmSpF5o8x44SZIkSdIE+FBqSZIkSa2ayg/KnuocgZMkSZKkPmECJ0mSJEl9wgROkiRJkvqE98BJkiRJatWgt8CNmyNwkiRJktQnHIFryU3rreh5jL941D09jwEweN9AK3E2YlrPY2wx0PvPC8Ct681sJc7O67XzNXDF2y7qeYyLZrbznj3/Y+9pJc4vTl7eSpx779+890FefTI3DG7Y8zAnr79+z2MAfOagN7YSZ/OTj28lzka7HtHzGHfe38735/2zqpU4s6udoYBdHn6g5zFuaun3zQO087mZ1dLnZloLKyLWr27veQytG0zgJElj0kbyJklauw36GIFxcwqlJEmSJPUJEzhJkiRJ6hNOoZQkSZLUqnbuolw7OQInSZIkSX3CBE6SJEmS+oRTKCVJkiS1anBNd6CPOQInSZIkSX1iyiRwSQaSLOna5ia5YBLbvynJoyarPUmSJElq21SaQrm8quYNK3vm8EpJplfVipb6JEmSJGmSDcYHeY/XlBmBG0mSe5uP+yU5L8lpwE+STEtyXJJFSZYmeUtXvXOTfDfJNUk+n+QR15jk20kuTXJVksO7yl+U5LIkVyQ5synbKMnxSS5JcnmSlzXlT2vKljR92KGVN0WSJEnSOmsqjcDNSrKk2b+xql4+7Ph8YOequrFJuu6uqj2TbACcn+SHTb29gKcCPwd+ALwCOHlYW2+sqruSzAIWJfkWnWT2i8C+TYzNm7pHAj+qqjcm2RS4JMn/Am8FPllVX0+yPjBtst4ISZIkSRrJVErgRppC2e2Sqrqx2X8hsGuSg5rXmwA7AA819W4ASHIisA+PTOD+NslQgvj45twtgXOHYlTVXV2xXprkiOb1TOAJwIXAkUm2AU6pquvGfMWSJEnSOsgHeY/flJ5COcx9XfsB3lFV85rtiVU1NAI3/OvhT14n2Q/YH9i7qnYDLqeTlI0mwCu7Yj2hqq6uqv8CXgosB76X5HmPODE5PMniJIuX3fOzsVyrJEmSJD1CPyVw3U4H3pZkBkCSJyfZqDm2V5InNve+HQz8eNi5mwC/rar7k+wEPKMpvwjYN8kTmzaHplCeDrwj6dxpmWT35uOTgBuq6lPA/wC7Du9kVS2sqgVVtWCXOdtNzpVLkiRJWmf1awL3JeAnwGVJrgS+wB+ngy4CPgNcDdwInDrs3B8A05NcDXyYTuJGVd0OHA6ckuQK4KSm/j8DM4ClSa5qXgO8GriyuW9vZ+Ark32RkiRJktRtytwDV1WzRyurqrOBs7vKB4H3NdsfNINkv6+qA0Zoa27XyxeP0ofvA98fVrYceMsIdT9MJwGUJEmSNAaDa7oDfaxfR+AkSZIkaZ0zZUbgJsPwkTpJkiRJWpusVQmcJEmSpKlvMGu6B/3LKZSSJEmS1CdM4CRJkiSpTziFUpIkSVKrBnEO5Xg5AidJkiRJfcIETpIkSZL6hFMoW7JRC7nyzb/ZpOcxAB748f2txNmgej+0/tiZ7VzLFiumtRLn5jPXbyXO9WzYQpR2HvH5i5OXtxLnrnvaeM/glmzQSpzfTO/99+eczOh5DICrrn50K3E22vWIVuLMX/rRnsf4j92P6nkMgOvrnlbiPDHtfH9+ZWbvv2/2WtHzEACsaGn22/KWhho2q94Huvf823seY8hGrUUav1rTHehjjsBJksakjeRNkiSNzAROkiRJkvqEUyglSZIktcoHeY+fI3CSJEmS1CdM4CRJkiSpT5jASZIkSVKf8B44SZIkSa1q52FBaydH4CRJkiSpT0xqApfk40ne1fX69CRf6nr9sSTvnkD7+yX5TrN/WJLbk1ye5Lom1jPH2e7cJFeOUL5hkq8nWZbkyiQ/TjK7OTaQZEnXNne81yVJkiRJq2Oyp1CeD7wa+ESS9YBHARt3HX8m8HeTGO+kqvobgCTPBU5J8tyqunqS2n8n8Ouq2qWJsSPwcHNseVXNm6Q4kiRJ0jqj1nQH+thkT6G8ANi72X8acCVwT5LNkmwAPAXYpBk1W5bk+KacJM8fpfxFSX6a5DLgFaMFrqqzgIXA4c152yX5QZJLk5yXZKem/DFJTk1yRbP9yahdkic1/dgT2Aq4tSvGNVX14KS8U5IkSZI0RpOawFXVL4EVSZ5AZ7TtQuBiOkndAuA64EvAwc2o1nTgbUlmAieMUv5F4C+APYDHrqILlwE7NfsLgXdU1R7AEcD/bco/BZxTVbsB84Grhk5uRti+BRxWVYuA44F/SHJhkg8l2aEr1qyu6ZOnjumNkiRJkqRx6MUqlBfQSd6eCfw78Lhm/27gFjpTD69t6n4ZeDtwFnDjCOVnN+XXAST5Gs0I2yjS1JvdxPxm8ofHvG/QfHwe8DqAqhoA7k6yGbAl8D/AK6rqJ83xJUmeBLwQ2B9YlGTvZormKqdQJjl8qL8v3nxP5s/ZfmXVJUmSpHXCYFZdRyPrxSqU59NJnnahM4XyIjojcM+kk5D10u7A1XSu63dVNa9re8oqzr0buBnYp7uwqu6tqlOq6v8AXwNesrqdqaqFVbWgqhaYvEmSJEmaqF4kcBcABwB3VdVAVd0FbEonifsWMDfJUDbzV8A5wDWjlP+0Kd+uKT9ktKBJnkNntOuLVfV74MYkr2qOJcluTdUzgbc15dOSbNKUPwS8HHhdktc2x5/VjM6RZH3gqcDPx/m+SJIkSdKE9GIK5TI6q0/+17Cy2VV1S5I30JnaOB1YBHy+qh5cSfnhwHeT3A+cB8zpavfgJPsAGwI3Aq/sWoHyUOBzSd4PzAC+AVxBZ2XJhUneBAzQSeZuA6iq+5IcAJyR5F46iefn0pmHuR7wXTpJqCRJkqRx8kHe4zfpCVxzX9nGw8oO69o/k85Ux+HnjVb+A/64MEl3+Ql0Fj4ZrR83Ai8aofzXwMtGOGXn5vjvgD27yr8ySvuzR4stSZIkSb3QiymUkiRJkqQe6MUUSkmSJEkalVMox88ROEmSJEnqEyZwkiRJktQnnEIpSZIkqVXlg7zHzRE4SZIkSeoTJnCSJEmS1CdM4CRJkiSpT3gPXEumtzDR96bpG/Q8BsCTH3NHK3Hu/+XGq6400RgPzuh5DIDZGzzUSpyHHp7WSpyd1ru35zFuqdk9jwGw+Tb3tRJn2q/aWTB5+t29jzN3AK5jw57H2WBaO39j3HLD5a3EufP+ma3E+Y/dj+p5jDdd/sGexwC4eMF7WomzYbXztbbjQO9/Tz/c0n1FA1QrcTYbaOeClk1/uOcxBld401c3HyMwfo7ASZLGpI3kTZIkjcwETpIkSZL6hFMoJUmSJLXKKZTj5wicJEmSJPUJEzhJkiRJ6hNOoZQkSZLUqnbWMV07OQInSZIkSX3CBE6SJEmS+sSkJnBJtknyP0muS/KzJJ9Msv4kxzg6ya1JliS5MslLJ6HNE5IcNEL5ekk+1cRZlmRRkic2x25qypY02zMn2g9JkiRpXTCYqb9NVZOWwCUJcArw7araAXgyMBs4drJidPl4Vc0DXgUcn2S1riPJtDHGORjYGti1qnYBXg78ruv4c6tqXrNdMMa2JUmSJGlMJnME7nnAA1X1nwBVNQD8HfDGJP+nGZk7uxmd+8DQSUn+MsklzSjWF4aSrCT3Jjk2yRVJLkrymOEBq+pqYAXwqCSHNCNiVyb5SFf79yb5WJIrgL2TvC7J0qbdr3Y1t2+SC5Lc0DUatxVwW1UNNvFuqarfTuJ7JkmSJEmrbTITuKcBl3YXVNXvgZvprHa5F/BKYFfgVUkWJHkKnVGuZzUjagPAoc3pGwEXVdVuwLnAm4cHTPJ0Os8BnAF8hE4SOQ/YM8mBXe1c3LTzW+D9wPOa1+/sam4rYB/gAODDTdl/A3/RJJcfS7L7sC6c1Ry7eHXfJEmSJGldN9gH21TV5iImZ1TVnVW1nM5Uy32A5wN7AIuSLGleP6mp/xDwnWb/UmBuV1t/19T/KJ0EcAFwdlXdXlUrgK8D+zZ1B4BvNfvPA75ZVXcAVNVdXW1+u6oGq+onwGOa47cAOwL/SOfzeGaS53edMzSF8ukjXXCSw5MsTrL40nuvX713SZIkSZJGMZnPgfsJ8CcLgSTZGHgCnWmOwx/3UECAL1fVP47Q3sNVNXTOwLC+fryqPtoV52Ur6dcDzXTOVXmwu+t/6GTVg8D3ge8n+TVwIHDmarRHVS0EFgIcve2hPu5CkiRJ0oRM5gjcmcCGSV4Hf1gw5GPACcD9wAuSbJ5kFp0k6PzmnIOSPLo5Z/Mk244j9iXAc5I8qol7CHDOCPV+RGf65hZD8VbWaJL5SbZu9tejM/3z5+PonyRJkiRN2KQlcM1o2cvpJEjXAdcCDwDva6pcQmcq41LgW1W1uJmu+H7gh0mWAmfQuRdtrLFvA94LnAVcAVxaVf8zQr2r6KyKeU6zqMm/r6LpRwP/L8mVTb9XAJ8Za/8kSZIk/dGavr+tn++Bm8wplFTVL4C/GF7eecIAt1TVgSOccxJw0gjls7v2TwZObvaPHiX2icCJK2unef1l4MvDyg4b6Zyq+gHwg1HizR2pXJIkSZJ6pc1FTCRJkiRJEzCpI3CjqaoT6NwLJ0mSJGkd5+p+4+cInCRJkiT1CRM4SZIkSeoTrUyhlCRJkqQhg1l1HY3METhJkiRJ6hMmcJIkSZLUJ5xCKUmSJKlVU/lB2VOdCVxLVqT3i6VeN6OdBVlPv3WTVuI8b2Baz2Pse+dFPY8B8Mwtd2olzs4ztmglzq4D6/c8xqYt/WT/0g3btBLnSu5rJc7667dxU8Fy5g/O6nmUJw/2/usM4NuZ0Uqc+2e18zP6+rqn5zEuXvCenscAWLj4uFbi/OOCI1uJc+H0h3seY/7AzJ7HAJhT7Uziun76QCtxNm3hv8Tv+MWcnscYcnJrkbQmOIVSkjQmbSRvkiRpZI7ASZIkSWqVD/IeP0fgJEmSJKlPmMBJkiRJUp8wgZMkSZKkPuE9cJIkSZJaNehdcOPmCJwkSZIk9QkTOEmSJEnqE2NO4JIMJFnStb13PIGT3JTkUeM5dzXanpvkymZ/vyR3N329OskHJqH9w5J8ZuI9lSRJktY9g32wTVXjGYFbXlXzurYPT3qvJt95VTUPWAD8ZZL5q3NSEu8RlCRJkjSiJC9Kck2S60ca2EqyQZKTmuMXJ5k70ZiTNoWyGVE7JsllSZYl2akpn53kP5uypUleOcK5705yZbO9qynbKMl3k1zRlB/clO+R5JwklyY5PclWXeVXJLkCePtIfayq+4BLge2TzEtyUdOnU5Ns1rRzdpJPJFkMvDPJnkkuaNq+JMmcprmtk/wgyXVJ/m2y3kdJkiRJU1+SacBngRcDTwUOSfLUYdXeBPy2qrYHPg58ZKJxx5PAzRo2hfLgrmN3VNV84HPAEU3ZPwF3V9UuVbUr8KPuxpLsAbwBeDrwDODNSXYHXgT8sqp2q6qdgR8kmQF8GjioqvYAjgeObZr6T+AdVbXbaB1PskUT4yrgK8A/NH1aBnRPrVy/qhY0sU4C3tm0uz+wvKkzDzgY2AU4OMnjV+O9kyRJktZ51QfbatgLuL6qbqiqh4BvAC8bVudlwJeb/ZOB5yfJ6jU/ssmYQnlS17FTmo+XAnOb/f3pZKYAVNVvh7W3D3BqVd1XVfc2bTybTlL1giQfSfLsqrob2BHYGTgjyRLg/cA2STYFNq2qc5s2vzosxrOTXA78EPgwcEtT/5zm+JeBfbvqD13TjsBtVbWo6fvvq2pFc+zMqrq7qh4AfgJsO/yNSnJ4ksVJFl92z/XDD0uSJEmaorr/L99shw+r8jjgF12vb2nKRqzT5BF3A1tMpF+TfY/Xg83HgYm2XVXXNveqvQT4UJIzgVOBq6pq7+66TQK3MudV1QFd9TdZRf37VqOLD3btj3i9VbUQWAjw/rmv9WEXkiRJUp/o/r/8VNLGYwTOoOuetKF7zbqcBxyYZMMkGwEvB85LsjVwf1V9DTgOmA9cA2yZZO+mrRlJnlZVvwN+l2Sfps1DV9ahZjTvt0me3RT9FXDOCFWvAbZKsmcTb44Lm0iSJEkTs6ZXmJykVShvBbpvo9qmKRuxTpNHbALcuXrNj2w8ycisZvrikB9U1coeJfAh4LPNsv4DwDH8caolVXVZkhOAS5qiL1XV5Un+DDguySDwMPC2qnooyUHAp5pRtOnAJ+jc0/YG4PgkRWeq5Kq8Hvh8kg2BG5rz/0QT72Dg00lm0bn/bf/VaFuSJEnS2m0RsEOSJ9JJ1F4DvHZYndPo5B0XAgcBP6qqCc3MG3MCV1XTRimf27W/GNiv2b+XTqdXVv/fgX8fdvx04PQRzlvCn96vNlR+KdC9gMnfN+VnA2eP0s4zRijfb9jrRSPUO6HZhuocgCRJkqR1RlWtSPI3dHKWacDxVXVVkg8Ci6vqNOA/gK8muR64i06SNyFOB5QkSZLUqsEJrcM4dVTV94DvDSs7qmv/AeBVkxmzjXvgJEmSJEmTwAROkiRJkvqEUyglSZIktWpwdR+VrUdwBE6SJEmS+oQJnCRJkiT1CRM4SZIkSeoT3gPXkvtX93nuE/DoaufTuaBmtxLn7hb+vPCRxz6390GA9Vua5j1toJ04P5/W+6/nDaqd9YVn0k6cvdr6vmnhnoLfpngwa8+9C219rc1uKc4Ts2HPY2xY7fz99x8XHNlKnH9dfGwrcY5s4Xp+Na2d783pLf0I2HJwxMcPT7oVLXx7Pmdw494H6SNrz2+R9jkCJ0kak7UpeZMkqd+YwEmSJElSn3AKpSRJkqRW9f5mjLWXI3CSJEmS1CdM4CRJkiSpTziFUpIkSVKrBl2HctwcgZMkSZKkPmECJ0mSJEl9Yo0ncElmJrkkyRVJrkpyTFN+QJLLm/KfJHnLONu/KcmyJEuT/DDJYyehz/dOtA1JkiRpXVV9sE1VU+EeuAeB51XVvUlmAD9O8r/AQmCvqrolyQbA3AnEeG5V3ZHkX4D3AX+7qhOSTK+qFROIKUmSJEmTao2PwFXH0IjWjGZ7iE5yeWdT58GqugYgyauSXNmMzJ3blB2W5JQkP0hyXZJ/GyXcucD2zajffzYjc5cneW5XO6cl+RFwZpLZXfWWJnnlUENJjm36cFGSx/TivZEkSZKkblNhBI4k04BLge2Bz1bVxUlOA36e5EzgO8CJVTUIHAX8WVXdmmTTrmbmAbvTGdG7Jsmnq+oXw0IdACwD3k4nd9wlyU7AD5M8uakzH9i1qu5K8hHg7qrapennZk2djYCLqurIJll8M/ChyXxPJEmSpLWVD/IevzU+AgdQVQNVNQ/YBtgryc5V9dfA84FLgCOA45vq5wMnJHkzMK2rmTOr6u6qegD4CbBt17GzkiwBNgb+FdgH+FoT+6fAz4GhBO6Mqrqr2d8f+GxXP3/b7D5EJ6mETuI5dwKXL0mSJEmrZUokcEOq6nfAWcCLmtfLqurjwAuAVzZlbwXeDzweuDTJFs3pD3Y1NcCfji4+t6rmVdXrmhgrc99qdPXhqhq6t3F4rD9IcniSxUkWL73nZ6vRrCRJkiSNbo0ncEm2HJoKmWQWnWTtp0n266o2j84oGUm2q6qLq+oo4HY6idxYnQcc2rT3ZOAJwDUj1DuDznTLob5uNkKdUVXVwqpaUFULdp2z3Ti6KUmSJEl/NBXugdsK+HJzH9x6wH8D5wAnJfkCsJzOqNhhTf3jkuwABDgTuIJOgjcW/xf4XJJlwArgsKp6MMnweh8CPpvkSjojbccAp4wxliRJkqQug1N6of6pbY0ncFW1lM7iI8O9ZJT6rxih+IRmG6pzQNf+3BHaeAB4wwjlw9u5F3j9CPVmd+2fDJw8Ul8lSZIkaTKt8SmUkiRJkqTVs8ZH4CRJkiStW5xAOX6OwEmSJElSnzCBkyRJkqQ+4RRKSZIkSa0aXNMd6GOOwEmSJElSnzCBkyRJkqQ+4RRKSZIkSa0q16EcNxO4lswgPY8xq6Xvg5ktTVoe6P1bxjXTVvQ+CO18/gF2fXhaK3Eem94P3j/c8wgdj1vRzjfO1eu3EoZtV7TxtRbubyHM9S19f2472M6vwl0efqCVOF+Z2ftPzo4DG/Q8BsCF09v5SXDkgiNbiXPs4mN7HuMLux/V8xgAD7bza62V/wsADLSQTDy2nR9pWgc4hVKSNCZtJG+SJGlkjsBJkiRJapWrUI6fI3CSJEmS1CdM4CRJkiSpTziFUpIkSVKrBl2FctwcgZMkSZKkPmECJ0mSJEl9wgROkiRJkvpETxO4JEcmuSrJ0iRLkjy9l/FG6cPZSa5JckWS85PsOAlt3pTkUZPRP0mSJGldU32wTVU9S+CS7A0cAMyvql2B/YFfrMZ5vVhY5dCq2g34MnDc6pzQo35IkiRJ0rj1cgRuK+COqnoQoKruqKpfJtkzyQXNiNglSeYkOSzJaUl+BJyZZKMkxzfHL0/yMoAk05Icl2RRM6r3lqZ8v2ak7eQkP03y9SQZoU/nAtun47gkVyZZluTgrnbOS3Ia8JMm3kebekuTvKOrrXckuaw5f6cevo+SJEmSBPT2MQI/BI5Kci3wv8BJwIXNx4OralGSjYHlTf35wK5VdVeSfwF+VFVvTLIpcEmS/wUOBe6uqj2TbACcn+SHzfm7A08DfgmcDzwL+PGwPv0FsAx4BTAP2A14FLAoybld/di5qm5M8jZgLjCvqlYk2byrrTuqan6S/wMcAfz1BN8vSZIkaZ3gYwTGr2cjcFV1L7AHcDhwO53E7S3AbVW1qKnz+6pa0ZxyRlXd1ey/EHhvkiXA2cBM4AlN+eua8ouBLYAdmnMuqapbqmoQWEIn8Rry9eacZ9FJtvYBTqyqgar6NXAOsGdXOzc2+/sDXxjqY1f/AE5pPl46LNYfJDk8yeIki5fcc/2q3jJJkiRJWqme3udVVQN0ErCzkywD3r6S6vd17Qd4ZVVd012hmRb5jqo6fVj5fsCDXUUD/Om1HVpVi7vqr6zb963sYJeheMNj/UFVLQQfbOPiAAAgAElEQVQWAvzD3EP8M4MkSZKkCenlIiY7Jtmhq2gecDWwVZI9mzpzRlks5HQ695ilqbd7V/nbksxoyp+cZKNxdO884ODmHrctgX2BS0aodwbwlqE+DptCKUmSJGkcBvtgm6p6OQI3G/h0cw/bCuB6OtMp/7Mpn0Xn/rf9Rzj3n4FPAEuTrAfcSGdFyy/Rma54WZPc3Q4cOI6+nQrsDVxBZ5XQv6+qX42wGMmXgCc3/XgY+CLwmXHEkyRJkqQJ61kCV1WXAs8c4dAdwDOGlZ3QbEPnLqdzv9zwNgeB9zVbt7Obbaje33Tt7zdCOwW8p9m6y4e3swJ4d7N115vbtb8YeEQMSZIkSZpsPutMkiRJUqvKVSjHrZfPgZMkSZIkTSITOEmSJEnqE06hlCRJktSqqbzK41TnCJwkSZIk9QkTOEmSJEnqEyZwkiRJktQnvAdOkiRJUqt8jMD4mcC1ZOPq/WDnxgM9DwHAQ2knzh3r9f4b+1E1recx2rTJQDu3BA+2MHj/QEvzA7YaeKiVOHcObNBOnJa+pDdr4efNnJYmicxs6U76m9ab2UqcvVb0PsbDLf0emD/Qznv2q2nt/EfyC7sf1fMYb7n8gz2PAfDJ+b2/FoA70s5/bu5vYUmNbTOj5zG0bnAKpSRpTNpI3iRJ0sgcgZMkSZLUKh8jMH6OwEmSJElSnzCBkyRJkqQ+4RRKSZIkSa0aLFehHC9H4CRJkiSpT5jASZIkSVKfcAqlJEmSpFY5gXL8pvwIXJKBJEuSXJnkm0k2HEcbhyX5zLCyJUm+MXk9lSRJkqTemvIJHLC8quZV1c7AQ8BbJ9pgkqcA04BnJ9lolDqOTkqSJEmaUvohget2HrB9ks2TfDvJ0iQXJdkVYLTyERwCfBX4IfCyocIkZyf5RJLFwDuT7JHknCSXJjk9yVZNvTcnWZTkiiTfGs+ooCRJkrSuGqSm/DZV9U0C14yIvRhYBhwDXF5VuwLvA77SVButfLiDgW8AJ9JJ5rqtX1ULgE8BnwYOqqo9gOOBY5s6p1TVnlW1G3A18KZJuERJkiRJWql+mCY4K8mSZv884D+Ai4FXAlTVj5JskWRjYJ9Ryv8gyQLgjqq6OcmtwPFJNq+qu5oqJzUfdwR2Bs5IAp0pl7c1x3ZO8iFgU2A2cPpIHU9yOHA4wMs234s9Z28/kfdBkiRJ0jquHxK45VU1r7ugSajG6xBgpyQ3Na83ppP0fbF5fd9QGOCqqtp7hDZOAA6sqiuSHAbsN1KgqloILAQ4dttDp+44rCRJkqS+0DdTKIc5DzgUIMl+dEbUfr+Scpqy9YBXA7tU1dyqmkvnHrjh0ygBrgG2TLJ3c+6MJE9rjs0BbksyYyieJEmSpNVTffBvquqHEbiRHE1n6uNS4H7g9asoH/Js4Naq+mVX2bnAU4cWKBlSVQ8lOQj4VJJN6LxXnwCuAv6JzjTO25uPcybv0iRJkiRpZFM+gauq2SOU3QUcOIbyE+hMewR4xrBjA8Bjm5f7DTu2BNh3hPY+B3xuNbovSZIkSZNmyidwkiRJktYug2u6A32sX++BkyRJkqR1jgmcJEmSJPUJp1BKkiRJatXgFF7lcapzBE6SJEmS+oQJnCRJkiT1CadQSpIkSWrVVH5Q9lTnCJwkSZIk9QlH4FryYHof43fTeh8D4Lb1BlqJs8OK3l/QddPbuZaZLf2t5KKZrYRh4xb+aPZgS3+Z+9XM9VuJsyLtPPFm/er9D5vbpkEbP262GWjn++Y3Lf0mfKClr+kVLfy+GWjpWuZUO18D01saCGjj/wKfnH9U74MA77zsg63EOXaPf2olzpwWfk9ftb5PPtPkMIGTJI1JS38rkiStxUxnx88plJIkSZLUJ0zgJEmSJKlPOIVSkiRJUquqXIVyvByBkyRJkqQ+YQInSZIkSX3CBE6SJEmS+oT3wEmSJElq1WBLz5NcG/X1CFySSvK1rtfTk9ye5DvN65cmee8Y2/xAkn8dVjYvydUrOefoJEeMtf+SJEmSNBZ9ncAB9wE7J5nVvH4BcOvQwao6rao+PMY2TwQOHlb2mqZckiRJktaYfk/gAL4H/HmzfwhdiVaSw5J8ptl/VZIrk1yR5NymbFqSjzblS5O8o6quBX6b5OldMV4NnJjkzUkWNW18K8mG7VyiJEmStPYY7INtqlobErhvAK9JMhPYFbh4lHpHAX9WVbsBL23KDgfmAvOqalfg6035iXRG3UjyDOCuqroOOKWq9mzauBp4Uw+uR5IkSZJG1PcJXFUtpZOEHUJnNG405wMnJHkzMK0p2x/4QlWtaNq6qyk/CTgoyXr86fTJnZOcl2QZcCjwtJX1LcnhSRYnWXzZPdeP/eIkSZIkqUvfJ3CN04CPspL71KrqrcD7gccDlybZYiV1fwHcCDwHeCWdhA7gBOBvqmoX4Bhg5so6VVULq2pBVS2YP2f71b8aSZIkaS1WffBvqlpbErjjgWOqatloFZJsV1UXV9VRwO10ErkzgLckmd7U2bzrlBOBjwM3VNUtTdkc4LYkM+iMwEmSJElSa9aKBK6qbqmqT62i2nFJliW5ErgAuAL4EnAzsDTJFcBru+p/k84Uye5RvX+ic4/d+cBPJ6v/kiRJkrQ6+vpB3lU1e4Sys4Gzm/0T6Ex7pKpeMUITK4B3N9vwdu4AZgwr+xzwuRHqHj22nkuSJEnrLh/kPX5rxQicJEmSJK0LTOAkSZIkqU/09RRKSZIkSf2nyimU4+UInCRJkiT1CRM4SZIkSeoTJnCSJEmS1Ce8B06SJElSqwbXdAf6mAlcS9p41sU96XkIAHZYMa2VOLdO6/17tlm1cy3tRIFp1c4XQRtD979fr52bmx830M5EhPVo53Pzqxa+b1YAWw70/nrubeln2oyW7qOf1dL35/IWvqQ3a+HzD3D99IFW4mw52M5P6TbetjvSznt27B7/1EqcIy/951biHDT/b3seY//BTXseQ+sGp1BKksakjeRNkiSNzBE4SZIkSa2qFmanra0cgZMkSZKkPmECJ0mSJEl9wimUkiRJklrVxgJ/aytH4CRJkiSpT5jASZIkSVKfcAqlJEmSpFZVOYVyvPp+BC5JJfla1+vpSW5P8p1VnPeYJN9JckWSnyT53irqz01y5SjHzk6yYHxXIEmSJEmrZ20YgbsP2DnJrKpaDrwAuHU1zvsgcEZVfRIgya497KMkSZIkTVjfj8A1vgf8ebN/CHDi0IEkmyf5dpKlSS7qStS2Am4ZqldVS5v6SXJckiuTLEty8PBgSWYl+UaSq5OcCszq1YVJkiRJa5tBaspvU9XaksB9A3hNkpnArsDFXceOAS6vql2B9wFfaco/C/xHkrOSHJlk66b8FcA8YDdgf+C4JFsNi/c24P6qegrwAWCPXlyUJEmSJHVbKxK4ZvRsLp3Rt+H3su0DfLWp9yNgiyQbV9XpwJOALwI7AZcn2bKpf2JVDVTVr4FzgD2Htbkv8LWu2Et7cV2SJEmS1G2tSOAapwEfpWv65KpU1V1V9V9V9VfAIjqJ2aRJcniSxUkWX3bP9ZPZtCRJktS3qg/+TVVrUwJ3PHBMVS0bVn4ecChAkv2AO6rq90mel2TDpnwOsB1wc1P/4CTTmhG5fYFLhrV5LvDa5tyd6UzbfISqWlhVC6pqwfw520/GNUqSJElah60Nq1ACUFW3AJ8a4dDRwPFJlgL3A69vyvcAPpNkBZ1E9ktVtSjJYmBv4AqggL+vql8lmdvV5ueA/0xyNXA1cOnkX5EkSZIk/am+T+CqavYIZWcDZzf7dwEHjlDnOOC4EcoLeE+zdZffBOzc7C8HXjPRvkuSJEnSWPR9AidJkiSpvwzW1L3HbKpbm+6BkyRJkqQ1rnkW9RlJrms+bjZCnXlJLkxyVfPM6kc8f3okJnCSJEmSNLneC5xZVTsAZzavh7sfeF1VPQ14EfCJJJuuqmETOEmSJEmtqj7YJuhlwJeb/S8z8poc11bVdc3+L4HfAFuuqmETOEmSJEkapvuZzs12+BhOf0xV3dbs/wp4zCpi7QWsD/xsVQ27iIkkSZIkDVNVC4GFox1P8r/AY0c4dOSwdirJqIN6SbYCvgq8vqoGV9UvEzhJkiRJrRqcjEmKa1hV7T/asSS/TrJVVd3WJGi/GaXexsB3gSOr6qLViesUSkmSJEmaXKcBr2/2Xw/8z/AKSdYHTgW+UlUnr27DjsC1ZKCFGA+NPjI7qa6fvsqR3Umx9eC0nse4fb12rmWLwXb+VnJnS9fz8FrwV7MhN01r47sTNiCtxJnWQpzbpxX30/uvta1a+BkAcNt67XwNtPG5Adisev/zZtn0h3seA2DTlv6bsqKdTw0DLfzsbON7E2BOS2MAB83/21binHzZp3oe4/0Ljlx1Ja1NPgz8d5I3AT8HXg2QZAHw1qr666ZsX2CL5P9v77zjJKuKNvy8xCUjAoKSkSAiYQFFwYTihwEToC4oKCoGFBEzBoJZMaAYQBFBRQXBhAoiGQWRsIRFEAVFFAN5AQnLvt8fdXrnTu/MLDtzbk/3UM/+9jd9b0+fut3T995Tp6re0mvL615re+ZYA6cDlyRJkiwUvZogJkmSJFOXqZBCORa2bwWeM8L+i4E3lMffBb67sGNnCmWSJEmSJEmSJMmAkA5ckiRJkiRJkiTJgJAplEmSJEmSJEmS9BR7aqdQtklG4JIkSZIkSZIkSQaEdOCSJEmSJEmSJEkGhHTgkiRJkiRJkiRJBoSsgUuSJEmSJEmSpKdM9TYCbTKlInAKzpf0/Ma+3SSdOsLv7i3pSklXSLpK0ksWMPa3Je06wv5nSTqlzjtIkiRJkiRJkiQZnSkVgbNtSW8GTpR0FvH+PgHs1PkdSQLWBD4ITLd9p6RlgVUm45iTJEmSJEmSJEkeLlPKgQOwfZWknwPvA5YBjgMeknQt8HtgK+CtwGzg7vKauzuPJW0BfB1YGvgLsLft25s2JO0EfBG4Fzi/B28rSZIkSZIkSaYMzhTKcTOlUigbHALsDjwf+EzZtwHwVdtPJJyufwM3SDpG0s6N1x4HvM/2ZsCVwEHNgSVNA74B7Ew4g6u1+UaSJEmSJEmSJEk6TEkHzvY9wA+B79i+v+z+m+0Ly/MPEWmVuwJ/Ar4g6WBJKwAr2j6nvOZY4Bldw28M3GD7OkcHwu+OdhyS9pF0saSLZ87+c7X3lyRJkiRJkiTJI5Mp6cAV5pb/He5pPungItufBF4F7FL7AGwfZXtr21tvsdzjaw+fJEmSJEmSJAOJ7b7/369MZQduVCQ9VtL0xq4tiAjdncDtkp5e9r8GOKfr5dcA60hav2zPaPdokyRJkiRJkiRJgiknYvIwWRw4TNJjgfuA/wJvLs/tBXxd0tLA9cDrmi+0fZ+kfYBfSLoXOA9YrmdHniRJkiRJkiTJI5Yp68DZPrjx+K/Apo3tvwE7jPK6mcC2I+x/bePxqUQtXJIkSZIkSZIkC0k28h4/j8gUyiRJkiRJkiRJkkEkHbgkSZIkSZIkSZIBYcqmUCZJkiRJkiRJ0p/0s8pjv5MRuCRJkiRJkiRJkgEhHbgkSZIkSZIkSZIBIR24JEmSJEmSJEmSASFr4JIkSZIkSZIk6SnZRmD8ZAQuSZIkSZIkSZJkQMgIXI/4Lw+0buP9S89u3QbA2mcf0RM7b33yga3bOHz/lVq3AcD97f/9AbTBBj2xc+cXf926jS/cuHrrNgA+9pM9emLHl5zdEzv//vLlPbFz/c2Pat3GEdPuad0GwHfftUZP7Phf/+2Jnbt/276duXPUug2At/99uZ7Yeebc5XtiZ7U57dtYW4u3bwSYtcTcnth57twVe2LnQ1t/sHUbH7v4463bSB4ZpAOXJEmSLBS9cN6SJEmSqY0zhXLcZAplkiRJkiRJkiTJgJAOXJIkSZIkSZIkyYCQKZRJkiRJkiRJkvSUuc4UyvGSEbgkSZIkSZIkSZIBIR24JEmSJEmSJEmSASFTKJMkSZIkSZIk6SmpQjl+MgKXJEmSJEmSJEkyIEy5CJykRwNnlM3VgIeATlfTJ9uu1lFZ0orA7ra/WmvMJEmSJEmSJEmS0ZhyDpztW4EtACQdDNxt+7AFvU7SYrbnLKS5FYG3AunAJUmSJEmSJMnDJFUox88jIoVS0hsl/UHS5ZJOkrR02f9tSV+X9HvgM5LWl3ShpCslfUzS3Y0x3lPGuELSIWX3p4D1Jc2U9NlJeGtJkiRJkiRJkjyCeEQ4cMDJtrexvTnwR+D1jefWAJ5m+wDgcOBw208Cbur8gqTnARsATyaie1tJegbwfuAvtrew/Z4evZckSZIkSZIkSR6hPFIcuE0lnSfpSmAP4ImN5060/VB5/FTgxPL4+MbvPK/8vwy4FNiYcOjGRNI+ki6WdPE1s6+f6HtIkiRJkiRJkimBB+Bfv/JIceC+DbytRNYOAaY1nrvnYbxewCdLpG0L24+3ffSCXmT7KNtb29564+XWG9eBJ0mSJEmSJEmSdHikOHDLATdLWpyIwI3GhcAu5fGrGvtPA/aWtCyApMdJWhWYXcZOkiRJkiRJkiRpnUeKA/dh4PfAb4Frxvi9/YEDJF0BPB64E8D2r4mUygtKGuaPgOWK4uVvJV2VIiZJkiRJkiRJkrTNlGsj0MT2wY3Nr43w/Gu7dv0D2Na2Jb0K2Kjxu4cTIifdY+xe5WCTJEmSJEmS5BFCthEYP1PagRsHWwFHSBJwB7D3JB9PkiRJkiRJkiTJPNKBa2D7PGDzyT6OJEmSJEmSJEmSkUgHLkmSJEmSJEmSntLPMv39ziNFxCRJkiRJkiRJkmTgSQcuSZIkSZIkSZJkQMgUyiRJkiRJkiRJekqqUI6fjMAlSZIkSZIkSZIMCOnAJUmSJEmSJEmSDAhyhi/7Fkn72D5qKtiZSu9lqtmZSu9lqtmZSu9lqtmZSu9lqtmZSu9lqtmZSu9lKtrpNeutvGXfOyHX33KZJvsYRiIjcP3NPlPIzlR6L1PNzlR6L1PNzlR6L1PNzlR6L1PNzlR6L1PNzlR6L1PRTjIgpAOXJEmSJEmSJEkyIKQKZZIkSZIkSZIkPcWeO9mHMLBkBK6/6VW+cy/sTKX3MtXsTKX3MtXsTKX3MtXsTKX3MtXsTKX3MtXsTKX3MhXtJANCipgkSZIkSZIkSdJT1n305n3vhNxw6+UpYpIkSZIkSZIkSZKMn6yBS5IkSZIkSZKkp8yl7wNwfUtG4JIkSZIkSZIkSQaEjMAlycNA0qdtv29B+yYw/kpjPW/7thp2kqSXSFoUOM72Hj2ws5/tL7Qw9pQ6NyWtZfvGyT6OJBkkJP0cRg8X2X5xZXutzjmSwSdFTPoMSdsBM23fI+nVwHTgcNt/a8HWpsAmwLTOPtvH1bbTFpIOGOt525+vaOtS29O79l1he7NK499A3BwErAXcXh6vCNxoe90adnqFpG1tXzhV7DTsPQrYgOHnzLmVbWwIvAdYm8Yim+0dKttZEtgFWKfLzqGV7ZwP7GD7gZrjjmDnIttPbmHcnp+bktYHbrJ9v6RnAZsRjvAdFcaedy2TdJLtXSY65ih2enpuFpuPY/7zpvb5uQrwRuY/b/aubKdX14FpwOuBJzL8ujbh99Ore7Sk6WM9b/vSCjaeWR6+HFgN+G7ZngH82/Y7J2qjy16rc45+Ya2VntT3TsiNt13ZlyImGYHrP74GbC5pc+BdwDeB44BnjvmqhUTSQcCzCAful8DzgfOLrYmOPZuRV6oE2PbyE7VRWK783AjYBvhZ2d4ZuKiGAUlvAd4KrCfpii7bv61hA6AzCZT0DeDHtn9Ztp8PvLSWHUmfsH1gebyj7dNrjd3FV4nFByRdYPupA24HSW8A3gGsAcwEtgUuAKpOqIATga8D3wAeqjx2k58CdwKXAPe3aOd64LeSfgbc09lZc4Gl8FtJRwA/7LIzoclbr87NLk4Ctpb0eEI+/KfA8cALKozdnIysV2G80ejZuVlsfBp4JXA1Q+eNgaoOHPG3OA/4De2en726DnwHuAb4P+BQYA/gj5XGXm7Bv1KFz5Wf04CtgcuJ7/lmwMXAhL97ts8BkPQ521s3nvq5pIsnOn6HXs05ksEnHbj+Y45tS3oJcITtoyW9vgU7uwKbA5fZfp2kxzC0ojQhbPfkom37EABJ5wLTbc8u2wcDv6hk5njgV8Angfc39s9uKXVqW9tv7GzY/pWkz1QcfyfgwPL400BbDlxzkjht1N8aHDsQzts2wIW2ny1pY+ATLdiZY/trLYzbzRq2d+qBnb+U/4vQ7oRui/KzGUE09Rzsts/NJnNtz5H0MuDLtr8s6bJKY3uUx7Xp5bkJ4UxvZLvNxQiApXuUxtar68Djbe8m6SW2j5V0POGgTpjOPbptbD8bQNLJxFzgyrK9KXBwZXPLSFrP9vXFxrrAMhXH7/WcIxlQ0oHrP2ZL+gDwauAZkhYBFm/Bzv9sz5U0R9LywH+ANVuwg6RVGZ6aUbv+4jFAMz3rgbKvBosCdwH7dj8haaUWLqj/lPQhhpzpPYB/VrbRCxYp6YaLNB7Pm9BV/Nx6ZQfgPtv3SULSkravkbRRrcEbtVY/l/RW4Mc0ImMtfNd+J+lJnclOW/R6EtcivTw3H5Q0A9iLyCiAeveBzSXdRZwnSzUeQ90MiV6emxCR3sVpN5oMcIqkF3QisbWZhOvAg+XnHcXh+Rewak0DbaZpdrFR83pm+ypJT6hs453A2ZKuJ77PawNvqjW47TuJzIgZkrYHNrB9jKSVJa1r+4ZatvqBVKEcP+nA9R+vBHYHXm/7X5LWAj7bgp2LJa1IpGdcAtxNpINVQ9KLidSGxxIO4tpEasYTa9oh0j4vkvTjsv1S4NhKY1/C0Cp1dx60qZ+CNAM4iLhpQ6T/zKg4/qqlLkGNx/OomNa2AvHZdT6zZhpbzc+tV3YAbirnzE+A0yXdDtSsTe181zrv5T2N56q9F0lXlvEWA15XJiL3M5TiXKuuc3tgvU5draQfAZ3J6cdsn1nJzhrAOrbPL9sHAMuWp4+3/ecadhh+bnZS82qem01eB7wZ+LjtG8oq/3dqDGx70RrjPAx6cm5K+nIZ715gpqQzGO7w7FfJTqc0QMCBku4nnJ/apQE9uQ40OKo41x8myhCWBT5S2UabaZpNrpD0TYYvslwxxu8vNLZPlbQBsHHZdU0bUd9S5rI1USJyDLAE8b62q20rGUxSxKTPkLQMsdL/UCli3hj4le0HF/DSidhcB1jedtULnaTLifSl39jeUtKzgVfbrp4SKmkrYPuyea7tWulGU4pyUxiVXkVLBp1S0L4CcGptcQ5J02zft6B9Exh/7bGeryWYVCbSb7d9ddm+EngtkW50YK30TUnfB75n+5SyfS1RN7Y0sLErKGCqR2qaXTaXAtayfW3lcZcGHuzcU0oU+QXAX23/eMwX9yGS9hrredu1FvOScSLpsjIHuML2ZpIWB86zvW1lO9OAtwDPKLvOBb5W49op6eVjPW/75Ina6LI3E9gSuNT2lmXflBMxWWOlTfveCbnptqtSxCR5WJwLPL2siP0a+AMRlas6cZCkMuZ6tg+VtJakJ9uuIv5ReND2rZIWkbSI7bMkfbHi+E1mAjdTvtOqJJUtaeOSKjeiylUNdatipycSxb1y0IqTcEdJB6E47y8F/gp8pZbT0ys7DXvNlJZVgMcBtVNafkcRf1jAvnHRcdAkfcf2a5rPSfoO8JoRX7jwLN9x3grX2b6k2PlkJRsQaVOnNLbvtf25YqdWLc9DktaWtETt79RISNoZOIxYdV9X0hbAoZWuA6cS6WzXKURSLgC+B7xI0lNsv3/MVz9MenVudhy05uJn2V4UWLKGjSalLvHMxvtaEXiW7Z9UtrMvsTBxR9l+FDDD9lcr23kMUcv7WNvPl7QJ8FTbR1c003qaJkBx1L5Q/tdm5zGeM1DVgQMeKHoIhnnf7ylHBpHGTzpw/Yds36sQLvmq7c+USFZtvgrMJSJkhwKzCeWzbSrauEPSsoRT+j1J/6GhDlcLSW8nUpv+Tah1ibig1lipOgDYhyGVqyY1BRIOKz9HlCiuZANJbwTOtn1dceKPJqTk/wbsVTFyeQLwMuDOMvk8kSjK3oL47r1hwOyMlNKyOBVTWiStRjiES0nakqEUquWJaFJthqUylwnvVhXHX7G5Ybu5gl2rRhXmF8h4TuPxyhXt9EpNE0J44cnA2cXGTEm1UuceZfu68ngv4Pu23y5pCSJ9r4oDRw/PzcIZwHOJcgCApYhF0KdVtnNQM1Jp+45ybajqwAFvtP2Vhp3by/W7qgMHfJu4nn2wbP+JUHKt6cCNlKb54VqDN9LCR6RG1Mr26yY6xkJygqQjgRXL331vouQlSYB04PoRSXoqER3rpBou0oKdp9ierqJsVm4OS1S28RLgPqLodw8i5axqj6nCO4hV+FtrD2x7n/KzVYEE90iimPisvl0ezyCUSNcjUjW+BDy9kp2lbHcEHl4NfMv25xSiPDMr2eilHYjJ6JaUWh7b/5RUU1Xx/4gUwzWAplMwmyHl0AmjEEk6kCEBCwhn8QEi9bAW10h6oe1hirCSXgTUTAucLWlD23+CIZEHhUro7Ip2eqWmCZG9cGesscxjbqWxmxPdHSg11rYfkFTLBvT23ASYZrvjvGH77pIuWpuR7sdtzKUWlSSXEEVZYKl9jwZY2fYJ5bqAQ/20StsCSVcTqorft307cA7ttK54UQtjjoikFYgF406a5jlEdPzOmnZsHyZpR0JEbSPgI26v7U8ygKQD13+8A/gA0W9oVll1PasFOw+WG0Ln5rAK9SYIANhuRtvarEP4O6Ha1BqS9hxpv+s3Pm9bonhOo57yRURdz63Ab1RXEr0589yB+E7jUD6taKZndqDllJaSCnaspF1sn1Rz7C47nwQ+KemTtj/Qlh1i4eYXknZlSMBiKyIiUnPCdRChDPjxLjsHEtfTKvS4PnSWpLC2kT4AACAASURBVN2JSfwGwH5EGm0NrpB0GPAP4PFElKqTCliTXp6bAPdImt5Ja1fURf+vBTsXS/o80ImO7UtELmtzGvDDEoWBUDo8tQU790h6NENzgW2pdz+dAbwK+LWkW4HvAz+wfXOl8YHhdbslJbSTSXSR7f/UtAV8C7gKeEXZfg0RwRyzRm48FIdtSjttczOFctykiMkjFEl7ELV10wnnalfgQ7ZPrGij2dB7CSLl7B7XU+vq2DmaWKH6BcPVx6qlNimUzjpMI9K0LrW9ay0bxc5ORBRkmESx7dMqjX8p8EKgo6C4g+1Z5bk/2q4iuSzpcGB1otZhZ2BD2w9KWh34eVeUsYadm4EXt2Wn2Ho3sAGwI5EKtjehcvjlMV84PlsvZH7J7erR65LWtEGXnWqNjyUtSUTfO+mas4jPrIogS8POpsB7G3auAj5r+6oKY/dETbPL5tJEStvzyq7Tiq0aYgxLEY7t6kRU7PKy/2nA+rarqF328tws9rYmUv/+SVw7VwNe2am7rGhnGSL977nE/e10Qi20anlASXF/U7FDsfPNTo1fRTvTgS8DmxLnzSrArq4varYtMefYhYhkH2+7akqgpFcQEeWzie/A04H32P5RRRszbW+xoH0V7DTnTx3uJBqTv6uzyDvorL7iJn3vhNx8x9V9KWKSDlyfUSJhnYlIc1JVq9aKksKyLXAb4YgIOMN2G7K+HZsiUiq3rVUk3xh7RGXFNlfMy2r1D9xCI+Qy6Z0nUQysaLtKHVxJXzuS6G/3c5fGxApVxffafmElOyJu1qsBJ9r+R9m/JbBqRYe0Y2d14ISW7axB/F2eR5wzp7WR0iLp60TN27OBbxKLKxe5snqrpDcQE/k1iJS2bYELal5rip0DgB92/jZt0Yy+VB63J2qavUbSVt3OjaQXebggzETG78m5WcZclIhSHkEs5gFc68rqzcXOb9pOqS92ZtneeIG/PHE7+xEO3EbEda3659Zl81mEyMgmtquKzCj0AnbsRN3KfOo3tjevaOMCwinstC3ZDjjM9lNr2SjjfhS4iUhBFRHJXJ/IMniL7WfVtDdZpAM3ftKB6zMk/ZpYRXw30QdoL+C/tt9X2c5lLtK0vaRNuwrBFJp1EG2hkEG+yna1Rs5d469IrFTuDjzB9mMrjr0EUQN5XmPfMsT1oNpn16vJTq+QdKXtJ/XATkdqu/NzWaKVSK36xI6dK4lUowttb1Fqxj7h4WIjNewcRKQb3UZc206stSDRZecsYsHgR4TDOOHoWxn3D7a3aWyf3PmMJP3WdvW+TJJOB3bzcAXCH9j+v4o2LgX27HxOisbh+9t+Si0bvUTSRbaf3AM7ZwAvr13zNIKdnxILBxNWU16AndY/N0nbEOmUuxCqvT8grgNV69a7r9FlsfrymtdtSZsTvWdXIByr24DXdiLZFe1c3u14diJ9Iz03qKy24hP63gn51x1/7EsHLmvg+o9H2z5a0jscwhbnSPpDC3bOkLQLcLJb8uI1vG/KIoSCX9XUqWJnU6JR6Epl+xZiYjKroo2mzP8iwCaEylo1SmrTSwinbUtCKOGlhIpnNRxiBV8qNjr7qquDOqTX50paoQeTnZcDnyakqVX+u3K67qWStrHdxvnYpFO3c6+kxwK3ElGM2txn+z5JSFrS0S6j+oJEiYQfImkzIiJzjqSbbD93AS9dWDvPVih5vgI4UtLyhCP3sQkO3Ss1zSYrd5y3YvN2SbVl13cFfqSotXs6sCdDKZvV6NG5CaEQegSxSNBUCa0dlb0buLI42U07VRqGN3gUUQt5UZedKi1lGrT2uUn6BHHO30Y4bdvZvmmi447BqZJOI2rtKLZ/WdNAcdQ2L9cXbN+1gJeMl3tLSmgn/XNXhuZPfe/0JO2TDlz/0UlduFlRB/NPhuotavImQiJ/jqT7aOem2uybMofo//OSiuN3OAo4wPZZMC9F4xvUlY8+rPF4DvC3mjciSccTk6hfE+ksZwJ/tn12LRtdtO7AF3o12fkMsHObacDAU4A9JP2NeC+dc6Z2Y9VTSgT2s0S6jGlHPvqmYucnwOmSOnWRbfEfoibyVlroAQVg+1/Al0o07r3AR4CJOnC9UtNsMleNXpaKnmpVz1Pb10t6FfH3vxF4nu02RD96cW5CtCeA4UrHNVu9dDiZ+j2/RqKazP4CaPNzuw/YyUNtK1rF9nvKgsH2ZddRrtScXtGb8QoPCabsD+xS7gfvsF27H+gewOFE2wgDFwKvLgu9b6tsKxlAMoWyzyiTgvOANYmJ/PLAIbZ/NqkH1seMkmrQWoqBpJWBW2s6PpJmEpG944hUqZskXW+7DcnlToH0MoQz2pYDj6S9Rtrv0ny3op1WUtm6bKw90v7GDb0Nm0sS8uhtRzCfSaQEner6zc/fSkTFViF6gZ3g4Q2+a9l5ArHivitwCxFROMkTVKFTNLv+BaECOZ+apkv7gppoSMzoHIbEGPapUTem+XtmrUqII9wPdXpmddlr/dzsNSUNfcOy2VrNmNpXVBzVbs00Z/WoKXnD3srEOXNjd53nBMa8gqjhv7fM0z5PpIVuSaQ710xvXhT4tO131xqzX8kUyvGTDlyCpPWJC9EM209c0O8/zDFfQqyAd1QNLyZ6pZxfO6VO0o+JiVVHPe3VwFa2X1Zh7G2BTxEpIB8tNlYmnK09bVeTdS41SDOISegtRFH5pm3UC001FIp3qxHRhKYSaSsr5aVm8GXEOVNF+KWMuzah1HpL+e5tT0RiazcJbtpcmkgJ/pvt/7Yw/ieJVMY2en817VzAUG3NPxf0+ws5dk/UNLtsrkwIy0DUKd5SadwRFyI61FqQaKTQP5MenJvqUX+ukuFxLJFRImKxdS9XVG8tdlpXVOyy12bd9UjKjdXq4SWdArzf9lUKldNLiTnHesA3bH+xgo15i8KSvkU47p8u25fanj5RG132LrS97YJ/c7B5zAob970T8u87r0kHLhkdhUz9qH+M2ilnpbbmVYTD8CRCFv1k21dWGPstRBPy9xIXUYj6t48RKQEH1oyOldW8Q4jJrokI5iGOxqETHftiop/UCsSK+PNtX1icre/XugGNYHcr4ka6G3CT7SrpoArJ6FGpVS8ywip/t53aq/zHjGzGe1e0sQTRgmF3oun2ScQ58/NK43+YUDc04Yg8l5i8PYUoxN+/kp0XE03bbwM+RPSz+jewDvC+2tHRYnM6Q+fnb1uoS+rYWYJQCjUxwaoWTVSP1DQb9tpu8bAtoXQ4u2wvT0zcf19p/JHOyQ5Vz81i7yRCBr/z/X0NsLnri/JcAuxu+9qyvSFxL9iqsp1eKCqOWndtu1pf2HI/2KyTtVIiTFdUXDCe1RlL0oHAxrb3lLQccb2Z8P2mROCeBtxLCLHsYvvi8tzVtjeZqI0ue18DHkdkLTTLD3qRvtsz0oEbP1kD1z9cvOBfmTiS9iGctscRIhyvB37qupL7+xHFyrc19p1ZcshvIhr8ThhJ04DlStRgv8b+VanXwHUx251Gt4favhDAIfhQycT8lLSPSxS9x2qqD35uLLPUqxep2ah5gdh+XVtjS3oecc48DziLSHPdpgWbM4iI9dJETdJqJV1nMULmvxYfJd7LCsT72azUQ60KnMHQBLgKxTF9BUN1Q8dIOtETFxfptvMCokXGX4iIxbqS3mT7V5VMLEc0JG5VTRNAo7R4oG4919eIPqAd7h5h37hp85wchfVt79LYPqSkptdm8Y7zBmD7TwpV4tos0pUyeSuR+VGFHtddn0q7TcmbKazPodQM254tqZYj+kXiXLwL+GPDeduS6HVYm2nE37x5zpve1F8mA0A6cP3DDxlyRuZRVt1mV7RzBDER2L1xAaq+AtLlvHX23Srpb7a/XsnMl4ibQPcFbTtigvqWCjaaF/9up7BmDdyYEVgqKVG6d5L+q3ec3TaR9F7bnxnt86sUuT6ViOpu71KoXlI2a3NfiRg9IOkvtu8FsD1HUs26tLmdui1JN7g0hLX9H0lzKtrp8GoiEnJfsfkpYiJU1YEjalKebfvPxc76RO1aFQfOPVLTLLyDoRYPzy4R/09UtqFORATA9tyyWFDXiHQsIfLQrH/6XO0IHPA/Sdt7eH+uNkRZLpb0TeC7ZXsP2lmAbVtRcRPgduCPhEPyUBtzgcL7gH0YuiefTvS4rMXfJb2dWCCeTnEOS4SxinNt+1sKMa51gfMbT/0LqL5YMQkLIJPC3BTUHDfpwPUPozkj21PPGYGQI98N+JxCcvsEKl3gGtwlaXN39UVR9E+pWY+wle19unfa/rGkWpPDzSXdRazoL1UeU7anjf6yhaYzAdiOuLH+sGzvBlQTfJD0CdsHlsc7uoVG1IWvUlbyJV3gyk1OG3Q+mzYj2NOJdOPfSLqeSG9ctAU7K5a6IQHLN2qIRETLarFImUQvQqgdPqrYgIor/A3+SZwrnXqxJYE20hBnd5y3wvXUXfzq0LqaJr1p8XC9pP2IqBvAW4nPrDabef6WCG2knr8ZOK7UwkE4JyOKKE2QtwD7MpT1cR5xvauKW1RULON3ej/OIK5ttwDLqbKASbE1F/g68HVJKwFr2H6ooonXEyqazwVe2fi+bQuMlcq7UNj+u6RfutFXznYb0bdOhtHribrbZhp17YWPZEDJGrg+QdIlo+XQN/O7K9tcg1jVm0EoEv64M7mf4LjbA98jLpwdBaitiZvpqzsrpBXs/NH2Exb2uX5G0oVEpGdO2V4cOK9WMXOz2LqNwuuGnXkF6mq3efu3bb+2PN6rjfqtLntPY6gh7eXEOXNUpbHHnGjUWpGVdAMRrRwpB9iupHzaiIquRUSTTi/bOxKKelVqkxqO7o7A2sSilInFjxttv7WSnZ6oaRZbPyZW9fcnUqhuJ1L3XlDRxqrEwuEOxOd1BtHIu6rSYanlepZLTXKZwJ/jus2VtwAeT4jL/APq9+cqn9eBxc6VwCdr2yh2NiDa1qxf7Ly7F3WXaqnuuox9NvBiImhwCbEI8jvbVcopRrC3LIDtu1sY+1jgCLfcD1TSicA1xN/kUCLS+0fb72jTbq9ZZYWN+t4J+e+d1/ZlDVw6cH1Cr52Rsqp7f2N7Q2Ll6qOVxl+NWNHtOJ5XA19x9GmqgqRzCFWui7r2b0Ok6Dxj5FcutJ1FiWL/jWuMtwBb1wJP7aSglujIhbarrL730IG7HHgWEdE5szyedxEcKcV2nHZ68n5GsLsIsdr7qkFbEe2kmUma5nZVFMeKftj2cZXs9EQsQz1S0xzBbmstHnqBpD0Jx+fEsms34OO2vzP6qxZq/I8QabqXEII/n7RdvW+ipFOLjXOJGt9l20hzk3QeUWd7LtFL9Wm1Fjsepn0BT3ddwZzLbG9ZajvXtH2QpCtcX8xqU0IpeiXifvNfQi16VkUb1xBOfCv9QCUtVtLmO5/ZFbY3q72Y2y+svPyGfe+E3HLXn/rSgcsUyv7hP5KePIozUl3am6iDmzfZdRRiv4wQOJgwxVH7SI2xxuA9wAmSvs3wSN+eRMpbFUptwLVqNNZtkU8BlykaEYuQxD644virKtT01Hg8D9ufr2RnBeJv0rnwNVUHTcg7DxSSfk7Uo/zU9j1E8f+vK45/wFjPV/zbHE70MfsdlQQrRmK0aKikNal7fo46iS7Xz1p2PiBpekk7bEVNs0SnuukoAy9LKIfWsrUhkT75GNubKmr7XuyK4jJloePPwMsZEmN4eeXI5SuBLRyCP48mShHaaHy/uu0PlsenSWpFSZWohe8c/7Vt2VGP6q4Liynk/V8BfHBBvzwBjgIOsH0WgKLlwzcI9cgJIemXxKJ0tX5vo3ARcV3uCLPcURzTf9FeynYygKQD1z/0xBkpkbHHEfVcWzI0wV6eUL+rYWM0CfmqK1W2L5L0FOKi+tqyexbwlNppQMCjgFmSLmK4pO+LaxqxfYykXxErySZk3atFLYmb2XIjPIaKoiy216k11gJYQ9KXiO9W53HzOGq23ziMmCx+UtIfiFq4UypGsZZb8K9U4UFJRzHC5wX1W5bAPDGm3Yj008cC1Wp5RrC1SbEzA7iDuI7WGLcXapqXMEZ6K3UXPr5B3HeOBLB9hUKZsNr7cQijfKWkULeSbgrc7yHBn1uL09gKGl4vumhzu1ZWATCt69487F5dcdGgJ8rXhUOB04Dzbf9B0nrAdS3YWabjvAHYPlvRs7MGxxALdscCn3FLzdsbHFW+Xx8CfkYs4Hy4ZZvJAJEplH1EybHfF9i07LqKSDus5oyUtKbXEpOa5gV8NvBtV+gxoh41iu0lJY1pPmyf04KtF9NoRutKfca6bGxn+7cL2jeB8XvVb25MkYI2auJKSu0OwBuBnWwvX9tGmygaRD8X+DQjRMlrfWaKHkwvJ2o4NiQcn1faXqPG+F221mHIaXuQqIXb2vZfK9q4luFqmksBM2ulN/caSX+wvU1Xvep8DZcr2DmMyPg42S1MOCTdwVC0qNPwel70qNYim6S/EqrEbdeOnjXG07Zds5VE025rdWO9otSOXkqkUUKk1m5l+2WVxu84UTsVG/NUqmtlSEi6iVDUHbZ7yEy1TIy+YKXlNuh7J+S22ddlCmUyNsVRO0jRjPYJxMXhjrFftdA2jgWOlbSL7ZNqjt2w0RMHrVeRPmjHURsJhcT6NoQIDMB+kp7qCuIyXXyZ+dPnRto3Xjr95qYRiwWXE3+XzYiFgyqqlG04aGNRJu07E5G46VTsmTZSNKxJrciY7VuAH5Ta2ssX+ILx8x8iHehDxMq7S5p2VSRdQGQQ/IBornudoj3CXyub6pWaJjBPnKXT/Pw82z+pbOIWRauFTnPlXWmnn9WbgAOAhyR1PjtXXPh4Sdf2YZXGHUavsgrcu1YvwPx1Y5Kq1Y2pN21emuwNHMJQlPy8sq8WDxAZOEsSGRPVmp03WJSIto0WhU8SIB24vkPtN6PtcIakz9OI9ACH2p6wzL+k2YztWNW6cfesWbSkbQkH5wnAEsRF9p4Woi8vIOo55ha7xwKXESIAE0bSU4l6gFW6aq6Wp6I0fmcSIulkYLrtK8v2plSs6St1aaPe1GqmuEo6AXgyUWNzBBEdrXkDv2TBvzJxmpMpjdCMvuKk6gNE+vdXge9L+uECfn+8/JtIC38MoRB5He30aLyTSKMepqZZy06Xza8SQgmdHmBvVrT92LeimX2JmqGNJf0DuIFQuquK7VZTg3u1uNZE0uOIKO+8OVRN0Y+GnacB63TZqSL+06C1ujGixxz0KF3ToXRaPQUcQNJORGTsZ8Q97d427AA32z60pbGTKUQ6cP1Hq81oGxxNpGi+omy/hsjxnrDaVds37IadXqZiHkFMRk9kqDZxw5ZsrciQWEHN/l8QzueyxLnf/DvdBexa2RbARh3nDcD2VZJqKqq2sto+CkcDM1y3f9E8ehhN7NVk6ovAF0u9y6uAnwCPlfQ+ov3CnyrZeami99fLgYMVMuwragRRqHHS+bwuYXjt3tm0tyK+A/CETsphWcippqQH4Gjg/txSI7SI7TZ65gHzpYWfbfuUimOPlokBQM1MjGLv00QE/mqgcy0wdUU/kPQdopXAzC47tR241urGOun/bV/bJP1sAcdRYyHvg8BuNSKTC6Av0/XaIsu4xk/WwPUZnbqExraInknV1NTKuPPVOrRR/1DGXZXhjSirKDn2MNKHpIttb62G9LFa6G8maQahRNlUoXy/7arRC0lrdxzgUvS/rNvpafR9IuXku2XXHsXWjNq22kLSDrbP1FC/sWHUqBvtsncWI6cbtVL70ktKBHYGUQv3+JZsPIZYmHoVsJbtNVuysybRRuKzLYx9CrBv4xxdm+g9tXOl8TcC9gE6rVH+SDSKruJUd9nqTgufAVxs+wOVxu/UXHeik836J9t+fw07DXvXEs3J71/gL0/Mzh+BTdqoG+yy01rdWI8cK0ra59+JiPXv6XKCJiNKO14kreR6gjh9z6OWfXzfOyG33/3nvnSq04HrE9SjZrQNexcQPdTOL9vbAYfZrlKbVMZ8MVEL9ViiHmZtohFl9abkbSPpXEL44ZuEnO/NwGttb96CrdWJCQ+E815ThbJj43jgzcTK7h+IFMrDa09GJU0D3sLQ6vu5wNdcuf9Yibp8EtiE4YsFExYWkHSIo2/RSP3G7Mp94BQNdTtMI5qGz7H93sp2VgHex/yfWVsiCcszPBWs9UlKc6Gi0njzqWnafnet8Rt2ziGuARcR94EnE5HAO2FiE9+SRn0ykap/GTHZ3ZIQ5Xm57QsndPDz27uC4WnhiwKXtRAZm29BTS30hlSoBO/mlsU+FI2c97PdRl1i086jiLqx7cuu84CDSzriRMfuiWNVvlM7EuflZkTW0vd7EC1LJkg6cOMnUyj7h+bK6r+Bjurhf4GlWrD3ZuC4knoEcDswpqLfOPgosC3wG0dDymcTq3ut0Fakr/Aaoin124B3AmsSE+s2WKX8XAx4mqTqUR5iZfcuSXsQ6bnvJ1LEqjpwxVH7QvnfJscABxU7zwZeR/y9Joztg8rP+fqNSar+HbDdXQv3W0X7itp8D/gh8ELierAXLfSclPQmYoJ4H0ORxeq9ABV9zd5DV20SQ/3HxjvuSGqa67oFNc0GbfbQ/AiRCnx2Y99PJJ1JnEPPb8Fmm2nhHaSGkm6pH2ujpcC9wExJZwDzonAtCHKsDFxdzv2mndqta24nxLKWi82qjulqDDlWu9OSY1XS2k8FTpW0ZLF3dll8O6KmrSTpFzIC9winrIpTJvP7l7qVWmN30g4vB7Z09AS6vHbUqleRPoUC4Vq2r605bpeNbxEriLMYUrhqI8ozC9gCOJ5IzTqnmR5a0c52hGhJd8F/7cn7Jba3knSl7Sc199W0M4LdG22vVXnMZjPnRYim219yZbn6xmfWTAselsJdyc51wFMd6petUa4zXycWIubVKY7gEC/suP9jfjXN62t/h0ewuzawge3flGvPYjXq1CT9yfaI9buSrm3he9artPCtgG8x5CDeAezt+s3WR1zorF3npR61rpH0JKKurnPduQXYy/ZVle10HKvPAtUdqzL+C4uNdQixkW/Zbk0pNpk4Kyy7ft87IXfe/ZeMwCULpqRpjVT/UnUC3xi3Wfd0AFDNgQPuUPRNORf4nqT/0GiCXZHWI32SdiYEM5YglEG3IFQ7q66GAtva3qTymCNxJPBXQt7/3DJZnLAC6QgcTUQsh02qW+D+Ust3naS3EfLuy7Zor0MbF/amwzGHUAd8fQt2Oo1ob5b0QkImf6Uxfn+8/IWIWrTNHNtfa2HcXqlpzkPSG4katZUIIYs1COf0ORWGH8sJbOP6fDqhctxpqP6+NtLCi6O+eSerxBUUlUexM89RK+mHa9q+ogU753Q58UtTUSm4wZHMr0J5FHVUKEdyrL7EcDGgGjaOI/rn/pJwDqs6n0nSj2QErs/oSsmaBrwM+GcL6Rkj2f57zYJ/hZLV/4gowh7Eyuj3bN9ay0ax03qkT9IlRCrW2R5qejsv2lPRztHA52xfXXPch2FXwBtsf6PyuL+3/ZSaY45iZxtCiGFFwqFfAfhM7XqeEexWi8BJWqty2u+C7L2IqHdZk2iRsTwx+RlTeGAcdrYkUlx/T4spZ5IOJiLwP+6yU6XWTkNqmjOADYh0w2pqml22ZhJ1b7+vfb0pC2k/GOkp4BW2HzNRG8XOzkREbA6xePPKTnpjGyjEaz4BPNb28yVtQkR+j65s52zgxcQC+CXEd+63tg8Y63XjsDPPibe9fqnz/brtGk58085898pa988ux+oHbTlWkuYytPjQnNRWFzRL6pIRuPGTDlyfU6IK59uushq2AFs1J6OLEhGx1puSSvoN8FJCxGJl4oa6Tc3PTNKFtrdtFsq3lHL4TCL141/EJLR6U/IxbLeRDvgpYtX4ZIZPqqumNbWJxm4Yv6HtJSvZmSe4IOkk223VWPaUUsNzPnAljca3LaSc3TDCbreR6qiW1TQ7Cx+d642kxYBLa1wHRksB7FDr76IQL3mF7WskPYVYUBkxLbCSvV8RCwUftL15+cwua2GRrfM3eQMRfTuopXtBa058l502VSjTsUrGZPll1ut7J+Sue67vSwcuUyj7nw2AVWsNprGl96uJpdh+SNJcSSu0lcrS4CVEpO+dDEX6qjTClPRLQp56lqTdgUXLSuh+wO9q2OjiaEIwZdhktxZlUjXiU0Qj5Np0om9bN/aZCQpLdKN2pfd71TC+eZNorb5KjUbeI9FCtH/x2tGJkbC9bpvjdzIKHGqKDxARxUNaMneOpAOBpSTtCLwV+HmNgWs7zmMwx/Y1xebvFSIZbbKy7RMkfaDYnCOpjbTtxRRKwa8geoO1xf22H4jkCCgOaRuT3b2J7/HJZfzzyr4JY7sNEZkkSUgHru9oOFgqP/9FSH1XwT1qsl24G7hS0uk0aitqThBLpO+UEumbC9SenBwDnEasTm5KRJGOL/s+WtkWwH9rp7B18Rjg/wjV0SaiskMqaWPgY8QK8t2N/W2o3DWl3OdJ79cY2PbfehRR9iiPa9Ns5H0IkQrYJr+StA/hgFRPbexQaoQOIISG9ikLLRu5XtPoc4Gnl7qnXxPtN15BO8q67wPeQCzkvIlIQftmjYEl/ZyxHfhadb2rSjpgtG3bn69kp8M9kh5NeW+StqWdut5Diev/+bb/UFJrr2vBTmtOPNBp8fJm4PHE9+xdth8c+1VJkvQLmUKZtMZoqTotpE6dQfQvaiXSV4RYPgzsRDhy86TQa09CJH2VqOPqnuxWaSNQauyOcen/1/Xc8bZ3r2RnPyJy+UdC7fIdtn9anqvem2mUY7jI9pMrjtf29+whYqGjEw3vCH+0lm6kFprRj2CjJ6mNRVzkEmBP25sWh+53treoNP6ltqdLejuwlO3P1K61LXYWBWbZ3niBvzy+8TtpjC8nZN6/W7ZnAP+2/c5KdsZcGLBdNXopaTpRy7kpPCsHBwAAE/xJREFUcBXRjmXXNgRGekEpn3g98DziGnAa8E1XmrSV8+VBIuL2fOCvtvevMXaSPFyWXXrdvndC7r73hkyhTEZHoTZ1R2dyqFBSfCmhFPgV2w9M4uGNC9vHqgfS+7Qf6XugjLskoWzY5gVnKcJxe15jn4n0lglje1Q1w1rOW+GNRB3F3ZLWAX4kaR3bh9OCcqNGlt6v3W+q1e+Z7TYU5hZotnUDLac2Nljf9isVsvXYvled/LM6SNEEew+GVEGrp4iV9PNr2xK1cZGhl/Q5283U5p9LuniUl43HTlvppaPZu7Q4pxsR15hra0aUiqjI2bavK9+ro4FdiXv0XrYvq2ULwCHG9V3g3Jbun5t4qOXK0USrjCRJBoR04PqHEwjFyTsVEvUnEqIcWxDy1W+YxGMbF+qd9P7JVHJwupG0E/B5Qlhkuu1W5dA9QrPoAWWRTtqk7b8qpKl/VBYqBlV6v/k96zg+fbky109IWhx4C9H/C+Bs4MgW0rUeKAtGnRS69WlEsSuwP9FS4Me2Z5XUubMqjt/kUUTd7UUMXyyoee1cRtJ6tq8HkLQusEzF8SnjrkIs6KzD8F6QtXtb7gacWv42HwKmS/pYRcGkdwDfLo9nAJsD6wJbEtL4T69kBwBFf9PP0t79c975V+oFKw2bJEkvyBTKPkHDm+keBsy1/d6SRjGzFyqEtdHI0vtX2d60BVutRPoknQe82fasmuOOYW9D4GvAY0oa2GbAi21/rBf2ayHpTKK30MzGvsUIWfE9JinaNC4kvQRYw/ZXyvZFRHqWiZ5WJ07m8S0sXUJGS9NyqqakbwKLM1Sf+hrgIdtVF6VKndCHgE2IGrXtgNfaPrumnV6gHjRxLotTRwHXE3/7tYF9bP+6lo1i53dEml53g/WTKtu5wvZmkrYn6pMPAz7iSm1MJM3spONKOp6o7T28bFdPCx/l/llNhbKRsg3D07ZTITLpGcssvU7fOyH33PvXvlzdyAhc/9D8guxArPR20igm54gmzoO27+w6/jaUFVuL9Nmuuqr6MPgG8B6iuSq2ryiThYFy4IA96RIRsT0H2FPSkTUNKRTh9iUm7hAiHUe6Xr/B9xL9vzosQaRoLkuI3AyUA9djISOIlh7NOrEzFT0bq2L7dEmXAtsS19N32L6l1viStgYOZP5IUvXFtZqO2hg2Ti1CL51au2ts14xYdljadjUhrjHoOIcvBL5h+xeSal4355Zrze1EQ/WPN56rpuDcYKT7Z7XJ7iAtoiVJMj/pwPUPZ0o6AbiZSJ85E+ZNTgeu/q3QK+n9g4l+OWcD2J5Z0psGkaVtX9R1066ipthLbN80xnPVmvmWSMV3CUfq22X3VsT59FLCkX/NBM0sYfvvje3zHQqKtymk5ZOxeUjS+rb/AnQaYrch7w6hQHo7cW/bRBK2z6009veIxZVWWnwASDrf9vaav91L9ahISW19E43UVkltpLaeIukFtn9Zedxu/lEWh3YEPi1pSerWKH6EWBxaFPhZJyujXIOur2inQ6/un0mSDCCZQtknlKLoVwKrAyfY/kfZvyWwqu3TJvP4xkNRgfsgQ4IcpwEfs31fZTs9abLdCxTNaN8GnFgU73YFXm+7Den9gaekM76pW0CgRGHPJeqVxmxc/DBs/NmjNGuW9Bfb609k/KmOpOcQDnYzVe91tqvWj0n6NHENncWQg+VaNUMd56rGWGPYWNv239q00bDVq9TW2URt3QPlf1upuksTSsFXFqGR1YEn1UwJLemZ9zvaB2xS7F1DCI3cPfarF9pWT+6fSTKZLLXU2n3vhPzvf3/ryzS4dOD6FEU/m2cAN9q+ZEG/349Iml6xgHwsO0cDZwDvJ/p/7Uc0D35z27ZrU6ITRwFPIyIJNxA1Yz2Z1A0akq62vckoz11H9AGbULRE0veIOpRvdO1/E/As2zMmMv4jgRIN2ahsXttGqp6ka4HNWkoD7DiiM4hrTfUWH8XGvFoqSSfZ3qXW2CPYmq8Fwkj7BglJmzMkJnKe7Wqpuoq2CM8norunA08hRGx2BE6z/fExXr6wtnrRdzJJJp104MZPplD2CZJOAd5v+6qycngpka6xvqSjbH9xco9wXHxO0mrAj4Af2r6qJTtvJ1Yqm022B61mDICiCPfckpq3CFFU/iogHbiRkaRH2b69a+dKwJyJOm+FdwI/KelMnQWJrYi2Ei+tMP6UpoepetcTEaVWHDjgdUS92OI0InzUVcBtThTaTgPvSWpryS7ZA1jX9kclrQmsbruqbL2kdxBql52/x3fLvfPLlUzsSqhCLwn8ixA2uquIjv2e4TVxE8LRSmKupBXcUt/JJEkGm4zA9QmSZtl+Ynl8ILCx7T0lLQf8dhDTAQGKA/cKIrVpecKRq+pc9SrS1yaSlieEOB4H/BT4Tdl+F3CF7ZdM4uH1LZL2ISZt72a4c/Vp4GjbR1W0tQPwxLI5y/aZtcaeyrSdqifpy4Qj9ThC2r07QlalT5+ka21vtODfnJCNZgSu1Yb3PUxt/Rrh8O5g+wmSHgX82vY2le1cATzV9j1lexngglr3zq4U/XmPy/Y8hcpaSPop0aKgrf6mSTLpZARu/GQErn9orkY/h1AjxPZsSa0UzPcC2/8CviTpLELN7yPUj471KtLXJt8hUiYvIBySDxKTqpe5IcWfDMf2UZL+SciGz3OuiFqRn1e2dSZFXChZKNpWoew0n76E6NfYFr+TtIntq1u0sbmkuyiy7uUxtFA3ZvuMIo7Ramor8JRSz3tZsXu7pCVasCOGRxAfom6fxgckLe3oBbrVPKPSCrQjatNaf9Mk6RcyiDR+0oHrH/4u6e3ATcB04FSY199s8ck8sPEi6QlE5G0X4FaiWfm7atux/exGpO/IEs2qHulrmfVc+vuUiMXNRF+7LFhfALZPkfSb/Kz6llZT9WwfW8ZdBrjP9kNle1Ei3a0W2wIzJd1ARPg6TlW17IheSrv3MLX1wfK36DRYX4V2HJ5jgN9L+nHZfilwdMXxn9FxcLtSsxcHJiSUNAo/ot3vc5IkA0ymUPYJklYFDiVUKL/SUc6S9GxgK9uHTebxjQdJFwK/IOT9/9CLCbakJxGRvlfabmOVtxW606XaTp+aakj6M/BvomHweYTUf9aO9AEl9fTbtJ+qdyHw3I4aoKRliVS9p1Uaf+2R9g+qwFAPVSj3IBbytiK+B7sCH7JdvX+ipOlARyn0vG512kGi7e9zkvQD06at1fdOyH333diXKZTpwCXVkbQY8Algb+DGsntNYoX0g7VXeEeJ9P3I9n9q2mkTSQ8RdQ6dC8VShIBJK5LbUxFJaxEKdNsBLwDuqF2XkiwcJWqwH/BV2lehnK8OqY3apMbYKwL71lQf7CW9VKGUtDFRGgBwpu0/Vhx7pbGed/RsHDh6/X1OkslgyWlr9r0Tcv99f+9LBy5TKPsESWPWbrhSL6Me8VlgOUJ1bDbME+k4rPx/R2V7xxCRvrfSo0hfbXqZOjUVkbQG4bg9nRCymAWcP6kHlXTU9GbY/gJwRcvm7mkKGknaCvjfRActqokfBh4L/AT4PpEt8ZryeFDpZYP1pYkG2CYWp2pySRm3M8nqTAhVHret5tkW3d/nranwfU6SZGqQEbg+QdJ/gb8TE4Lf01V8bfucyTiu8aDov7Whu75cZTX+GtsbVLLT00hf25TPZ5btjSf7WAaNIvTzB+ATtn862ceTDCHpC0Sq3g8ZrqZXVTlW0jbAD4B/EtfP1YBX2b54zBcueNyzgHMIgaGdyv+ZwDuLSNNAIWl/4HfAioRY1g3lqXWAvWurq0r6CLAbcBLxd3kpcOKA1Sj3nK7vM0R5xSs9oH1hk2QkMgI3ftKB6xPK5H1HolHsZkRE6fu2Z03qgY0DSX+yveHCPjcOO18gIn3vHCHS9z/btSN9rVOko99u+8YF/nIyD0UD3+0JQYa1gOuAc2zXFDFIxkFxgLqx7R0q21mSEMeYl6oJLDLRdM3utEJJNxECQwOpDlz6lj0NeAJxntxENKQ+yfY/x3rtOO1dC2zeyYwowlwza7dkkPQyIj3zzrK9IvAs2z+paadtiuP2d9v/agjNvBy4GvjIoKaEJslILLHkGn3vhDxw/03pwCUPjzIRmUGkIh5i+4hJPqSFQtJPgJNtH9e1/9XAK2qlg/Yq0tdLJJ1L9P65iOHRikFKoZ0USpH/9kQa5asBbI8oPJFMPUYS/qkhBlRaHjyLoayIs5rbgzqhLlL+WxPO3FPL/ztsb1LZzllEO5Q7yvaKxP2htgM/Us3YsH5tg4CkSwnxktskPYOIwr2daCL+BNu7TuoBJklF0oEbP1kD10cUx+2FhPO2DvAl4MdjvaZP2Rc4WdLeRH0CxERhKeBlFe2423krOx+S1PcXhVH48GQfwCAi6WJCYvt3hArlMwZVHXCqIOmAsZ63/flKdlYjmngvJWlLhhyt5Ynaq4myAnEda97EO+mfg1xjtRTxGa1Q/v8TuLIFO3cCsySdXrafC1wk6UtQtTH1IiPsG8Q5zqKNRYFXAkfZPgk4SVL2BE2SBBjMi9uURNJxwKbAL4mo2yA2owbA9j+ApxT58E5z5V/aPqOyqasl7TlKpO+ayrZ6wiDVOvYZz7f938k+iGQYy5WfGwHbMNRke2ciwlyL/wNeC6wBNJ3C2cCBEx3c9joTHaOfkHQUcV2eTdRb/w74vO3bWzJ5GnAG4ezOISKYbXCxpM8DXynb+zK0gDhILCppMdtzCOXOfRrP5ZwtmVJkFuD4yRTKPqGIMHRS5pp/lJSRHwVJjwNOJpS55ov0FUdyoJC0LfBloj5lCUK57Z78+4+NpBWAgxhqSnwOcGj2gpt8SlrwCxt1qssBv7D9jLFfudB2dimRilaQdIbt5yxoX78j6VRgZeAqwnm7ALhqpGyGCdppikz9jbiXrUWITB3YQjuZZYgMhueWXacDH7N9z+iv6j8kfZBog3IL8XlNt21JjweOtb3dpB5gklRk8SUe1/dOyIMP/KMvUyjTgUsGnq5I39UtRPp6RkkFfBVwIuGM7knU+X1gUg+sz5F0EjEhbTYl3tz2yyfvqBKYJ2KxWUdMpKSKX1FLxELSq21/V9K7GL74BUw8VVPSNGAZ4EyG18ItD5w6iKqxkkRcM59W/m8K3AZcYPugSjbGEpm61/b+NexMRcpC3upE4+57yr4NgWVrq7cmyWSSDtz4yXB8MvAU2euq0teTie0/S1rU9kPAMZIuA9KBG5v1be/S2D4k60X6huOImqdOPe9LGXK0a7BM+bnsCM/VmBy8Cdif6APXnDzfBQyUwFSHEm27StIdRI3ancCLgCcTkewavIgukSnbd0l6C5HiXtWBKw7Ou4n68Xlzm9piKb3A9oUj7PvTZBxLkrRJ33tvfUw6cEnSX9xb1OFmSvoMcDMjF+cnw/mfpO1tnw8gaTuy6W1fYPvjkn5FqIMCvM72ZRVN/LLYOaT7CUkvmujgtg8HDpf0dttfnuh4k42k/RiKvD1IpFH+DvgWdUVMei0ydSLwdeCbtNeQPEmSpC9IBy5J+ovXEA7b24B3Eo3JdxnzFQnAm4HjSi0cwO3AXpN4PMlwlgbusn2MpFUkrWv7hgW+6uFxuqSdbP+1uVPS64APAadUsnNkcX46tXtnA0fWruXqAesQzs47bd/cop1ei0zNsf21FsZNkiTpO7IGLkn6jNLodi3b1072sQwapcamk6q1v+0vTvYxPdKRdBBRz7mR7Q0lPRY4sZYYg6QXAF8khFKuK/s+AOxOqJPeVMnON4HFGV5n+ZDtN9QYf6rRa5EpSQcD/yFa78xr3j6offqSJEnGIh24JOkjJO1MFPkvYXtdSVsQaorZyHshkXSj7bUm+zge6ZRaxC2BSztNlSVdYXuzijaeAxxJ1Ne9gajlemENafyOpLuky21v3vXcfPuS4fRKZErSSBFd2x7UPn1JkiSjkimUSdJfHExMPs8GsD1T0rqTeUADTF8qRz0CeaDIoBvmyb1XxfYZJWXybKKeawfb91Ua/iJgOvCQpPVt/wVA0npkrdUC6ZXIlO28TiZJ8oghxRGSpL94cITeZRkmHx/5ufUHJ0g6ElhR0huB3wDfqDW4pNmS7iLETJYnmh//p7F/wibKz3cDZ0k6W9LZhFPyrgrjJxNA0nsbj3freu4TvT+iJEmS9skUyiTpAyT9EtiXEF04A3g/IV6yH7C47TdP4uH1LZJmM7KjJmAp25ll0AdI2hF4HvF3Oc326ZN8SA8bSTcBnV5ySwGLlscPAf+baJ+5ZGJIutT29O7HI20nSZJMFXJykyT9wTHAacB3iKa69wPHl30fncTj6mtsLzfZx5AsmOKwnS5pZeDWyT6ehWRRosdcd0ruYkSj6mRy0SiPR9pOkiSZEqQDlyR9gO0TS6+sDwM7EY5cJ7K0L0MRgCQZCCRtC3wKuI1YhPgOsDKwSJGXP3Uyj28huNn2oZN9EMmoeJTHI20nSZJMCdKBS5L+4QHgHmBJYsU/Jx/JIHMEcCCwAlEv9nzbF0raGPg+MCgOXEZx+pvNS62jgKUadY8Cpk3eYSVJkrRHOnBJ0gdI2omIsv0MmG773kk+pCSZKIvZ/jWApENtXwhg+xppoHyi50z2ASSjY3vRBf9WkiTJ1CIduCTpDz4I7GZ71mQfSJJUYm7j8f+6nhuY6HI2gk6SJEn6jVShTJIkSaoj6SEiJViEemMnqixgmu3FJ+vYkiRJkmSQSQcuSZIkSZIkSZJkQMhG3kmSJEmSJEmSJANCOnBJkiRJkiRJkiQDQjpwSZIkSZIkSZIkA0I6cEmSJEmSJEmSJANCOnBJkiRJkiRJkiQDwv8DeqL6BGTgtI0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x1080 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "x2A_bc2tqLf7"
      },
      "cell_type": "markdown",
      "source": [
        "From the correlation heatmap above, we see that about 15 features are highly correlated with the target."
      ]
    },
    {
      "metadata": {
        "id": "WV0ziidQqnE3"
      },
      "cell_type": "markdown",
      "source": [
        "**One Hot Encode The Categorical Features :**\n",
        "\n",
        "We will encode the categorical features using one hot encoding."
      ]
    },
    {
      "metadata": {
        "id": "Mo2ZI9barWSl"
      },
      "cell_type": "code",
      "source": [
        "def oneHotEncode(df,colNames):\n",
        "    for col in colNames:\n",
        "        if( df[col].dtype == np.dtype('object')):\n",
        "            dummies = pd.get_dummies(df[col],prefix=col)\n",
        "            df = pd.concat([df,dummies],axis=1)\n",
        "\n",
        "            #drop the encoded column\n",
        "            df.drop([col],axis = 1 , inplace=True)\n",
        "    return df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ng_BSoDwrr3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64691221-024a-4e95-fba0-6f89044c7601"
      },
      "cell_type": "code",
      "source": [
        "print('There were {} columns before encoding categorical features'.format(combined.shape[1]))\n",
        "combined = oneHotEncode(combined, cat_cols)\n",
        "print('There are {} columns after encoding categorical features'.format(combined.shape[1]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There were 45 columns before encoding categorical features\n",
            "There are 149 columns after encoding categorical features\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MosRPZPEtEgt"
      },
      "cell_type": "markdown",
      "source": [
        "Now, split back combined dataFrame to training data and test data "
      ]
    },
    {
      "metadata": {
        "id": "INvgC50TtPmQ"
      },
      "cell_type": "code",
      "source": [
        "def split_combined():\n",
        "    global combined\n",
        "    train = combined[:1460]\n",
        "    test = combined[1460:]\n",
        "\n",
        "    return train , test "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tZL6rBHUtYDy"
      },
      "cell_type": "code",
      "source": [
        "train, test = split_combined()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JPKYTh4wtinr"
      },
      "cell_type": "markdown",
      "source": [
        "## Second : Make the Deep Neural Network\n",
        " * Define a sequential model\n",
        " * Add some dense layers\n",
        " * Use '**relu**' as the activation function in the hidden layers\n",
        " * Use a '**normal**' initializer as the kernal_intializer \n",
        "           Initializers define the way to set the initial random weights of Keras layers.\n",
        " * We will use mean_absolute_error as a loss function\n",
        " * Define the output layer with only one node\n",
        " * Use 'linear 'as the activation function for the output layer\n",
        " \n"
      ]
    },
    {
      "metadata": {
        "id": "CsJsrFlIvmzf"
      },
      "cell_type": "code",
      "source": [
        "NN_model = Sequential()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XrZVKcMbwCcI"
      },
      "cell_type": "markdown",
      "source": [
        "**The Input Layer**"
      ]
    },
    {
      "metadata": {
        "id": "ILFBftZnvqFj"
      },
      "cell_type": "code",
      "source": [
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a6RW_lRRwG2i"
      },
      "cell_type": "markdown",
      "source": [
        "**The Hidden Layers**"
      ]
    },
    {
      "metadata": {
        "id": "Yz61h9vLv7xz"
      },
      "cell_type": "code",
      "source": [
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CHSMp0zJwKRc"
      },
      "cell_type": "markdown",
      "source": [
        "**The Output Layer**"
      ]
    },
    {
      "metadata": {
        "id": "S9v_kTrsv-38"
      },
      "cell_type": "code",
      "source": [
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pHpg10glxNam"
      },
      "cell_type": "markdown",
      "source": [
        "**Compile the network**"
      ]
    },
    {
      "metadata": {
        "id": "ROXr07cAv-pW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37450a52-0d9e-4c0d-e7f5-3be4c5b88a7e"
      },
      "cell_type": "code",
      "source": [
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               19200     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 184,065\n",
            "Trainable params: 184,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "zMirCFkrxUA8"
      },
      "cell_type": "markdown",
      "source": [
        "**Define a checkpoint callback :**"
      ]
    },
    {
      "metadata": {
        "id": "P4kZNz7HxafP"
      },
      "cell_type": "code",
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYgT8VgWxicp"
      },
      "cell_type": "markdown",
      "source": [
        "## Third : Train the model :"
      ]
    },
    {
      "metadata": {
        "id": "rGEt2nCHzMZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd866642-ef7f-44ff-dd06-2b4234acd406"
      },
      "cell_type": "code",
      "source": [
        "NN_model.fit(train, target, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 145956.1562 - mean_absolute_error: 145956.1562\n",
            "Epoch 00001: val_loss improved from inf to 52594.89844, saving model to Weights-001--52594.89844.hdf5\n",
            "37/37 [==============================] - 3s 16ms/step - loss: 130323.3984 - mean_absolute_error: 130323.3984 - val_loss: 52594.8984 - val_mean_absolute_error: 52594.8984\n",
            "Epoch 2/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 53774.9102 - mean_absolute_error: 53774.9102\n",
            "Epoch 00002: val_loss improved from 52594.89844 to 49475.92578, saving model to Weights-002--49475.92578.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 52504.8008 - mean_absolute_error: 52504.8008 - val_loss: 49475.9258 - val_mean_absolute_error: 49475.9258\n",
            "Epoch 3/500\n",
            "25/37 [===================>..........] - ETA: 0s - loss: 46321.6211 - mean_absolute_error: 46321.6211\n",
            "Epoch 00003: val_loss improved from 49475.92578 to 42892.73438, saving model to Weights-003--42892.73438.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 44343.9375 - mean_absolute_error: 44343.9375 - val_loss: 42892.7344 - val_mean_absolute_error: 42892.7344\n",
            "Epoch 4/500\n",
            "26/37 [====================>.........] - ETA: 0s - loss: 41185.8828 - mean_absolute_error: 41185.8828\n",
            "Epoch 00004: val_loss improved from 42892.73438 to 40430.15234, saving model to Weights-004--40430.15234.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 41176.1719 - mean_absolute_error: 41176.1719 - val_loss: 40430.1523 - val_mean_absolute_error: 40430.1523\n",
            "Epoch 5/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 36895.0000 - mean_absolute_error: 36895.0000\n",
            "Epoch 00005: val_loss improved from 40430.15234 to 39291.26562, saving model to Weights-005--39291.26562.hdf5\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 36968.2031 - mean_absolute_error: 36968.2031 - val_loss: 39291.2656 - val_mean_absolute_error: 39291.2656\n",
            "Epoch 6/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 34993.9023 - mean_absolute_error: 34993.9023\n",
            "Epoch 00006: val_loss improved from 39291.26562 to 39144.05469, saving model to Weights-006--39144.05469.hdf5\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 34858.6289 - mean_absolute_error: 34858.6289 - val_loss: 39144.0547 - val_mean_absolute_error: 39144.0547\n",
            "Epoch 7/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 35203.8984 - mean_absolute_error: 35203.8984\n",
            "Epoch 00007: val_loss did not improve from 39144.05469\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 34554.7031 - mean_absolute_error: 34554.7031 - val_loss: 39328.1172 - val_mean_absolute_error: 39328.1172\n",
            "Epoch 8/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 33334.3828 - mean_absolute_error: 33334.3828\n",
            "Epoch 00008: val_loss improved from 39144.05469 to 37500.45312, saving model to Weights-008--37500.45312.hdf5\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 32892.5703 - mean_absolute_error: 32892.5703 - val_loss: 37500.4531 - val_mean_absolute_error: 37500.4531\n",
            "Epoch 9/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 33955.0586 - mean_absolute_error: 33955.0586\n",
            "Epoch 00009: val_loss improved from 37500.45312 to 35309.94531, saving model to Weights-009--35309.94531.hdf5\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 32667.5312 - mean_absolute_error: 32667.5312 - val_loss: 35309.9453 - val_mean_absolute_error: 35309.9453\n",
            "Epoch 10/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 33284.3125 - mean_absolute_error: 33284.3125\n",
            "Epoch 00010: val_loss improved from 35309.94531 to 35105.34766, saving model to Weights-010--35105.34766.hdf5\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 33082.6211 - mean_absolute_error: 33082.6211 - val_loss: 35105.3477 - val_mean_absolute_error: 35105.3477\n",
            "Epoch 11/500\n",
            "26/37 [====================>.........] - ETA: 0s - loss: 32539.2402 - mean_absolute_error: 32539.2402\n",
            "Epoch 00011: val_loss improved from 35105.34766 to 34760.94531, saving model to Weights-011--34760.94531.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 32421.8672 - mean_absolute_error: 32421.8672 - val_loss: 34760.9453 - val_mean_absolute_error: 34760.9453\n",
            "Epoch 12/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 31377.8652 - mean_absolute_error: 31377.8652\n",
            "Epoch 00012: val_loss did not improve from 34760.94531\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 32672.2285 - mean_absolute_error: 32672.2285 - val_loss: 35121.4609 - val_mean_absolute_error: 35121.4609\n",
            "Epoch 13/500\n",
            "25/37 [===================>..........] - ETA: 0s - loss: 33154.3086 - mean_absolute_error: 33154.3086\n",
            "Epoch 00013: val_loss did not improve from 34760.94531\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 32556.9180 - mean_absolute_error: 32556.9180 - val_loss: 35649.6836 - val_mean_absolute_error: 35649.6836\n",
            "Epoch 14/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 32873.0508 - mean_absolute_error: 32873.0508\n",
            "Epoch 00014: val_loss did not improve from 34760.94531\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 32420.9688 - mean_absolute_error: 32420.9688 - val_loss: 35441.4062 - val_mean_absolute_error: 35441.4062\n",
            "Epoch 15/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 32777.2930 - mean_absolute_error: 32777.2930\n",
            "Epoch 00015: val_loss did not improve from 34760.94531\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 31820.3691 - mean_absolute_error: 31820.3691 - val_loss: 34907.8906 - val_mean_absolute_error: 34907.8906\n",
            "Epoch 16/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 33261.6484 - mean_absolute_error: 33261.6484\n",
            "Epoch 00016: val_loss did not improve from 34760.94531\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 32786.1133 - mean_absolute_error: 32786.1133 - val_loss: 35291.2188 - val_mean_absolute_error: 35291.2188\n",
            "Epoch 17/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 33084.6094 - mean_absolute_error: 33084.6094\n",
            "Epoch 00017: val_loss improved from 34760.94531 to 34135.57422, saving model to Weights-017--34135.57422.hdf5\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 32185.5508 - mean_absolute_error: 32185.5508 - val_loss: 34135.5742 - val_mean_absolute_error: 34135.5742\n",
            "Epoch 18/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 31809.6855 - mean_absolute_error: 31809.6855\n",
            "Epoch 00018: val_loss improved from 34135.57422 to 34095.35938, saving model to Weights-018--34095.35938.hdf5\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 31444.0723 - mean_absolute_error: 31444.0723 - val_loss: 34095.3594 - val_mean_absolute_error: 34095.3594\n",
            "Epoch 19/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 31047.2910 - mean_absolute_error: 31047.2910\n",
            "Epoch 00019: val_loss did not improve from 34095.35938\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 31154.0352 - mean_absolute_error: 31154.0352 - val_loss: 37016.9180 - val_mean_absolute_error: 37016.9180\n",
            "Epoch 20/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 31601.2422 - mean_absolute_error: 31601.2422\n",
            "Epoch 00020: val_loss did not improve from 34095.35938\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 32410.5918 - mean_absolute_error: 32410.5918 - val_loss: 34394.5781 - val_mean_absolute_error: 34394.5781\n",
            "Epoch 21/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 32666.7969 - mean_absolute_error: 32666.7969\n",
            "Epoch 00021: val_loss did not improve from 34095.35938\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 32290.7422 - mean_absolute_error: 32290.7422 - val_loss: 34427.0117 - val_mean_absolute_error: 34427.0117\n",
            "Epoch 22/500\n",
            "26/37 [====================>.........] - ETA: 0s - loss: 31957.2852 - mean_absolute_error: 31957.2852\n",
            "Epoch 00022: val_loss did not improve from 34095.35938\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 31777.0273 - mean_absolute_error: 31777.0273 - val_loss: 34107.5781 - val_mean_absolute_error: 34107.5781\n",
            "Epoch 23/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 31568.0195 - mean_absolute_error: 31568.0195\n",
            "Epoch 00023: val_loss improved from 34095.35938 to 33679.26562, saving model to Weights-023--33679.26562.hdf5\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 30965.4316 - mean_absolute_error: 30965.4316 - val_loss: 33679.2656 - val_mean_absolute_error: 33679.2656\n",
            "Epoch 24/500\n",
            "26/37 [====================>.........] - ETA: 0s - loss: 31152.1074 - mean_absolute_error: 31152.1074\n",
            "Epoch 00024: val_loss did not improve from 33679.26562\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 30956.4785 - mean_absolute_error: 30956.4785 - val_loss: 34376.4023 - val_mean_absolute_error: 34376.4023\n",
            "Epoch 25/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 30875.8125 - mean_absolute_error: 30875.8125\n",
            "Epoch 00025: val_loss did not improve from 33679.26562\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 31612.6602 - mean_absolute_error: 31612.6602 - val_loss: 33920.5039 - val_mean_absolute_error: 33920.5039\n",
            "Epoch 26/500\n",
            "25/37 [===================>..........] - ETA: 0s - loss: 31346.8945 - mean_absolute_error: 31346.8945\n",
            "Epoch 00026: val_loss improved from 33679.26562 to 33600.33203, saving model to Weights-026--33600.33203.hdf5\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 32098.1719 - mean_absolute_error: 32098.1719 - val_loss: 33600.3320 - val_mean_absolute_error: 33600.3320\n",
            "Epoch 27/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 30605.1738 - mean_absolute_error: 30605.1738\n",
            "Epoch 00027: val_loss did not improve from 33600.33203\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 31047.1777 - mean_absolute_error: 31047.1777 - val_loss: 33862.4922 - val_mean_absolute_error: 33862.4922\n",
            "Epoch 28/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 30977.1758 - mean_absolute_error: 30977.1758\n",
            "Epoch 00028: val_loss did not improve from 33600.33203\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 30882.2148 - mean_absolute_error: 30882.2148 - val_loss: 34128.5508 - val_mean_absolute_error: 34128.5508\n",
            "Epoch 29/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 31174.7773 - mean_absolute_error: 31174.7773\n",
            "Epoch 00029: val_loss did not improve from 33600.33203\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 30966.1543 - mean_absolute_error: 30966.1543 - val_loss: 33692.7344 - val_mean_absolute_error: 33692.7344\n",
            "Epoch 30/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 30736.3711 - mean_absolute_error: 30736.3711\n",
            "Epoch 00030: val_loss did not improve from 33600.33203\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 30711.4512 - mean_absolute_error: 30711.4512 - val_loss: 33855.2539 - val_mean_absolute_error: 33855.2539\n",
            "Epoch 31/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 33231.2422 - mean_absolute_error: 33231.2422\n",
            "Epoch 00031: val_loss did not improve from 33600.33203\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 32266.1875 - mean_absolute_error: 32266.1875 - val_loss: 33883.5352 - val_mean_absolute_error: 33883.5352\n",
            "Epoch 32/500\n",
            "25/37 [===================>..........] - ETA: 0s - loss: 30510.7598 - mean_absolute_error: 30510.7598\n",
            "Epoch 00032: val_loss did not improve from 33600.33203\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 31010.1270 - mean_absolute_error: 31010.1270 - val_loss: 37043.2695 - val_mean_absolute_error: 37043.2695\n",
            "Epoch 33/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 30872.5508 - mean_absolute_error: 30872.5508\n",
            "Epoch 00033: val_loss did not improve from 33600.33203\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 30626.8184 - mean_absolute_error: 30626.8184 - val_loss: 37050.5273 - val_mean_absolute_error: 37050.5273\n",
            "Epoch 34/500\n",
            "25/37 [===================>..........] - ETA: 0s - loss: 29418.9043 - mean_absolute_error: 29418.9043\n",
            "Epoch 00034: val_loss improved from 33600.33203 to 33593.42188, saving model to Weights-034--33593.42188.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 30564.0000 - mean_absolute_error: 30564.0000 - val_loss: 33593.4219 - val_mean_absolute_error: 33593.4219\n",
            "Epoch 35/500\n",
            "26/37 [====================>.........] - ETA: 0s - loss: 29774.6289 - mean_absolute_error: 29774.6289\n",
            "Epoch 00035: val_loss did not improve from 33593.42188\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 30235.9043 - mean_absolute_error: 30235.9043 - val_loss: 33600.8555 - val_mean_absolute_error: 33600.8555\n",
            "Epoch 36/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 30342.3984 - mean_absolute_error: 30342.3984\n",
            "Epoch 00036: val_loss improved from 33593.42188 to 33547.75000, saving model to Weights-036--33547.75000.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 30236.1875 - mean_absolute_error: 30236.1875 - val_loss: 33547.7500 - val_mean_absolute_error: 33547.7500\n",
            "Epoch 37/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 30855.2031 - mean_absolute_error: 30855.2031\n",
            "Epoch 00037: val_loss improved from 33547.75000 to 33334.93359, saving model to Weights-037--33334.93359.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 30727.8398 - mean_absolute_error: 30727.8398 - val_loss: 33334.9336 - val_mean_absolute_error: 33334.9336\n",
            "Epoch 38/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 29670.2168 - mean_absolute_error: 29670.2168\n",
            "Epoch 00038: val_loss did not improve from 33334.93359\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 30365.7324 - mean_absolute_error: 30365.7324 - val_loss: 36059.8438 - val_mean_absolute_error: 36059.8438\n",
            "Epoch 39/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 31355.3965 - mean_absolute_error: 31355.3965\n",
            "Epoch 00039: val_loss improved from 33334.93359 to 33257.28516, saving model to Weights-039--33257.28516.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 31185.6582 - mean_absolute_error: 31185.6582 - val_loss: 33257.2852 - val_mean_absolute_error: 33257.2852\n",
            "Epoch 40/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 29659.9082 - mean_absolute_error: 29659.9082\n",
            "Epoch 00040: val_loss did not improve from 33257.28516\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 29826.8672 - mean_absolute_error: 29826.8672 - val_loss: 34141.9023 - val_mean_absolute_error: 34141.9023\n",
            "Epoch 41/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 30145.9375 - mean_absolute_error: 30145.9375\n",
            "Epoch 00041: val_loss improved from 33257.28516 to 32999.16016, saving model to Weights-041--32999.16016.hdf5\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 30089.6719 - mean_absolute_error: 30089.6719 - val_loss: 32999.1602 - val_mean_absolute_error: 32999.1602\n",
            "Epoch 42/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 30357.9727 - mean_absolute_error: 30357.9727\n",
            "Epoch 00042: val_loss improved from 32999.16016 to 32511.83203, saving model to Weights-042--32511.83203.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 29381.4512 - mean_absolute_error: 29381.4512 - val_loss: 32511.8320 - val_mean_absolute_error: 32511.8320\n",
            "Epoch 43/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 29279.0488 - mean_absolute_error: 29279.0488\n",
            "Epoch 00043: val_loss did not improve from 32511.83203\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 29279.0488 - mean_absolute_error: 29279.0488 - val_loss: 32590.8008 - val_mean_absolute_error: 32590.8008\n",
            "Epoch 44/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 30246.9961 - mean_absolute_error: 30246.9961\n",
            "Epoch 00044: val_loss did not improve from 32511.83203\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 29583.4453 - mean_absolute_error: 29583.4453 - val_loss: 33796.3477 - val_mean_absolute_error: 33796.3477\n",
            "Epoch 45/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 28498.3945 - mean_absolute_error: 28498.3945\n",
            "Epoch 00045: val_loss did not improve from 32511.83203\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 29121.3086 - mean_absolute_error: 29121.3086 - val_loss: 33747.1680 - val_mean_absolute_error: 33747.1680\n",
            "Epoch 46/500\n",
            "26/37 [====================>.........] - ETA: 0s - loss: 29045.6016 - mean_absolute_error: 29045.6016\n",
            "Epoch 00046: val_loss did not improve from 32511.83203\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 29215.5410 - mean_absolute_error: 29215.5410 - val_loss: 35472.9844 - val_mean_absolute_error: 35472.9844\n",
            "Epoch 47/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 30390.6797 - mean_absolute_error: 30390.6797\n",
            "Epoch 00047: val_loss did not improve from 32511.83203\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 29768.6191 - mean_absolute_error: 29768.6191 - val_loss: 32683.0039 - val_mean_absolute_error: 32683.0039\n",
            "Epoch 48/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 30811.9004 - mean_absolute_error: 30811.9004\n",
            "Epoch 00048: val_loss did not improve from 32511.83203\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 30590.3809 - mean_absolute_error: 30590.3809 - val_loss: 33412.2227 - val_mean_absolute_error: 33412.2227\n",
            "Epoch 49/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 28569.1660 - mean_absolute_error: 28569.1660\n",
            "Epoch 00049: val_loss did not improve from 32511.83203\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 28968.2012 - mean_absolute_error: 28968.2012 - val_loss: 33533.4883 - val_mean_absolute_error: 33533.4883\n",
            "Epoch 50/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 30012.6621 - mean_absolute_error: 30012.6621\n",
            "Epoch 00050: val_loss did not improve from 32511.83203\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 29489.5488 - mean_absolute_error: 29489.5488 - val_loss: 34922.0586 - val_mean_absolute_error: 34922.0586\n",
            "Epoch 51/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 29389.7676 - mean_absolute_error: 29389.7676\n",
            "Epoch 00051: val_loss did not improve from 32511.83203\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 29222.1367 - mean_absolute_error: 29222.1367 - val_loss: 33161.4023 - val_mean_absolute_error: 33161.4023\n",
            "Epoch 52/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 29547.9160 - mean_absolute_error: 29547.9160\n",
            "Epoch 00052: val_loss improved from 32511.83203 to 32070.09570, saving model to Weights-052--32070.09570.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 29116.4082 - mean_absolute_error: 29116.4082 - val_loss: 32070.0957 - val_mean_absolute_error: 32070.0957\n",
            "Epoch 53/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 30359.5527 - mean_absolute_error: 30359.5527\n",
            "Epoch 00053: val_loss did not improve from 32070.09570\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 29655.4922 - mean_absolute_error: 29655.4922 - val_loss: 32430.2949 - val_mean_absolute_error: 32430.2949\n",
            "Epoch 54/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 29803.0586 - mean_absolute_error: 29803.0586\n",
            "Epoch 00054: val_loss did not improve from 32070.09570\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 29769.5078 - mean_absolute_error: 29769.5078 - val_loss: 33202.6797 - val_mean_absolute_error: 33202.6797\n",
            "Epoch 55/500\n",
            "26/37 [====================>.........] - ETA: 0s - loss: 28558.8770 - mean_absolute_error: 28558.8770\n",
            "Epoch 00055: val_loss did not improve from 32070.09570\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 28897.3691 - mean_absolute_error: 28897.3691 - val_loss: 33440.8789 - val_mean_absolute_error: 33440.8789\n",
            "Epoch 56/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 28735.0000 - mean_absolute_error: 28735.0000\n",
            "Epoch 00056: val_loss did not improve from 32070.09570\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 28799.9355 - mean_absolute_error: 28799.9355 - val_loss: 32455.6582 - val_mean_absolute_error: 32455.6582\n",
            "Epoch 57/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 28436.6777 - mean_absolute_error: 28436.6777\n",
            "Epoch 00057: val_loss did not improve from 32070.09570\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 28448.2812 - mean_absolute_error: 28448.2812 - val_loss: 35836.0117 - val_mean_absolute_error: 35836.0117\n",
            "Epoch 58/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 28611.8867 - mean_absolute_error: 28611.8867\n",
            "Epoch 00058: val_loss improved from 32070.09570 to 31806.20117, saving model to Weights-058--31806.20117.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 28467.7812 - mean_absolute_error: 28467.7812 - val_loss: 31806.2012 - val_mean_absolute_error: 31806.2012\n",
            "Epoch 59/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 28286.7285 - mean_absolute_error: 28286.7285\n",
            "Epoch 00059: val_loss improved from 31806.20117 to 31435.55469, saving model to Weights-059--31435.55469.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 28078.3633 - mean_absolute_error: 28078.3633 - val_loss: 31435.5547 - val_mean_absolute_error: 31435.5547\n",
            "Epoch 60/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 28094.5430 - mean_absolute_error: 28094.5430\n",
            "Epoch 00060: val_loss did not improve from 31435.55469\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 28094.5430 - mean_absolute_error: 28094.5430 - val_loss: 31442.0215 - val_mean_absolute_error: 31442.0215\n",
            "Epoch 61/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 28479.5723 - mean_absolute_error: 28479.5723\n",
            "Epoch 00061: val_loss did not improve from 31435.55469\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 28784.3145 - mean_absolute_error: 28784.3145 - val_loss: 31563.0078 - val_mean_absolute_error: 31563.0078\n",
            "Epoch 62/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 28176.1758 - mean_absolute_error: 28176.1758\n",
            "Epoch 00062: val_loss improved from 31435.55469 to 30866.74023, saving model to Weights-062--30866.74023.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 28176.1758 - mean_absolute_error: 28176.1758 - val_loss: 30866.7402 - val_mean_absolute_error: 30866.7402\n",
            "Epoch 63/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 27547.8418 - mean_absolute_error: 27547.8418\n",
            "Epoch 00063: val_loss did not improve from 30866.74023\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 27591.4102 - mean_absolute_error: 27591.4102 - val_loss: 31039.3008 - val_mean_absolute_error: 31039.3008\n",
            "Epoch 64/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 29071.8574 - mean_absolute_error: 29071.8574\n",
            "Epoch 00064: val_loss did not improve from 30866.74023\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 28896.8125 - mean_absolute_error: 28896.8125 - val_loss: 33893.7930 - val_mean_absolute_error: 33893.7930\n",
            "Epoch 65/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 28420.3027 - mean_absolute_error: 28420.3027\n",
            "Epoch 00065: val_loss did not improve from 30866.74023\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 28801.6270 - mean_absolute_error: 28801.6270 - val_loss: 33004.4375 - val_mean_absolute_error: 33004.4375\n",
            "Epoch 66/500\n",
            "26/37 [====================>.........] - ETA: 0s - loss: 28055.4902 - mean_absolute_error: 28055.4902\n",
            "Epoch 00066: val_loss did not improve from 30866.74023\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 28878.9766 - mean_absolute_error: 28878.9766 - val_loss: 31732.0000 - val_mean_absolute_error: 31732.0000\n",
            "Epoch 67/500\n",
            "25/37 [===================>..........] - ETA: 0s - loss: 28105.3867 - mean_absolute_error: 28105.3867\n",
            "Epoch 00067: val_loss did not improve from 30866.74023\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 28310.4434 - mean_absolute_error: 28310.4434 - val_loss: 31215.7402 - val_mean_absolute_error: 31215.7402\n",
            "Epoch 68/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 27079.5488 - mean_absolute_error: 27079.5488\n",
            "Epoch 00068: val_loss did not improve from 30866.74023\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 27204.6934 - mean_absolute_error: 27204.6934 - val_loss: 30958.0508 - val_mean_absolute_error: 30958.0508\n",
            "Epoch 69/500\n",
            "26/37 [====================>.........] - ETA: 0s - loss: 27140.4023 - mean_absolute_error: 27140.4023\n",
            "Epoch 00069: val_loss did not improve from 30866.74023\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 26984.4844 - mean_absolute_error: 26984.4844 - val_loss: 30941.8457 - val_mean_absolute_error: 30941.8457\n",
            "Epoch 70/500\n",
            "25/37 [===================>..........] - ETA: 0s - loss: 25900.6055 - mean_absolute_error: 25900.6055\n",
            "Epoch 00070: val_loss did not improve from 30866.74023\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 26698.9902 - mean_absolute_error: 26698.9902 - val_loss: 32568.1680 - val_mean_absolute_error: 32568.1680\n",
            "Epoch 71/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 26940.3457 - mean_absolute_error: 26940.3457\n",
            "Epoch 00071: val_loss improved from 30866.74023 to 30421.92773, saving model to Weights-071--30421.92773.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 26724.5332 - mean_absolute_error: 26724.5332 - val_loss: 30421.9277 - val_mean_absolute_error: 30421.9277\n",
            "Epoch 72/500\n",
            "26/37 [====================>.........] - ETA: 0s - loss: 27500.1445 - mean_absolute_error: 27500.1445\n",
            "Epoch 00072: val_loss improved from 30421.92773 to 29962.39453, saving model to Weights-072--29962.39453.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 27573.2793 - mean_absolute_error: 27573.2793 - val_loss: 29962.3945 - val_mean_absolute_error: 29962.3945\n",
            "Epoch 73/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 26763.4180 - mean_absolute_error: 26763.4180\n",
            "Epoch 00073: val_loss did not improve from 29962.39453\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 26763.4180 - mean_absolute_error: 26763.4180 - val_loss: 31406.1582 - val_mean_absolute_error: 31406.1582\n",
            "Epoch 74/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 26487.1699 - mean_absolute_error: 26487.1699\n",
            "Epoch 00074: val_loss improved from 29962.39453 to 29336.84570, saving model to Weights-074--29336.84570.hdf5\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 26154.2578 - mean_absolute_error: 26154.2578 - val_loss: 29336.8457 - val_mean_absolute_error: 29336.8457\n",
            "Epoch 75/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 25792.8262 - mean_absolute_error: 25792.8262\n",
            "Epoch 00075: val_loss did not improve from 29336.84570\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 25792.8262 - mean_absolute_error: 25792.8262 - val_loss: 29697.5488 - val_mean_absolute_error: 29697.5488\n",
            "Epoch 76/500\n",
            "26/37 [====================>.........] - ETA: 0s - loss: 26346.1543 - mean_absolute_error: 26346.1543\n",
            "Epoch 00076: val_loss did not improve from 29336.84570\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 26308.3613 - mean_absolute_error: 26308.3613 - val_loss: 31729.0234 - val_mean_absolute_error: 31729.0234\n",
            "Epoch 77/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 26323.1445 - mean_absolute_error: 26323.1445\n",
            "Epoch 00077: val_loss did not improve from 29336.84570\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 26188.3203 - mean_absolute_error: 26188.3203 - val_loss: 34502.6445 - val_mean_absolute_error: 34502.6445\n",
            "Epoch 78/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 26745.1934 - mean_absolute_error: 26745.1934\n",
            "Epoch 00078: val_loss improved from 29336.84570 to 29146.71289, saving model to Weights-078--29146.71289.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 26745.1934 - mean_absolute_error: 26745.1934 - val_loss: 29146.7129 - val_mean_absolute_error: 29146.7129\n",
            "Epoch 79/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 25559.7676 - mean_absolute_error: 25559.7676\n",
            "Epoch 00079: val_loss did not improve from 29146.71289\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 25559.7676 - mean_absolute_error: 25559.7676 - val_loss: 31080.3418 - val_mean_absolute_error: 31080.3418\n",
            "Epoch 80/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 25277.7930 - mean_absolute_error: 25277.7930\n",
            "Epoch 00080: val_loss did not improve from 29146.71289\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 25200.0391 - mean_absolute_error: 25200.0391 - val_loss: 31457.7578 - val_mean_absolute_error: 31457.7578\n",
            "Epoch 81/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 27439.9863 - mean_absolute_error: 27439.9863\n",
            "Epoch 00081: val_loss did not improve from 29146.71289\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 27439.9863 - mean_absolute_error: 27439.9863 - val_loss: 31175.4629 - val_mean_absolute_error: 31175.4629\n",
            "Epoch 82/500\n",
            "25/37 [===================>..........] - ETA: 0s - loss: 25001.6680 - mean_absolute_error: 25001.6680\n",
            "Epoch 00082: val_loss improved from 29146.71289 to 27608.97461, saving model to Weights-082--27608.97461.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 25200.1660 - mean_absolute_error: 25200.1660 - val_loss: 27608.9746 - val_mean_absolute_error: 27608.9746\n",
            "Epoch 83/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 25645.8711 - mean_absolute_error: 25645.8711\n",
            "Epoch 00083: val_loss did not improve from 27608.97461\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 25900.8828 - mean_absolute_error: 25900.8828 - val_loss: 31699.3457 - val_mean_absolute_error: 31699.3457\n",
            "Epoch 84/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 25963.9883 - mean_absolute_error: 25963.9883\n",
            "Epoch 00084: val_loss did not improve from 27608.97461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 25963.9883 - mean_absolute_error: 25963.9883 - val_loss: 30700.5098 - val_mean_absolute_error: 30700.5098\n",
            "Epoch 85/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 26018.8555 - mean_absolute_error: 26018.8555\n",
            "Epoch 00085: val_loss did not improve from 27608.97461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 26018.8555 - mean_absolute_error: 26018.8555 - val_loss: 28839.3965 - val_mean_absolute_error: 28839.3965\n",
            "Epoch 86/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 24751.8184 - mean_absolute_error: 24751.8184\n",
            "Epoch 00086: val_loss did not improve from 27608.97461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 24675.9824 - mean_absolute_error: 24675.9824 - val_loss: 33868.8555 - val_mean_absolute_error: 33868.8555\n",
            "Epoch 87/500\n",
            "25/37 [===================>..........] - ETA: 0s - loss: 26341.3516 - mean_absolute_error: 26341.3516\n",
            "Epoch 00087: val_loss did not improve from 27608.97461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 26022.8242 - mean_absolute_error: 26022.8242 - val_loss: 27692.0215 - val_mean_absolute_error: 27692.0215\n",
            "Epoch 88/500\n",
            "26/37 [====================>.........] - ETA: 0s - loss: 26254.0586 - mean_absolute_error: 26254.0586\n",
            "Epoch 00088: val_loss did not improve from 27608.97461\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 25316.7656 - mean_absolute_error: 25316.7656 - val_loss: 27968.5742 - val_mean_absolute_error: 27968.5742\n",
            "Epoch 89/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 24199.6484 - mean_absolute_error: 24199.6484\n",
            "Epoch 00089: val_loss did not improve from 27608.97461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 24398.9609 - mean_absolute_error: 24398.9609 - val_loss: 28866.2949 - val_mean_absolute_error: 28866.2949\n",
            "Epoch 90/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 24965.8535 - mean_absolute_error: 24965.8535\n",
            "Epoch 00090: val_loss improved from 27608.97461 to 26967.06250, saving model to Weights-090--26967.06250.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 25301.3984 - mean_absolute_error: 25301.3984 - val_loss: 26967.0625 - val_mean_absolute_error: 26967.0625\n",
            "Epoch 91/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 22354.4766 - mean_absolute_error: 22354.4766\n",
            "Epoch 00091: val_loss did not improve from 26967.06250\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 22609.3379 - mean_absolute_error: 22609.3379 - val_loss: 30661.0996 - val_mean_absolute_error: 30661.0996\n",
            "Epoch 92/500\n",
            "26/37 [====================>.........] - ETA: 0s - loss: 22990.3984 - mean_absolute_error: 22990.3984\n",
            "Epoch 00092: val_loss did not improve from 26967.06250\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 22847.8555 - mean_absolute_error: 22847.8555 - val_loss: 28458.5996 - val_mean_absolute_error: 28458.5996\n",
            "Epoch 93/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 22557.4688 - mean_absolute_error: 22557.4688\n",
            "Epoch 00093: val_loss improved from 26967.06250 to 26277.49609, saving model to Weights-093--26277.49609.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 22557.4688 - mean_absolute_error: 22557.4688 - val_loss: 26277.4961 - val_mean_absolute_error: 26277.4961\n",
            "Epoch 94/500\n",
            "25/37 [===================>..........] - ETA: 0s - loss: 21919.5352 - mean_absolute_error: 21919.5352\n",
            "Epoch 00094: val_loss improved from 26277.49609 to 25561.43164, saving model to Weights-094--25561.43164.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 22444.0215 - mean_absolute_error: 22444.0215 - val_loss: 25561.4316 - val_mean_absolute_error: 25561.4316\n",
            "Epoch 95/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 24084.2734 - mean_absolute_error: 24084.2734\n",
            "Epoch 00095: val_loss did not improve from 25561.43164\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 24084.2734 - mean_absolute_error: 24084.2734 - val_loss: 26837.8516 - val_mean_absolute_error: 26837.8516\n",
            "Epoch 96/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 22387.4922 - mean_absolute_error: 22387.4922\n",
            "Epoch 00096: val_loss did not improve from 25561.43164\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 22387.4922 - mean_absolute_error: 22387.4922 - val_loss: 28586.8906 - val_mean_absolute_error: 28586.8906\n",
            "Epoch 97/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 24112.2793 - mean_absolute_error: 24112.2793\n",
            "Epoch 00097: val_loss did not improve from 25561.43164\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 23474.1113 - mean_absolute_error: 23474.1113 - val_loss: 25669.5410 - val_mean_absolute_error: 25669.5410\n",
            "Epoch 98/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 21961.9863 - mean_absolute_error: 21961.9863\n",
            "Epoch 00098: val_loss improved from 25561.43164 to 24907.85742, saving model to Weights-098--24907.85742.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 21818.4824 - mean_absolute_error: 21818.4824 - val_loss: 24907.8574 - val_mean_absolute_error: 24907.8574\n",
            "Epoch 99/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 22375.1133 - mean_absolute_error: 22375.1133\n",
            "Epoch 00099: val_loss improved from 24907.85742 to 24892.75977, saving model to Weights-099--24892.75977.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 22484.8203 - mean_absolute_error: 22484.8203 - val_loss: 24892.7598 - val_mean_absolute_error: 24892.7598\n",
            "Epoch 100/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 22812.5762 - mean_absolute_error: 22812.5762\n",
            "Epoch 00100: val_loss improved from 24892.75977 to 24814.64648, saving model to Weights-100--24814.64648.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 22812.5762 - mean_absolute_error: 22812.5762 - val_loss: 24814.6465 - val_mean_absolute_error: 24814.6465\n",
            "Epoch 101/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 21948.2070 - mean_absolute_error: 21948.2070\n",
            "Epoch 00101: val_loss did not improve from 24814.64648\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 21948.2070 - mean_absolute_error: 21948.2070 - val_loss: 31141.2773 - val_mean_absolute_error: 31141.2773\n",
            "Epoch 102/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 22373.7520 - mean_absolute_error: 22373.7520\n",
            "Epoch 00102: val_loss did not improve from 24814.64648\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 22289.7344 - mean_absolute_error: 22289.7344 - val_loss: 26828.8652 - val_mean_absolute_error: 26828.8652\n",
            "Epoch 103/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 24479.1836 - mean_absolute_error: 24479.1836\n",
            "Epoch 00103: val_loss did not improve from 24814.64648\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 24010.3027 - mean_absolute_error: 24010.3027 - val_loss: 29027.6172 - val_mean_absolute_error: 29027.6172\n",
            "Epoch 104/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 21331.3398 - mean_absolute_error: 21331.3398\n",
            "Epoch 00104: val_loss did not improve from 24814.64648\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 21736.2715 - mean_absolute_error: 21736.2715 - val_loss: 25485.3027 - val_mean_absolute_error: 25485.3027\n",
            "Epoch 105/500\n",
            "25/37 [===================>..........] - ETA: 0s - loss: 24482.2793 - mean_absolute_error: 24482.2793\n",
            "Epoch 00105: val_loss improved from 24814.64648 to 23968.50781, saving model to Weights-105--23968.50781.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 23796.9043 - mean_absolute_error: 23796.9043 - val_loss: 23968.5078 - val_mean_absolute_error: 23968.5078\n",
            "Epoch 106/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 21508.2246 - mean_absolute_error: 21508.2246\n",
            "Epoch 00106: val_loss did not improve from 23968.50781\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 21363.5430 - mean_absolute_error: 21363.5430 - val_loss: 24803.1484 - val_mean_absolute_error: 24803.1484\n",
            "Epoch 107/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 20748.7539 - mean_absolute_error: 20748.7539\n",
            "Epoch 00107: val_loss did not improve from 23968.50781\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 20696.2559 - mean_absolute_error: 20696.2559 - val_loss: 24355.1191 - val_mean_absolute_error: 24355.1191\n",
            "Epoch 108/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 20655.0840 - mean_absolute_error: 20655.0840\n",
            "Epoch 00108: val_loss did not improve from 23968.50781\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20626.0449 - mean_absolute_error: 20626.0449 - val_loss: 24531.5898 - val_mean_absolute_error: 24531.5898\n",
            "Epoch 109/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 21730.5332 - mean_absolute_error: 21730.5332\n",
            "Epoch 00109: val_loss improved from 23968.50781 to 23515.35352, saving model to Weights-109--23515.35352.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 21730.5332 - mean_absolute_error: 21730.5332 - val_loss: 23515.3535 - val_mean_absolute_error: 23515.3535\n",
            "Epoch 110/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 23902.8438 - mean_absolute_error: 23902.8438\n",
            "Epoch 00110: val_loss did not improve from 23515.35352\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 24366.3926 - mean_absolute_error: 24366.3926 - val_loss: 26069.0117 - val_mean_absolute_error: 26069.0117\n",
            "Epoch 111/500\n",
            "25/37 [===================>..........] - ETA: 0s - loss: 23295.1777 - mean_absolute_error: 23295.1777\n",
            "Epoch 00111: val_loss did not improve from 23515.35352\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 22311.7754 - mean_absolute_error: 22311.7754 - val_loss: 24591.6309 - val_mean_absolute_error: 24591.6309\n",
            "Epoch 112/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 21309.3008 - mean_absolute_error: 21309.3008\n",
            "Epoch 00112: val_loss did not improve from 23515.35352\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 21340.0918 - mean_absolute_error: 21340.0918 - val_loss: 23828.1895 - val_mean_absolute_error: 23828.1895\n",
            "Epoch 113/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 23291.6973 - mean_absolute_error: 23291.6973\n",
            "Epoch 00113: val_loss did not improve from 23515.35352\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 23042.5312 - mean_absolute_error: 23042.5312 - val_loss: 26903.9336 - val_mean_absolute_error: 26903.9336\n",
            "Epoch 114/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 21959.1719 - mean_absolute_error: 21959.1719\n",
            "Epoch 00114: val_loss did not improve from 23515.35352\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 21651.2598 - mean_absolute_error: 21651.2598 - val_loss: 24628.7500 - val_mean_absolute_error: 24628.7500\n",
            "Epoch 115/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 20466.5469 - mean_absolute_error: 20466.5469\n",
            "Epoch 00115: val_loss did not improve from 23515.35352\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 20874.5840 - mean_absolute_error: 20874.5840 - val_loss: 23721.8262 - val_mean_absolute_error: 23721.8262\n",
            "Epoch 116/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 20656.5625 - mean_absolute_error: 20656.5625\n",
            "Epoch 00116: val_loss did not improve from 23515.35352\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 20443.1504 - mean_absolute_error: 20443.1504 - val_loss: 24685.0332 - val_mean_absolute_error: 24685.0332\n",
            "Epoch 117/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 21675.0781 - mean_absolute_error: 21675.0781\n",
            "Epoch 00117: val_loss did not improve from 23515.35352\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 21687.7891 - mean_absolute_error: 21687.7891 - val_loss: 24582.9121 - val_mean_absolute_error: 24582.9121\n",
            "Epoch 118/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 20196.3398 - mean_absolute_error: 20196.3398\n",
            "Epoch 00118: val_loss did not improve from 23515.35352\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20410.6465 - mean_absolute_error: 20410.6465 - val_loss: 23894.8809 - val_mean_absolute_error: 23894.8809\n",
            "Epoch 119/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 20017.6562 - mean_absolute_error: 20017.6562\n",
            "Epoch 00119: val_loss did not improve from 23515.35352\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20029.3145 - mean_absolute_error: 20029.3145 - val_loss: 24125.9922 - val_mean_absolute_error: 24125.9922\n",
            "Epoch 120/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 22543.3770 - mean_absolute_error: 22543.3770\n",
            "Epoch 00120: val_loss did not improve from 23515.35352\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 22543.3770 - mean_absolute_error: 22543.3770 - val_loss: 30601.4961 - val_mean_absolute_error: 30601.4961\n",
            "Epoch 121/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 21410.3535 - mean_absolute_error: 21410.3535\n",
            "Epoch 00121: val_loss did not improve from 23515.35352\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 21436.8828 - mean_absolute_error: 21436.8828 - val_loss: 23970.5293 - val_mean_absolute_error: 23970.5293\n",
            "Epoch 122/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 20280.5566 - mean_absolute_error: 20280.5566\n",
            "Epoch 00122: val_loss improved from 23515.35352 to 23021.10547, saving model to Weights-122--23021.10547.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 20293.2129 - mean_absolute_error: 20293.2129 - val_loss: 23021.1055 - val_mean_absolute_error: 23021.1055\n",
            "Epoch 123/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 20151.6680 - mean_absolute_error: 20151.6680\n",
            "Epoch 00123: val_loss did not improve from 23021.10547\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20043.8105 - mean_absolute_error: 20043.8105 - val_loss: 27550.9512 - val_mean_absolute_error: 27550.9512\n",
            "Epoch 124/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 19803.4062 - mean_absolute_error: 19803.4062\n",
            "Epoch 00124: val_loss did not improve from 23021.10547\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19832.7051 - mean_absolute_error: 19832.7051 - val_loss: 24363.8320 - val_mean_absolute_error: 24363.8320\n",
            "Epoch 125/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 21044.9570 - mean_absolute_error: 21044.9570\n",
            "Epoch 00125: val_loss did not improve from 23021.10547\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20973.4043 - mean_absolute_error: 20973.4043 - val_loss: 27374.6855 - val_mean_absolute_error: 27374.6855\n",
            "Epoch 126/500\n",
            "25/37 [===================>..........] - ETA: 0s - loss: 21440.1309 - mean_absolute_error: 21440.1309\n",
            "Epoch 00126: val_loss did not improve from 23021.10547\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20629.2324 - mean_absolute_error: 20629.2324 - val_loss: 23438.8711 - val_mean_absolute_error: 23438.8711\n",
            "Epoch 127/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 18889.8438 - mean_absolute_error: 18889.8438\n",
            "Epoch 00127: val_loss improved from 23021.10547 to 22488.46875, saving model to Weights-127--22488.46875.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 19194.7852 - mean_absolute_error: 19194.7852 - val_loss: 22488.4688 - val_mean_absolute_error: 22488.4688\n",
            "Epoch 128/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 21233.4961 - mean_absolute_error: 21233.4961\n",
            "Epoch 00128: val_loss did not improve from 22488.46875\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 21519.0781 - mean_absolute_error: 21519.0781 - val_loss: 24661.7168 - val_mean_absolute_error: 24661.7168\n",
            "Epoch 129/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 22467.8105 - mean_absolute_error: 22467.8105\n",
            "Epoch 00129: val_loss did not improve from 22488.46875\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 22431.3457 - mean_absolute_error: 22431.3457 - val_loss: 26602.4238 - val_mean_absolute_error: 26602.4238\n",
            "Epoch 130/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 19067.4102 - mean_absolute_error: 19067.4102\n",
            "Epoch 00130: val_loss did not improve from 22488.46875\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 19126.2656 - mean_absolute_error: 19126.2656 - val_loss: 22715.8301 - val_mean_absolute_error: 22715.8301\n",
            "Epoch 131/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 19259.9863 - mean_absolute_error: 19259.9863\n",
            "Epoch 00131: val_loss did not improve from 22488.46875\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19148.4766 - mean_absolute_error: 19148.4766 - val_loss: 23485.0098 - val_mean_absolute_error: 23485.0098\n",
            "Epoch 132/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 19496.8457 - mean_absolute_error: 19496.8457\n",
            "Epoch 00132: val_loss did not improve from 22488.46875\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19525.5293 - mean_absolute_error: 19525.5293 - val_loss: 24094.6270 - val_mean_absolute_error: 24094.6270\n",
            "Epoch 133/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 20465.3984 - mean_absolute_error: 20465.3984\n",
            "Epoch 00133: val_loss did not improve from 22488.46875\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20554.7012 - mean_absolute_error: 20554.7012 - val_loss: 23087.0078 - val_mean_absolute_error: 23087.0078\n",
            "Epoch 134/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 20976.7617 - mean_absolute_error: 20976.7617\n",
            "Epoch 00134: val_loss improved from 22488.46875 to 22484.73828, saving model to Weights-134--22484.73828.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 20976.7617 - mean_absolute_error: 20976.7617 - val_loss: 22484.7383 - val_mean_absolute_error: 22484.7383\n",
            "Epoch 135/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 18630.7988 - mean_absolute_error: 18630.7988\n",
            "Epoch 00135: val_loss improved from 22484.73828 to 22276.03516, saving model to Weights-135--22276.03516.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 18630.7988 - mean_absolute_error: 18630.7988 - val_loss: 22276.0352 - val_mean_absolute_error: 22276.0352\n",
            "Epoch 136/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 20485.6074 - mean_absolute_error: 20485.6074\n",
            "Epoch 00136: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20520.7617 - mean_absolute_error: 20520.7617 - val_loss: 24106.0332 - val_mean_absolute_error: 24106.0332\n",
            "Epoch 137/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 22230.3438 - mean_absolute_error: 22230.3438\n",
            "Epoch 00137: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 23097.5645 - mean_absolute_error: 23097.5645 - val_loss: 32308.3125 - val_mean_absolute_error: 32308.3125\n",
            "Epoch 138/500\n",
            "26/37 [====================>.........] - ETA: 0s - loss: 24384.0723 - mean_absolute_error: 24384.0723\n",
            "Epoch 00138: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 22967.2012 - mean_absolute_error: 22967.2012 - val_loss: 23199.6816 - val_mean_absolute_error: 23199.6816\n",
            "Epoch 139/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 19384.2930 - mean_absolute_error: 19384.2930\n",
            "Epoch 00139: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19824.5938 - mean_absolute_error: 19824.5938 - val_loss: 24057.0352 - val_mean_absolute_error: 24057.0352\n",
            "Epoch 140/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 19788.7676 - mean_absolute_error: 19788.7676\n",
            "Epoch 00140: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19876.1133 - mean_absolute_error: 19876.1133 - val_loss: 22997.6309 - val_mean_absolute_error: 22997.6309\n",
            "Epoch 141/500\n",
            "26/37 [====================>.........] - ETA: 0s - loss: 19079.0195 - mean_absolute_error: 19079.0195\n",
            "Epoch 00141: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 19767.2188 - mean_absolute_error: 19767.2188 - val_loss: 26221.6641 - val_mean_absolute_error: 26221.6641\n",
            "Epoch 142/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 21046.6289 - mean_absolute_error: 21046.6289\n",
            "Epoch 00142: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 21710.4355 - mean_absolute_error: 21710.4355 - val_loss: 25041.8555 - val_mean_absolute_error: 25041.8555\n",
            "Epoch 143/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 19734.5449 - mean_absolute_error: 19734.5449\n",
            "Epoch 00143: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19517.6465 - mean_absolute_error: 19517.6465 - val_loss: 29514.2285 - val_mean_absolute_error: 29514.2285\n",
            "Epoch 144/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 20043.0547 - mean_absolute_error: 20043.0547\n",
            "Epoch 00144: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19938.9238 - mean_absolute_error: 19938.9238 - val_loss: 24353.6426 - val_mean_absolute_error: 24353.6426\n",
            "Epoch 145/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 19079.6504 - mean_absolute_error: 19079.6504\n",
            "Epoch 00145: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19079.6504 - mean_absolute_error: 19079.6504 - val_loss: 24818.6719 - val_mean_absolute_error: 24818.6719\n",
            "Epoch 146/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 18820.1445 - mean_absolute_error: 18820.1445\n",
            "Epoch 00146: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 18921.8691 - mean_absolute_error: 18921.8691 - val_loss: 22454.2617 - val_mean_absolute_error: 22454.2617\n",
            "Epoch 147/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 19729.0762 - mean_absolute_error: 19729.0762\n",
            "Epoch 00147: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20097.9609 - mean_absolute_error: 20097.9609 - val_loss: 25833.6973 - val_mean_absolute_error: 25833.6973\n",
            "Epoch 148/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 20065.3457 - mean_absolute_error: 20065.3457\n",
            "Epoch 00148: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 20103.5801 - mean_absolute_error: 20103.5801 - val_loss: 22901.6797 - val_mean_absolute_error: 22901.6797\n",
            "Epoch 149/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 20018.6816 - mean_absolute_error: 20018.6816\n",
            "Epoch 00149: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20210.3828 - mean_absolute_error: 20210.3828 - val_loss: 24291.4160 - val_mean_absolute_error: 24291.4160\n",
            "Epoch 150/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 19739.3457 - mean_absolute_error: 19739.3457\n",
            "Epoch 00150: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 19739.3457 - mean_absolute_error: 19739.3457 - val_loss: 25296.0684 - val_mean_absolute_error: 25296.0684\n",
            "Epoch 151/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 19066.4883 - mean_absolute_error: 19066.4883\n",
            "Epoch 00151: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 18737.3203 - mean_absolute_error: 18737.3203 - val_loss: 23729.3340 - val_mean_absolute_error: 23729.3340\n",
            "Epoch 152/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 20442.4531 - mean_absolute_error: 20442.4531\n",
            "Epoch 00152: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20549.0977 - mean_absolute_error: 20549.0977 - val_loss: 29387.2188 - val_mean_absolute_error: 29387.2188\n",
            "Epoch 153/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 19863.0781 - mean_absolute_error: 19863.0781\n",
            "Epoch 00153: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20145.7324 - mean_absolute_error: 20145.7324 - val_loss: 22450.2227 - val_mean_absolute_error: 22450.2227\n",
            "Epoch 154/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 19553.0020 - mean_absolute_error: 19553.0020\n",
            "Epoch 00154: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 19484.6660 - mean_absolute_error: 19484.6660 - val_loss: 28607.1523 - val_mean_absolute_error: 28607.1523\n",
            "Epoch 155/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 22084.2656 - mean_absolute_error: 22084.2656\n",
            "Epoch 00155: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 21724.7637 - mean_absolute_error: 21724.7637 - val_loss: 22688.4902 - val_mean_absolute_error: 22688.4902\n",
            "Epoch 156/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 18739.7207 - mean_absolute_error: 18739.7207\n",
            "Epoch 00156: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 19083.6738 - mean_absolute_error: 19083.6738 - val_loss: 27153.0000 - val_mean_absolute_error: 27153.0000\n",
            "Epoch 157/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 18468.2734 - mean_absolute_error: 18468.2734\n",
            "Epoch 00157: val_loss did not improve from 22276.03516\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 18340.3496 - mean_absolute_error: 18340.3496 - val_loss: 25887.7305 - val_mean_absolute_error: 25887.7305\n",
            "Epoch 158/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 17875.9590 - mean_absolute_error: 17875.9590\n",
            "Epoch 00158: val_loss improved from 22276.03516 to 22237.06836, saving model to Weights-158--22237.06836.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 17875.9590 - mean_absolute_error: 17875.9590 - val_loss: 22237.0684 - val_mean_absolute_error: 22237.0684\n",
            "Epoch 159/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 22255.7246 - mean_absolute_error: 22255.7246\n",
            "Epoch 00159: val_loss did not improve from 22237.06836\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 21821.6641 - mean_absolute_error: 21821.6641 - val_loss: 28167.1816 - val_mean_absolute_error: 28167.1816\n",
            "Epoch 160/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 21730.1680 - mean_absolute_error: 21730.1680\n",
            "Epoch 00160: val_loss did not improve from 22237.06836\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 21278.3828 - mean_absolute_error: 21278.3828 - val_loss: 25296.8828 - val_mean_absolute_error: 25296.8828\n",
            "Epoch 161/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 20109.3828 - mean_absolute_error: 20109.3828\n",
            "Epoch 00161: val_loss did not improve from 22237.06836\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20330.3125 - mean_absolute_error: 20330.3125 - val_loss: 27217.1797 - val_mean_absolute_error: 27217.1797\n",
            "Epoch 162/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 22345.0137 - mean_absolute_error: 22345.0137\n",
            "Epoch 00162: val_loss did not improve from 22237.06836\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 22219.8906 - mean_absolute_error: 22219.8906 - val_loss: 23052.6426 - val_mean_absolute_error: 23052.6426\n",
            "Epoch 163/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 18284.1230 - mean_absolute_error: 18284.1230\n",
            "Epoch 00163: val_loss did not improve from 22237.06836\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 18218.8574 - mean_absolute_error: 18218.8574 - val_loss: 23326.0664 - val_mean_absolute_error: 23326.0664\n",
            "Epoch 164/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 19078.0488 - mean_absolute_error: 19078.0488\n",
            "Epoch 00164: val_loss did not improve from 22237.06836\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 18797.7949 - mean_absolute_error: 18797.7949 - val_loss: 22408.8086 - val_mean_absolute_error: 22408.8086\n",
            "Epoch 165/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 20358.1816 - mean_absolute_error: 20358.1816\n",
            "Epoch 00165: val_loss did not improve from 22237.06836\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20162.3594 - mean_absolute_error: 20162.3594 - val_loss: 24851.8516 - val_mean_absolute_error: 24851.8516\n",
            "Epoch 166/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 18238.6934 - mean_absolute_error: 18238.6934\n",
            "Epoch 00166: val_loss improved from 22237.06836 to 22110.21680, saving model to Weights-166--22110.21680.hdf5\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 18259.7715 - mean_absolute_error: 18259.7715 - val_loss: 22110.2168 - val_mean_absolute_error: 22110.2168\n",
            "Epoch 167/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 19803.5195 - mean_absolute_error: 19803.5195\n",
            "Epoch 00167: val_loss did not improve from 22110.21680\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19670.9023 - mean_absolute_error: 19670.9023 - val_loss: 29180.7422 - val_mean_absolute_error: 29180.7422\n",
            "Epoch 168/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 18343.6289 - mean_absolute_error: 18343.6289\n",
            "Epoch 00168: val_loss did not improve from 22110.21680\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 18369.5566 - mean_absolute_error: 18369.5566 - val_loss: 24389.2422 - val_mean_absolute_error: 24389.2422\n",
            "Epoch 169/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 18153.5938 - mean_absolute_error: 18153.5938\n",
            "Epoch 00169: val_loss did not improve from 22110.21680\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 18177.0078 - mean_absolute_error: 18177.0078 - val_loss: 22474.0137 - val_mean_absolute_error: 22474.0137\n",
            "Epoch 170/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 18359.4199 - mean_absolute_error: 18359.4199\n",
            "Epoch 00170: val_loss did not improve from 22110.21680\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 18436.7227 - mean_absolute_error: 18436.7227 - val_loss: 22329.5547 - val_mean_absolute_error: 22329.5547\n",
            "Epoch 171/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 17928.7773 - mean_absolute_error: 17928.7773\n",
            "Epoch 00171: val_loss did not improve from 22110.21680\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 18018.7852 - mean_absolute_error: 18018.7852 - val_loss: 22390.1836 - val_mean_absolute_error: 22390.1836\n",
            "Epoch 172/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 18122.1602 - mean_absolute_error: 18122.1602\n",
            "Epoch 00172: val_loss improved from 22110.21680 to 22109.80273, saving model to Weights-172--22109.80273.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 18021.6465 - mean_absolute_error: 18021.6465 - val_loss: 22109.8027 - val_mean_absolute_error: 22109.8027\n",
            "Epoch 173/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 17718.2676 - mean_absolute_error: 17718.2676\n",
            "Epoch 00173: val_loss did not improve from 22109.80273\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 17766.7598 - mean_absolute_error: 17766.7598 - val_loss: 24265.9590 - val_mean_absolute_error: 24265.9590\n",
            "Epoch 174/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 18418.4219 - mean_absolute_error: 18418.4219\n",
            "Epoch 00174: val_loss improved from 22109.80273 to 22038.45117, saving model to Weights-174--22038.45117.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 18301.5723 - mean_absolute_error: 18301.5723 - val_loss: 22038.4512 - val_mean_absolute_error: 22038.4512\n",
            "Epoch 175/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 18509.2344 - mean_absolute_error: 18509.2344\n",
            "Epoch 00175: val_loss did not improve from 22038.45117\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 18232.1328 - mean_absolute_error: 18232.1328 - val_loss: 22883.3438 - val_mean_absolute_error: 22883.3438\n",
            "Epoch 176/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 18386.3418 - mean_absolute_error: 18386.3418\n",
            "Epoch 00176: val_loss did not improve from 22038.45117\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 18552.6406 - mean_absolute_error: 18552.6406 - val_loss: 23179.9336 - val_mean_absolute_error: 23179.9336\n",
            "Epoch 177/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 18362.0898 - mean_absolute_error: 18362.0898\n",
            "Epoch 00177: val_loss did not improve from 22038.45117\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 18413.1934 - mean_absolute_error: 18413.1934 - val_loss: 22096.9844 - val_mean_absolute_error: 22096.9844\n",
            "Epoch 178/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 18615.4121 - mean_absolute_error: 18615.4121\n",
            "Epoch 00178: val_loss did not improve from 22038.45117\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 18967.2168 - mean_absolute_error: 18967.2168 - val_loss: 22308.6055 - val_mean_absolute_error: 22308.6055\n",
            "Epoch 179/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 18147.9707 - mean_absolute_error: 18147.9707\n",
            "Epoch 00179: val_loss did not improve from 22038.45117\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 18156.1445 - mean_absolute_error: 18156.1445 - val_loss: 22693.6836 - val_mean_absolute_error: 22693.6836\n",
            "Epoch 180/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 17572.6602 - mean_absolute_error: 17572.6602\n",
            "Epoch 00180: val_loss did not improve from 22038.45117\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 17669.9668 - mean_absolute_error: 17669.9668 - val_loss: 24502.0449 - val_mean_absolute_error: 24502.0449\n",
            "Epoch 181/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 18478.3867 - mean_absolute_error: 18478.3867\n",
            "Epoch 00181: val_loss improved from 22038.45117 to 21743.94531, saving model to Weights-181--21743.94531.hdf5\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 18557.3477 - mean_absolute_error: 18557.3477 - val_loss: 21743.9453 - val_mean_absolute_error: 21743.9453\n",
            "Epoch 182/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 19337.5254 - mean_absolute_error: 19337.5254\n",
            "Epoch 00182: val_loss improved from 21743.94531 to 21728.69336, saving model to Weights-182--21728.69336.hdf5\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 18606.2949 - mean_absolute_error: 18606.2949 - val_loss: 21728.6934 - val_mean_absolute_error: 21728.6934\n",
            "Epoch 183/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 17389.3027 - mean_absolute_error: 17389.3027\n",
            "Epoch 00183: val_loss improved from 21728.69336 to 21549.77539, saving model to Weights-183--21549.77539.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 17678.2070 - mean_absolute_error: 17678.2070 - val_loss: 21549.7754 - val_mean_absolute_error: 21549.7754\n",
            "Epoch 184/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 20374.4902 - mean_absolute_error: 20374.4902\n",
            "Epoch 00184: val_loss did not improve from 21549.77539\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20292.4238 - mean_absolute_error: 20292.4238 - val_loss: 22183.1426 - val_mean_absolute_error: 22183.1426\n",
            "Epoch 185/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 19522.0840 - mean_absolute_error: 19522.0840\n",
            "Epoch 00185: val_loss did not improve from 21549.77539\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 19502.0352 - mean_absolute_error: 19502.0352 - val_loss: 22090.5762 - val_mean_absolute_error: 22090.5762\n",
            "Epoch 186/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 19576.5645 - mean_absolute_error: 19576.5645\n",
            "Epoch 00186: val_loss did not improve from 21549.77539\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19183.7012 - mean_absolute_error: 19183.7012 - val_loss: 23051.2012 - val_mean_absolute_error: 23051.2012\n",
            "Epoch 187/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 19053.2188 - mean_absolute_error: 19053.2188\n",
            "Epoch 00187: val_loss did not improve from 21549.77539\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 18812.1680 - mean_absolute_error: 18812.1680 - val_loss: 22025.4102 - val_mean_absolute_error: 22025.4102\n",
            "Epoch 188/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 17834.7344 - mean_absolute_error: 17834.7344\n",
            "Epoch 00188: val_loss did not improve from 21549.77539\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 17752.6387 - mean_absolute_error: 17752.6387 - val_loss: 21678.8301 - val_mean_absolute_error: 21678.8301\n",
            "Epoch 189/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 19720.5078 - mean_absolute_error: 19720.5078\n",
            "Epoch 00189: val_loss did not improve from 21549.77539\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19814.5059 - mean_absolute_error: 19814.5059 - val_loss: 22962.5020 - val_mean_absolute_error: 22962.5020\n",
            "Epoch 190/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 17436.4746 - mean_absolute_error: 17436.4746\n",
            "Epoch 00190: val_loss did not improve from 21549.77539\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17087.7695 - mean_absolute_error: 17087.7695 - val_loss: 23585.3457 - val_mean_absolute_error: 23585.3457\n",
            "Epoch 191/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 17389.7441 - mean_absolute_error: 17389.7441\n",
            "Epoch 00191: val_loss did not improve from 21549.77539\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 17325.0859 - mean_absolute_error: 17325.0859 - val_loss: 22031.5156 - val_mean_absolute_error: 22031.5156\n",
            "Epoch 192/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 19663.5703 - mean_absolute_error: 19663.5703\n",
            "Epoch 00192: val_loss did not improve from 21549.77539\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19662.4297 - mean_absolute_error: 19662.4297 - val_loss: 22511.6191 - val_mean_absolute_error: 22511.6191\n",
            "Epoch 193/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 17260.9043 - mean_absolute_error: 17260.9043\n",
            "Epoch 00193: val_loss improved from 21549.77539 to 21384.06641, saving model to Weights-193--21384.06641.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 17558.8418 - mean_absolute_error: 17558.8418 - val_loss: 21384.0664 - val_mean_absolute_error: 21384.0664\n",
            "Epoch 194/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 18699.5156 - mean_absolute_error: 18699.5156\n",
            "Epoch 00194: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19064.4902 - mean_absolute_error: 19064.4902 - val_loss: 25885.5410 - val_mean_absolute_error: 25885.5410\n",
            "Epoch 195/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 19001.1582 - mean_absolute_error: 19001.1582\n",
            "Epoch 00195: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 18708.6035 - mean_absolute_error: 18708.6035 - val_loss: 24546.0078 - val_mean_absolute_error: 24546.0078\n",
            "Epoch 196/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 19197.0430 - mean_absolute_error: 19197.0430\n",
            "Epoch 00196: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 18761.3672 - mean_absolute_error: 18761.3672 - val_loss: 23298.5762 - val_mean_absolute_error: 23298.5762\n",
            "Epoch 197/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 18212.4805 - mean_absolute_error: 18212.4805\n",
            "Epoch 00197: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 18122.3730 - mean_absolute_error: 18122.3730 - val_loss: 22701.9375 - val_mean_absolute_error: 22701.9375\n",
            "Epoch 198/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 18127.3516 - mean_absolute_error: 18127.3516\n",
            "Epoch 00198: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 18206.2441 - mean_absolute_error: 18206.2441 - val_loss: 22132.8770 - val_mean_absolute_error: 22132.8770\n",
            "Epoch 199/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 17005.1738 - mean_absolute_error: 17005.1738\n",
            "Epoch 00199: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17487.9902 - mean_absolute_error: 17487.9902 - val_loss: 22509.2266 - val_mean_absolute_error: 22509.2266\n",
            "Epoch 200/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 21318.6738 - mean_absolute_error: 21318.6738\n",
            "Epoch 00200: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 20350.4863 - mean_absolute_error: 20350.4863 - val_loss: 22685.3887 - val_mean_absolute_error: 22685.3887\n",
            "Epoch 201/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 16987.7773 - mean_absolute_error: 16987.7773\n",
            "Epoch 00201: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17052.2637 - mean_absolute_error: 17052.2637 - val_loss: 21586.8340 - val_mean_absolute_error: 21586.8340\n",
            "Epoch 202/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 20227.6387 - mean_absolute_error: 20227.6387\n",
            "Epoch 00202: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20285.1797 - mean_absolute_error: 20285.1797 - val_loss: 26381.9219 - val_mean_absolute_error: 26381.9219\n",
            "Epoch 203/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 18792.3867 - mean_absolute_error: 18792.3867\n",
            "Epoch 00203: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 18710.6348 - mean_absolute_error: 18710.6348 - val_loss: 28518.5312 - val_mean_absolute_error: 28518.5312\n",
            "Epoch 204/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 20496.3535 - mean_absolute_error: 20496.3535\n",
            "Epoch 00204: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 20484.4277 - mean_absolute_error: 20484.4277 - val_loss: 24229.7422 - val_mean_absolute_error: 24229.7422\n",
            "Epoch 205/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 17791.4590 - mean_absolute_error: 17791.4590\n",
            "Epoch 00205: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17707.5898 - mean_absolute_error: 17707.5898 - val_loss: 22555.4766 - val_mean_absolute_error: 22555.4766\n",
            "Epoch 206/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 17332.2715 - mean_absolute_error: 17332.2715\n",
            "Epoch 00206: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17311.2012 - mean_absolute_error: 17311.2012 - val_loss: 21607.0957 - val_mean_absolute_error: 21607.0957\n",
            "Epoch 207/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 17376.1191 - mean_absolute_error: 17376.1191\n",
            "Epoch 00207: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17940.0703 - mean_absolute_error: 17940.0703 - val_loss: 25166.7969 - val_mean_absolute_error: 25166.7969\n",
            "Epoch 208/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 17127.5527 - mean_absolute_error: 17127.5527\n",
            "Epoch 00208: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17370.4648 - mean_absolute_error: 17370.4648 - val_loss: 22340.4922 - val_mean_absolute_error: 22340.4922\n",
            "Epoch 209/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 21157.1855 - mean_absolute_error: 21157.1855\n",
            "Epoch 00209: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 21258.1055 - mean_absolute_error: 21258.1055 - val_loss: 21928.8770 - val_mean_absolute_error: 21928.8770\n",
            "Epoch 210/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 19326.4141 - mean_absolute_error: 19326.4141\n",
            "Epoch 00210: val_loss did not improve from 21384.06641\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 18730.1875 - mean_absolute_error: 18730.1875 - val_loss: 22988.8281 - val_mean_absolute_error: 22988.8281\n",
            "Epoch 211/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 18195.2344 - mean_absolute_error: 18195.2344\n",
            "Epoch 00211: val_loss improved from 21384.06641 to 21350.33984, saving model to Weights-211--21350.33984.hdf5\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 18148.0078 - mean_absolute_error: 18148.0078 - val_loss: 21350.3398 - val_mean_absolute_error: 21350.3398\n",
            "Epoch 212/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 16047.1348 - mean_absolute_error: 16047.1348\n",
            "Epoch 00212: val_loss did not improve from 21350.33984\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16349.8633 - mean_absolute_error: 16349.8633 - val_loss: 22841.5625 - val_mean_absolute_error: 22841.5625\n",
            "Epoch 213/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 17430.2422 - mean_absolute_error: 17430.2422\n",
            "Epoch 00213: val_loss did not improve from 21350.33984\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 17227.7012 - mean_absolute_error: 17227.7012 - val_loss: 25075.5371 - val_mean_absolute_error: 25075.5371\n",
            "Epoch 214/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 19797.5332 - mean_absolute_error: 19797.5332\n",
            "Epoch 00214: val_loss did not improve from 21350.33984\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19864.7441 - mean_absolute_error: 19864.7441 - val_loss: 21600.4238 - val_mean_absolute_error: 21600.4238\n",
            "Epoch 215/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 18124.1465 - mean_absolute_error: 18124.1465\n",
            "Epoch 00215: val_loss did not improve from 21350.33984\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 18102.7559 - mean_absolute_error: 18102.7559 - val_loss: 23719.4648 - val_mean_absolute_error: 23719.4648\n",
            "Epoch 216/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 16935.2793 - mean_absolute_error: 16935.2793\n",
            "Epoch 00216: val_loss did not improve from 21350.33984\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 17002.0488 - mean_absolute_error: 17002.0488 - val_loss: 23993.3359 - val_mean_absolute_error: 23993.3359\n",
            "Epoch 217/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 18140.8750 - mean_absolute_error: 18140.8750\n",
            "Epoch 00217: val_loss improved from 21350.33984 to 21334.43164, saving model to Weights-217--21334.43164.hdf5\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 18168.1406 - mean_absolute_error: 18168.1406 - val_loss: 21334.4316 - val_mean_absolute_error: 21334.4316\n",
            "Epoch 218/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 16983.0664 - mean_absolute_error: 16983.0664\n",
            "Epoch 00218: val_loss did not improve from 21334.43164\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 17128.8828 - mean_absolute_error: 17128.8828 - val_loss: 21367.1426 - val_mean_absolute_error: 21367.1426\n",
            "Epoch 219/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 16746.8555 - mean_absolute_error: 16746.8555\n",
            "Epoch 00219: val_loss did not improve from 21334.43164\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16603.1328 - mean_absolute_error: 16603.1328 - val_loss: 21534.2891 - val_mean_absolute_error: 21534.2891\n",
            "Epoch 220/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 17186.1914 - mean_absolute_error: 17186.1914\n",
            "Epoch 00220: val_loss did not improve from 21334.43164\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17493.5918 - mean_absolute_error: 17493.5918 - val_loss: 24639.4492 - val_mean_absolute_error: 24639.4492\n",
            "Epoch 221/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 17336.3887 - mean_absolute_error: 17336.3887\n",
            "Epoch 00221: val_loss did not improve from 21334.43164\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17415.7168 - mean_absolute_error: 17415.7168 - val_loss: 24375.0625 - val_mean_absolute_error: 24375.0625\n",
            "Epoch 222/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 19254.4980 - mean_absolute_error: 19254.4980\n",
            "Epoch 00222: val_loss did not improve from 21334.43164\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 19016.0918 - mean_absolute_error: 19016.0918 - val_loss: 21869.9141 - val_mean_absolute_error: 21869.9141\n",
            "Epoch 223/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 17171.5410 - mean_absolute_error: 17171.5410\n",
            "Epoch 00223: val_loss did not improve from 21334.43164\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16859.8789 - mean_absolute_error: 16859.8789 - val_loss: 24818.3359 - val_mean_absolute_error: 24818.3359\n",
            "Epoch 224/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 19411.8613 - mean_absolute_error: 19411.8613\n",
            "Epoch 00224: val_loss did not improve from 21334.43164\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 19099.0469 - mean_absolute_error: 19099.0469 - val_loss: 21895.3965 - val_mean_absolute_error: 21895.3965\n",
            "Epoch 225/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 17414.7461 - mean_absolute_error: 17414.7461\n",
            "Epoch 00225: val_loss did not improve from 21334.43164\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17292.2109 - mean_absolute_error: 17292.2109 - val_loss: 21829.2324 - val_mean_absolute_error: 21829.2324\n",
            "Epoch 226/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 17154.1953 - mean_absolute_error: 17154.1953\n",
            "Epoch 00226: val_loss did not improve from 21334.43164\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 17022.7715 - mean_absolute_error: 17022.7715 - val_loss: 22797.9238 - val_mean_absolute_error: 22797.9238\n",
            "Epoch 227/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 16881.1602 - mean_absolute_error: 16881.1602\n",
            "Epoch 00227: val_loss did not improve from 21334.43164\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 16884.6797 - mean_absolute_error: 16884.6797 - val_loss: 21488.3418 - val_mean_absolute_error: 21488.3418\n",
            "Epoch 228/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 17346.2871 - mean_absolute_error: 17346.2871\n",
            "Epoch 00228: val_loss did not improve from 21334.43164\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 17172.2949 - mean_absolute_error: 17172.2949 - val_loss: 22099.0352 - val_mean_absolute_error: 22099.0352\n",
            "Epoch 229/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 16149.6846 - mean_absolute_error: 16149.6846\n",
            "Epoch 00229: val_loss did not improve from 21334.43164\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16263.0049 - mean_absolute_error: 16263.0049 - val_loss: 25365.1719 - val_mean_absolute_error: 25365.1719\n",
            "Epoch 230/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 17941.5820 - mean_absolute_error: 17941.5820\n",
            "Epoch 00230: val_loss improved from 21334.43164 to 20856.78906, saving model to Weights-230--20856.78906.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 17470.4434 - mean_absolute_error: 17470.4434 - val_loss: 20856.7891 - val_mean_absolute_error: 20856.7891\n",
            "Epoch 231/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 15476.3984 - mean_absolute_error: 15476.3984\n",
            "Epoch 00231: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16255.9268 - mean_absolute_error: 16255.9268 - val_loss: 21704.9863 - val_mean_absolute_error: 21704.9863\n",
            "Epoch 232/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 19574.3340 - mean_absolute_error: 19574.3340\n",
            "Epoch 00232: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19839.3516 - mean_absolute_error: 19839.3516 - val_loss: 30217.9141 - val_mean_absolute_error: 30217.9141\n",
            "Epoch 233/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 20840.6465 - mean_absolute_error: 20840.6465\n",
            "Epoch 00233: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 21235.7031 - mean_absolute_error: 21235.7031 - val_loss: 21804.0410 - val_mean_absolute_error: 21804.0410\n",
            "Epoch 234/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 17654.5156 - mean_absolute_error: 17654.5156\n",
            "Epoch 00234: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 17455.1699 - mean_absolute_error: 17455.1699 - val_loss: 21574.5684 - val_mean_absolute_error: 21574.5684\n",
            "Epoch 235/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16808.0605 - mean_absolute_error: 16808.0605\n",
            "Epoch 00235: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16808.0605 - mean_absolute_error: 16808.0605 - val_loss: 21388.6855 - val_mean_absolute_error: 21388.6855\n",
            "Epoch 236/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 16821.6426 - mean_absolute_error: 16821.6426\n",
            "Epoch 00236: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16845.0918 - mean_absolute_error: 16845.0918 - val_loss: 21021.3535 - val_mean_absolute_error: 21021.3535\n",
            "Epoch 237/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 17840.4434 - mean_absolute_error: 17840.4434\n",
            "Epoch 00237: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 18120.6797 - mean_absolute_error: 18120.6797 - val_loss: 21941.9199 - val_mean_absolute_error: 21941.9199\n",
            "Epoch 238/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 17793.1582 - mean_absolute_error: 17793.1582\n",
            "Epoch 00238: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17793.1582 - mean_absolute_error: 17793.1582 - val_loss: 21352.8203 - val_mean_absolute_error: 21352.8203\n",
            "Epoch 239/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 17079.9551 - mean_absolute_error: 17079.9551\n",
            "Epoch 00239: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17372.6523 - mean_absolute_error: 17372.6523 - val_loss: 21258.7637 - val_mean_absolute_error: 21258.7637\n",
            "Epoch 240/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 16956.7168 - mean_absolute_error: 16956.7168\n",
            "Epoch 00240: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16427.3457 - mean_absolute_error: 16427.3457 - val_loss: 21397.9199 - val_mean_absolute_error: 21397.9199\n",
            "Epoch 241/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 16169.9854 - mean_absolute_error: 16169.9854\n",
            "Epoch 00241: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16161.4229 - mean_absolute_error: 16161.4229 - val_loss: 21363.9883 - val_mean_absolute_error: 21363.9883\n",
            "Epoch 242/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 15973.6631 - mean_absolute_error: 15973.6631\n",
            "Epoch 00242: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16586.1816 - mean_absolute_error: 16586.1816 - val_loss: 21859.0488 - val_mean_absolute_error: 21859.0488\n",
            "Epoch 243/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 18830.6270 - mean_absolute_error: 18830.6270\n",
            "Epoch 00243: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 18694.5020 - mean_absolute_error: 18694.5020 - val_loss: 20869.5312 - val_mean_absolute_error: 20869.5312\n",
            "Epoch 244/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 15892.6152 - mean_absolute_error: 15892.6152\n",
            "Epoch 00244: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15977.6201 - mean_absolute_error: 15977.6201 - val_loss: 21831.2910 - val_mean_absolute_error: 21831.2910\n",
            "Epoch 245/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 18687.1074 - mean_absolute_error: 18687.1074\n",
            "Epoch 00245: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 18659.9141 - mean_absolute_error: 18659.9141 - val_loss: 21659.0332 - val_mean_absolute_error: 21659.0332\n",
            "Epoch 246/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 16952.6875 - mean_absolute_error: 16952.6875\n",
            "Epoch 00246: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17221.2598 - mean_absolute_error: 17221.2598 - val_loss: 24636.6152 - val_mean_absolute_error: 24636.6152\n",
            "Epoch 247/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 16309.0869 - mean_absolute_error: 16309.0869\n",
            "Epoch 00247: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16080.0225 - mean_absolute_error: 16080.0225 - val_loss: 22985.3789 - val_mean_absolute_error: 22985.3789\n",
            "Epoch 248/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 16002.2471 - mean_absolute_error: 16002.2471\n",
            "Epoch 00248: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15957.3086 - mean_absolute_error: 15957.3086 - val_loss: 21868.3652 - val_mean_absolute_error: 21868.3652\n",
            "Epoch 249/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 17274.6895 - mean_absolute_error: 17274.6895\n",
            "Epoch 00249: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16962.9805 - mean_absolute_error: 16962.9805 - val_loss: 21224.7422 - val_mean_absolute_error: 21224.7422\n",
            "Epoch 250/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 17410.0508 - mean_absolute_error: 17410.0508\n",
            "Epoch 00250: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17146.3438 - mean_absolute_error: 17146.3438 - val_loss: 21631.3008 - val_mean_absolute_error: 21631.3008\n",
            "Epoch 251/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 16185.9980 - mean_absolute_error: 16185.9980\n",
            "Epoch 00251: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16540.3301 - mean_absolute_error: 16540.3301 - val_loss: 25623.1797 - val_mean_absolute_error: 25623.1797\n",
            "Epoch 252/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 18379.3223 - mean_absolute_error: 18379.3223\n",
            "Epoch 00252: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 18354.6738 - mean_absolute_error: 18354.6738 - val_loss: 21284.0469 - val_mean_absolute_error: 21284.0469\n",
            "Epoch 253/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 16619.5410 - mean_absolute_error: 16619.5410\n",
            "Epoch 00253: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16703.7578 - mean_absolute_error: 16703.7578 - val_loss: 22389.8086 - val_mean_absolute_error: 22389.8086\n",
            "Epoch 254/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 17082.4238 - mean_absolute_error: 17082.4238\n",
            "Epoch 00254: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 17236.9102 - mean_absolute_error: 17236.9102 - val_loss: 21895.0625 - val_mean_absolute_error: 21895.0625\n",
            "Epoch 255/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16765.4902 - mean_absolute_error: 16765.4902\n",
            "Epoch 00255: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16765.4902 - mean_absolute_error: 16765.4902 - val_loss: 21670.7812 - val_mean_absolute_error: 21670.7812\n",
            "Epoch 256/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 17141.5879 - mean_absolute_error: 17141.5879\n",
            "Epoch 00256: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17068.8281 - mean_absolute_error: 17068.8281 - val_loss: 21040.2148 - val_mean_absolute_error: 21040.2148\n",
            "Epoch 257/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 17282.8164 - mean_absolute_error: 17282.8164\n",
            "Epoch 00257: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17769.3516 - mean_absolute_error: 17769.3516 - val_loss: 24791.7148 - val_mean_absolute_error: 24791.7148\n",
            "Epoch 258/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 17038.0000 - mean_absolute_error: 17038.0000\n",
            "Epoch 00258: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16922.5586 - mean_absolute_error: 16922.5586 - val_loss: 21898.5352 - val_mean_absolute_error: 21898.5352\n",
            "Epoch 259/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 16816.9277 - mean_absolute_error: 16816.9277\n",
            "Epoch 00259: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 17204.7402 - mean_absolute_error: 17204.7402 - val_loss: 22295.8027 - val_mean_absolute_error: 22295.8027\n",
            "Epoch 260/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 16597.1074 - mean_absolute_error: 16597.1074\n",
            "Epoch 00260: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 17012.5078 - mean_absolute_error: 17012.5078 - val_loss: 34711.6523 - val_mean_absolute_error: 34711.6523\n",
            "Epoch 261/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 17423.3145 - mean_absolute_error: 17423.3145\n",
            "Epoch 00261: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17415.7324 - mean_absolute_error: 17415.7324 - val_loss: 22161.6133 - val_mean_absolute_error: 22161.6133\n",
            "Epoch 262/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 16175.9785 - mean_absolute_error: 16175.9785\n",
            "Epoch 00262: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16097.1914 - mean_absolute_error: 16097.1914 - val_loss: 23056.5352 - val_mean_absolute_error: 23056.5352\n",
            "Epoch 263/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 17805.5859 - mean_absolute_error: 17805.5859\n",
            "Epoch 00263: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 17832.2441 - mean_absolute_error: 17832.2441 - val_loss: 22400.3457 - val_mean_absolute_error: 22400.3457\n",
            "Epoch 264/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 16746.4434 - mean_absolute_error: 16746.4434\n",
            "Epoch 00264: val_loss did not improve from 20856.78906\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16622.0215 - mean_absolute_error: 16622.0215 - val_loss: 23871.6328 - val_mean_absolute_error: 23871.6328\n",
            "Epoch 265/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 16740.9316 - mean_absolute_error: 16740.9316\n",
            "Epoch 00265: val_loss improved from 20856.78906 to 20727.22266, saving model to Weights-265--20727.22266.hdf5\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 16417.0566 - mean_absolute_error: 16417.0566 - val_loss: 20727.2227 - val_mean_absolute_error: 20727.2227\n",
            "Epoch 266/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 16743.4570 - mean_absolute_error: 16743.4570\n",
            "Epoch 00266: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17091.4102 - mean_absolute_error: 17091.4102 - val_loss: 24963.7871 - val_mean_absolute_error: 24963.7871\n",
            "Epoch 267/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16795.8496 - mean_absolute_error: 16795.8496\n",
            "Epoch 00267: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16795.8496 - mean_absolute_error: 16795.8496 - val_loss: 22208.4551 - val_mean_absolute_error: 22208.4551\n",
            "Epoch 268/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 19475.6270 - mean_absolute_error: 19475.6270\n",
            "Epoch 00268: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 19111.8594 - mean_absolute_error: 19111.8594 - val_loss: 23792.5078 - val_mean_absolute_error: 23792.5078\n",
            "Epoch 269/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 18052.3633 - mean_absolute_error: 18052.3633\n",
            "Epoch 00269: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 18052.3633 - mean_absolute_error: 18052.3633 - val_loss: 23723.8008 - val_mean_absolute_error: 23723.8008\n",
            "Epoch 270/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 16067.7061 - mean_absolute_error: 16067.7061\n",
            "Epoch 00270: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 16214.2139 - mean_absolute_error: 16214.2139 - val_loss: 22637.2480 - val_mean_absolute_error: 22637.2480\n",
            "Epoch 271/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 15886.8037 - mean_absolute_error: 15886.8037\n",
            "Epoch 00271: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15841.1777 - mean_absolute_error: 15841.1777 - val_loss: 21259.2676 - val_mean_absolute_error: 21259.2676\n",
            "Epoch 272/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 16501.1504 - mean_absolute_error: 16501.1504\n",
            "Epoch 00272: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16500.7715 - mean_absolute_error: 16500.7715 - val_loss: 21647.1328 - val_mean_absolute_error: 21647.1328\n",
            "Epoch 273/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 15561.5127 - mean_absolute_error: 15561.5127\n",
            "Epoch 00273: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15959.2822 - mean_absolute_error: 15959.2822 - val_loss: 21673.1543 - val_mean_absolute_error: 21673.1543\n",
            "Epoch 274/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 17052.4883 - mean_absolute_error: 17052.4883\n",
            "Epoch 00274: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16475.1328 - mean_absolute_error: 16475.1328 - val_loss: 22062.1855 - val_mean_absolute_error: 22062.1855\n",
            "Epoch 275/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 15235.6914 - mean_absolute_error: 15235.6914\n",
            "Epoch 00275: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15396.1387 - mean_absolute_error: 15396.1387 - val_loss: 21952.2578 - val_mean_absolute_error: 21952.2578\n",
            "Epoch 276/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 15833.3164 - mean_absolute_error: 15833.3164\n",
            "Epoch 00276: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15896.8711 - mean_absolute_error: 15896.8711 - val_loss: 20966.5859 - val_mean_absolute_error: 20966.5859\n",
            "Epoch 277/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 15868.1182 - mean_absolute_error: 15868.1182\n",
            "Epoch 00277: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15825.3594 - mean_absolute_error: 15825.3594 - val_loss: 21243.7383 - val_mean_absolute_error: 21243.7383\n",
            "Epoch 278/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 17105.4199 - mean_absolute_error: 17105.4199\n",
            "Epoch 00278: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 17120.2168 - mean_absolute_error: 17120.2168 - val_loss: 21538.6445 - val_mean_absolute_error: 21538.6445\n",
            "Epoch 279/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15908.2822 - mean_absolute_error: 15908.2822\n",
            "Epoch 00279: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15908.2822 - mean_absolute_error: 15908.2822 - val_loss: 23289.6582 - val_mean_absolute_error: 23289.6582\n",
            "Epoch 280/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 18138.9746 - mean_absolute_error: 18138.9746\n",
            "Epoch 00280: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 17615.1738 - mean_absolute_error: 17615.1738 - val_loss: 21400.0566 - val_mean_absolute_error: 21400.0566\n",
            "Epoch 281/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 15462.8184 - mean_absolute_error: 15462.8184\n",
            "Epoch 00281: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16009.3545 - mean_absolute_error: 16009.3545 - val_loss: 21696.7812 - val_mean_absolute_error: 21696.7812\n",
            "Epoch 282/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 15880.1250 - mean_absolute_error: 15880.1250\n",
            "Epoch 00282: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15764.6816 - mean_absolute_error: 15764.6816 - val_loss: 20741.2266 - val_mean_absolute_error: 20741.2266\n",
            "Epoch 283/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 15461.3428 - mean_absolute_error: 15461.3428\n",
            "Epoch 00283: val_loss did not improve from 20727.22266\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15949.3154 - mean_absolute_error: 15949.3154 - val_loss: 21850.1387 - val_mean_absolute_error: 21850.1387\n",
            "Epoch 284/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 16287.8584 - mean_absolute_error: 16287.8584\n",
            "Epoch 00284: val_loss improved from 20727.22266 to 20698.39648, saving model to Weights-284--20698.39648.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16518.2168 - mean_absolute_error: 16518.2168 - val_loss: 20698.3965 - val_mean_absolute_error: 20698.3965\n",
            "Epoch 285/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 16462.5645 - mean_absolute_error: 16462.5645\n",
            "Epoch 00285: val_loss did not improve from 20698.39648\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16485.6113 - mean_absolute_error: 16485.6113 - val_loss: 21625.9570 - val_mean_absolute_error: 21625.9570\n",
            "Epoch 286/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 16628.2578 - mean_absolute_error: 16628.2578\n",
            "Epoch 00286: val_loss did not improve from 20698.39648\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16702.0215 - mean_absolute_error: 16702.0215 - val_loss: 21712.4453 - val_mean_absolute_error: 21712.4453\n",
            "Epoch 287/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 15509.3916 - mean_absolute_error: 15509.3916\n",
            "Epoch 00287: val_loss improved from 20698.39648 to 20589.17383, saving model to Weights-287--20589.17383.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15446.3184 - mean_absolute_error: 15446.3184 - val_loss: 20589.1738 - val_mean_absolute_error: 20589.1738\n",
            "Epoch 288/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 16707.6602 - mean_absolute_error: 16707.6602\n",
            "Epoch 00288: val_loss did not improve from 20589.17383\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16913.1230 - mean_absolute_error: 16913.1230 - val_loss: 22809.1465 - val_mean_absolute_error: 22809.1465\n",
            "Epoch 289/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 15989.4785 - mean_absolute_error: 15989.4785\n",
            "Epoch 00289: val_loss did not improve from 20589.17383\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16155.6562 - mean_absolute_error: 16155.6562 - val_loss: 22762.5527 - val_mean_absolute_error: 22762.5527\n",
            "Epoch 290/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 16080.6094 - mean_absolute_error: 16080.6094\n",
            "Epoch 00290: val_loss improved from 20589.17383 to 20253.31836, saving model to Weights-290--20253.31836.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16227.7109 - mean_absolute_error: 16227.7109 - val_loss: 20253.3184 - val_mean_absolute_error: 20253.3184\n",
            "Epoch 291/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 17403.2891 - mean_absolute_error: 17403.2891\n",
            "Epoch 00291: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 17459.0938 - mean_absolute_error: 17459.0938 - val_loss: 21124.7246 - val_mean_absolute_error: 21124.7246\n",
            "Epoch 292/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 16209.3984 - mean_absolute_error: 16209.3984\n",
            "Epoch 00292: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16036.0449 - mean_absolute_error: 16036.0449 - val_loss: 22096.9824 - val_mean_absolute_error: 22096.9824\n",
            "Epoch 293/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 15822.1777 - mean_absolute_error: 15822.1777\n",
            "Epoch 00293: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15605.7793 - mean_absolute_error: 15605.7793 - val_loss: 22096.8887 - val_mean_absolute_error: 22096.8887\n",
            "Epoch 294/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 16436.9570 - mean_absolute_error: 16436.9570\n",
            "Epoch 00294: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16507.3262 - mean_absolute_error: 16507.3262 - val_loss: 21991.9980 - val_mean_absolute_error: 21991.9980\n",
            "Epoch 295/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 16074.9365 - mean_absolute_error: 16074.9365\n",
            "Epoch 00295: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16598.9355 - mean_absolute_error: 16598.9355 - val_loss: 25961.9648 - val_mean_absolute_error: 25961.9648\n",
            "Epoch 296/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 16119.8730 - mean_absolute_error: 16119.8730\n",
            "Epoch 00296: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16277.1270 - mean_absolute_error: 16277.1270 - val_loss: 22392.7168 - val_mean_absolute_error: 22392.7168\n",
            "Epoch 297/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 15742.9248 - mean_absolute_error: 15742.9248\n",
            "Epoch 00297: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15786.3369 - mean_absolute_error: 15786.3369 - val_loss: 21094.3809 - val_mean_absolute_error: 21094.3809\n",
            "Epoch 298/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16056.4248 - mean_absolute_error: 16056.4248\n",
            "Epoch 00298: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16056.4248 - mean_absolute_error: 16056.4248 - val_loss: 26172.6562 - val_mean_absolute_error: 26172.6562\n",
            "Epoch 299/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 16018.7178 - mean_absolute_error: 16018.7178\n",
            "Epoch 00299: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15713.8564 - mean_absolute_error: 15713.8564 - val_loss: 20619.3711 - val_mean_absolute_error: 20619.3711\n",
            "Epoch 300/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 15734.7656 - mean_absolute_error: 15734.7656\n",
            "Epoch 00300: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15602.8477 - mean_absolute_error: 15602.8477 - val_loss: 21231.1035 - val_mean_absolute_error: 21231.1035\n",
            "Epoch 301/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 15447.4492 - mean_absolute_error: 15447.4492\n",
            "Epoch 00301: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15759.2158 - mean_absolute_error: 15759.2158 - val_loss: 21932.1484 - val_mean_absolute_error: 21932.1484\n",
            "Epoch 302/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 15472.8115 - mean_absolute_error: 15472.8115\n",
            "Epoch 00302: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15319.1523 - mean_absolute_error: 15319.1523 - val_loss: 22362.5078 - val_mean_absolute_error: 22362.5078\n",
            "Epoch 303/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15942.8252 - mean_absolute_error: 15942.8252\n",
            "Epoch 00303: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15942.8252 - mean_absolute_error: 15942.8252 - val_loss: 20748.0645 - val_mean_absolute_error: 20748.0645\n",
            "Epoch 304/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 17043.6074 - mean_absolute_error: 17043.6074\n",
            "Epoch 00304: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16737.5137 - mean_absolute_error: 16737.5137 - val_loss: 20915.1875 - val_mean_absolute_error: 20915.1875\n",
            "Epoch 305/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 16096.6953 - mean_absolute_error: 16096.6953\n",
            "Epoch 00305: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 16503.4785 - mean_absolute_error: 16503.4785 - val_loss: 21128.3828 - val_mean_absolute_error: 21128.3828\n",
            "Epoch 306/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 16389.5762 - mean_absolute_error: 16389.5762\n",
            "Epoch 00306: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15785.0771 - mean_absolute_error: 15785.0771 - val_loss: 21379.8633 - val_mean_absolute_error: 21379.8633\n",
            "Epoch 307/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 15541.7666 - mean_absolute_error: 15541.7666\n",
            "Epoch 00307: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15245.5176 - mean_absolute_error: 15245.5176 - val_loss: 21483.8242 - val_mean_absolute_error: 21483.8242\n",
            "Epoch 308/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 15564.1045 - mean_absolute_error: 15564.1045\n",
            "Epoch 00308: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15543.6436 - mean_absolute_error: 15543.6436 - val_loss: 21673.5898 - val_mean_absolute_error: 21673.5898\n",
            "Epoch 309/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 16946.6504 - mean_absolute_error: 16946.6504\n",
            "Epoch 00309: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16613.4648 - mean_absolute_error: 16613.4648 - val_loss: 22715.2188 - val_mean_absolute_error: 22715.2188\n",
            "Epoch 310/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16063.1543 - mean_absolute_error: 16063.1543\n",
            "Epoch 00310: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16063.1543 - mean_absolute_error: 16063.1543 - val_loss: 20829.4023 - val_mean_absolute_error: 20829.4023\n",
            "Epoch 311/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 17444.3887 - mean_absolute_error: 17444.3887\n",
            "Epoch 00311: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 17884.4102 - mean_absolute_error: 17884.4102 - val_loss: 23459.6035 - val_mean_absolute_error: 23459.6035\n",
            "Epoch 312/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 19171.8418 - mean_absolute_error: 19171.8418\n",
            "Epoch 00312: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 19287.5176 - mean_absolute_error: 19287.5176 - val_loss: 20798.9512 - val_mean_absolute_error: 20798.9512\n",
            "Epoch 313/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16766.8594 - mean_absolute_error: 16766.8594\n",
            "Epoch 00313: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16766.8594 - mean_absolute_error: 16766.8594 - val_loss: 20742.2207 - val_mean_absolute_error: 20742.2207\n",
            "Epoch 314/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 16118.3379 - mean_absolute_error: 16118.3379\n",
            "Epoch 00314: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15907.9336 - mean_absolute_error: 15907.9336 - val_loss: 21672.8965 - val_mean_absolute_error: 21672.8965\n",
            "Epoch 315/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 15537.1123 - mean_absolute_error: 15537.1123\n",
            "Epoch 00315: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15374.9658 - mean_absolute_error: 15374.9658 - val_loss: 20935.9238 - val_mean_absolute_error: 20935.9238\n",
            "Epoch 316/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 16530.5176 - mean_absolute_error: 16530.5176\n",
            "Epoch 00316: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16481.9180 - mean_absolute_error: 16481.9180 - val_loss: 21184.5059 - val_mean_absolute_error: 21184.5059\n",
            "Epoch 317/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 14425.2295 - mean_absolute_error: 14425.2295\n",
            "Epoch 00317: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15202.0801 - mean_absolute_error: 15202.0801 - val_loss: 23185.7520 - val_mean_absolute_error: 23185.7520\n",
            "Epoch 318/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 16380.6729 - mean_absolute_error: 16380.6729\n",
            "Epoch 00318: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15913.2109 - mean_absolute_error: 15913.2109 - val_loss: 20834.5703 - val_mean_absolute_error: 20834.5703\n",
            "Epoch 319/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 14401.9062 - mean_absolute_error: 14401.9062\n",
            "Epoch 00319: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15034.7021 - mean_absolute_error: 15034.7021 - val_loss: 25205.4824 - val_mean_absolute_error: 25205.4824\n",
            "Epoch 320/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 18283.3613 - mean_absolute_error: 18283.3613\n",
            "Epoch 00320: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 17931.4824 - mean_absolute_error: 17931.4824 - val_loss: 22779.7891 - val_mean_absolute_error: 22779.7891\n",
            "Epoch 321/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 17358.3125 - mean_absolute_error: 17358.3125\n",
            "Epoch 00321: val_loss did not improve from 20253.31836\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 17288.1133 - mean_absolute_error: 17288.1133 - val_loss: 21584.0293 - val_mean_absolute_error: 21584.0293\n",
            "Epoch 322/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 16345.9062 - mean_absolute_error: 16345.9062\n",
            "Epoch 00322: val_loss improved from 20253.31836 to 20186.66602, saving model to Weights-322--20186.66602.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 17333.7383 - mean_absolute_error: 17333.7383 - val_loss: 20186.6660 - val_mean_absolute_error: 20186.6660\n",
            "Epoch 323/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 16253.5938 - mean_absolute_error: 16253.5938\n",
            "Epoch 00323: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16172.5635 - mean_absolute_error: 16172.5635 - val_loss: 23558.3535 - val_mean_absolute_error: 23558.3535\n",
            "Epoch 324/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 15928.9600 - mean_absolute_error: 15928.9600\n",
            "Epoch 00324: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15828.2656 - mean_absolute_error: 15828.2656 - val_loss: 20836.2988 - val_mean_absolute_error: 20836.2988\n",
            "Epoch 325/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16295.6660 - mean_absolute_error: 16295.6660\n",
            "Epoch 00325: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 16295.6660 - mean_absolute_error: 16295.6660 - val_loss: 20607.4355 - val_mean_absolute_error: 20607.4355\n",
            "Epoch 326/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 16171.2842 - mean_absolute_error: 16171.2842\n",
            "Epoch 00326: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16005.6660 - mean_absolute_error: 16005.6660 - val_loss: 20697.9121 - val_mean_absolute_error: 20697.9121\n",
            "Epoch 327/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 15661.4854 - mean_absolute_error: 15661.4854\n",
            "Epoch 00327: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15726.4844 - mean_absolute_error: 15726.4844 - val_loss: 22141.5645 - val_mean_absolute_error: 22141.5645\n",
            "Epoch 328/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15423.3340 - mean_absolute_error: 15423.3340\n",
            "Epoch 00328: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15423.3340 - mean_absolute_error: 15423.3340 - val_loss: 20978.9434 - val_mean_absolute_error: 20978.9434\n",
            "Epoch 329/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 16286.7002 - mean_absolute_error: 16286.7002\n",
            "Epoch 00329: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15835.4707 - mean_absolute_error: 15835.4707 - val_loss: 21177.4492 - val_mean_absolute_error: 21177.4492\n",
            "Epoch 330/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 14282.0322 - mean_absolute_error: 14282.0322\n",
            "Epoch 00330: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14779.4814 - mean_absolute_error: 14779.4814 - val_loss: 21110.8555 - val_mean_absolute_error: 21110.8555\n",
            "Epoch 331/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 16421.4844 - mean_absolute_error: 16421.4844\n",
            "Epoch 00331: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16221.1631 - mean_absolute_error: 16221.1631 - val_loss: 20690.0645 - val_mean_absolute_error: 20690.0645\n",
            "Epoch 332/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 18517.1738 - mean_absolute_error: 18517.1738\n",
            "Epoch 00332: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 18517.1738 - mean_absolute_error: 18517.1738 - val_loss: 21032.7773 - val_mean_absolute_error: 21032.7773\n",
            "Epoch 333/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 15283.9492 - mean_absolute_error: 15283.9492\n",
            "Epoch 00333: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15331.8955 - mean_absolute_error: 15331.8955 - val_loss: 20420.1797 - val_mean_absolute_error: 20420.1797\n",
            "Epoch 334/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 16300.6729 - mean_absolute_error: 16300.6729\n",
            "Epoch 00334: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15846.6592 - mean_absolute_error: 15846.6592 - val_loss: 20831.4648 - val_mean_absolute_error: 20831.4648\n",
            "Epoch 335/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 15621.4004 - mean_absolute_error: 15621.4004\n",
            "Epoch 00335: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15708.8340 - mean_absolute_error: 15708.8340 - val_loss: 20938.9941 - val_mean_absolute_error: 20938.9941\n",
            "Epoch 336/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 14723.8271 - mean_absolute_error: 14723.8271\n",
            "Epoch 00336: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15270.9092 - mean_absolute_error: 15270.9092 - val_loss: 21939.1934 - val_mean_absolute_error: 21939.1934\n",
            "Epoch 337/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 18162.8242 - mean_absolute_error: 18162.8242\n",
            "Epoch 00337: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 18423.8398 - mean_absolute_error: 18423.8398 - val_loss: 23441.8379 - val_mean_absolute_error: 23441.8379\n",
            "Epoch 338/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 14865.4727 - mean_absolute_error: 14865.4727\n",
            "Epoch 00338: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14963.8750 - mean_absolute_error: 14963.8750 - val_loss: 20705.7910 - val_mean_absolute_error: 20705.7910\n",
            "Epoch 339/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15029.4795 - mean_absolute_error: 15029.4795\n",
            "Epoch 00339: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15029.4795 - mean_absolute_error: 15029.4795 - val_loss: 23370.2793 - val_mean_absolute_error: 23370.2793\n",
            "Epoch 340/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15589.1387 - mean_absolute_error: 15589.1387\n",
            "Epoch 00340: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 15589.1387 - mean_absolute_error: 15589.1387 - val_loss: 21173.0215 - val_mean_absolute_error: 21173.0215\n",
            "Epoch 341/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 14782.0059 - mean_absolute_error: 14782.0059\n",
            "Epoch 00341: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15053.8477 - mean_absolute_error: 15053.8477 - val_loss: 21697.2832 - val_mean_absolute_error: 21697.2832\n",
            "Epoch 342/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 14900.1602 - mean_absolute_error: 14900.1602\n",
            "Epoch 00342: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 15000.9570 - mean_absolute_error: 15000.9570 - val_loss: 22330.2324 - val_mean_absolute_error: 22330.2324\n",
            "Epoch 343/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 15427.3203 - mean_absolute_error: 15427.3203\n",
            "Epoch 00343: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15446.4004 - mean_absolute_error: 15446.4004 - val_loss: 20554.6328 - val_mean_absolute_error: 20554.6328\n",
            "Epoch 344/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 14805.9551 - mean_absolute_error: 14805.9551\n",
            "Epoch 00344: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14880.5977 - mean_absolute_error: 14880.5977 - val_loss: 22620.8359 - val_mean_absolute_error: 22620.8359\n",
            "Epoch 345/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 16936.1504 - mean_absolute_error: 16936.1504\n",
            "Epoch 00345: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16913.0508 - mean_absolute_error: 16913.0508 - val_loss: 25041.3242 - val_mean_absolute_error: 25041.3242\n",
            "Epoch 346/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 17392.0918 - mean_absolute_error: 17392.0918\n",
            "Epoch 00346: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16897.9004 - mean_absolute_error: 16897.9004 - val_loss: 21048.8359 - val_mean_absolute_error: 21048.8359\n",
            "Epoch 347/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 14729.2158 - mean_absolute_error: 14729.2158\n",
            "Epoch 00347: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14735.7842 - mean_absolute_error: 14735.7842 - val_loss: 22005.3066 - val_mean_absolute_error: 22005.3066\n",
            "Epoch 348/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 15906.3760 - mean_absolute_error: 15906.3760\n",
            "Epoch 00348: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 16079.0244 - mean_absolute_error: 16079.0244 - val_loss: 24623.3223 - val_mean_absolute_error: 24623.3223\n",
            "Epoch 349/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 16278.5762 - mean_absolute_error: 16278.5762\n",
            "Epoch 00349: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16402.4316 - mean_absolute_error: 16402.4316 - val_loss: 20843.8926 - val_mean_absolute_error: 20843.8926\n",
            "Epoch 350/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 15506.0234 - mean_absolute_error: 15506.0234\n",
            "Epoch 00350: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15610.8408 - mean_absolute_error: 15610.8408 - val_loss: 21515.3750 - val_mean_absolute_error: 21515.3750\n",
            "Epoch 351/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 17261.0996 - mean_absolute_error: 17261.0996\n",
            "Epoch 00351: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 17114.4648 - mean_absolute_error: 17114.4648 - val_loss: 20616.5684 - val_mean_absolute_error: 20616.5684\n",
            "Epoch 352/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 15275.6025 - mean_absolute_error: 15275.6025\n",
            "Epoch 00352: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15267.8643 - mean_absolute_error: 15267.8643 - val_loss: 22127.6055 - val_mean_absolute_error: 22127.6055\n",
            "Epoch 353/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15871.2979 - mean_absolute_error: 15871.2979\n",
            "Epoch 00353: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15871.2979 - mean_absolute_error: 15871.2979 - val_loss: 20800.9219 - val_mean_absolute_error: 20800.9219\n",
            "Epoch 354/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 15181.8535 - mean_absolute_error: 15181.8535\n",
            "Epoch 00354: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15228.9482 - mean_absolute_error: 15228.9482 - val_loss: 21790.4648 - val_mean_absolute_error: 21790.4648\n",
            "Epoch 355/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 15497.5752 - mean_absolute_error: 15497.5752\n",
            "Epoch 00355: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15490.2275 - mean_absolute_error: 15490.2275 - val_loss: 25754.5391 - val_mean_absolute_error: 25754.5391\n",
            "Epoch 356/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 16164.4639 - mean_absolute_error: 16164.4639\n",
            "Epoch 00356: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16248.6982 - mean_absolute_error: 16248.6982 - val_loss: 22427.8613 - val_mean_absolute_error: 22427.8613\n",
            "Epoch 357/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 15216.1875 - mean_absolute_error: 15216.1875\n",
            "Epoch 00357: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15286.8477 - mean_absolute_error: 15286.8477 - val_loss: 21042.5625 - val_mean_absolute_error: 21042.5625\n",
            "Epoch 358/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 14951.6182 - mean_absolute_error: 14951.6182\n",
            "Epoch 00358: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14919.3887 - mean_absolute_error: 14919.3887 - val_loss: 20937.8477 - val_mean_absolute_error: 20937.8477\n",
            "Epoch 359/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 16760.5488 - mean_absolute_error: 16760.5488\n",
            "Epoch 00359: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16946.6699 - mean_absolute_error: 16946.6699 - val_loss: 23636.7402 - val_mean_absolute_error: 23636.7402\n",
            "Epoch 360/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 15522.8965 - mean_absolute_error: 15522.8965\n",
            "Epoch 00360: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15401.1729 - mean_absolute_error: 15401.1729 - val_loss: 22227.3750 - val_mean_absolute_error: 22227.3750\n",
            "Epoch 361/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 16307.6826 - mean_absolute_error: 16307.6826\n",
            "Epoch 00361: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15829.4346 - mean_absolute_error: 15829.4346 - val_loss: 20909.1738 - val_mean_absolute_error: 20909.1738\n",
            "Epoch 362/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 15072.6816 - mean_absolute_error: 15072.6816\n",
            "Epoch 00362: val_loss did not improve from 20186.66602\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14908.8867 - mean_absolute_error: 14908.8867 - val_loss: 20909.5449 - val_mean_absolute_error: 20909.5449\n",
            "Epoch 363/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 14909.3574 - mean_absolute_error: 14909.3574\n",
            "Epoch 00363: val_loss improved from 20186.66602 to 19928.72461, saving model to Weights-363--19928.72461.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14962.0000 - mean_absolute_error: 14962.0000 - val_loss: 19928.7246 - val_mean_absolute_error: 19928.7246\n",
            "Epoch 364/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 14298.7500 - mean_absolute_error: 14298.7500\n",
            "Epoch 00364: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14458.7568 - mean_absolute_error: 14458.7568 - val_loss: 19955.6035 - val_mean_absolute_error: 19955.6035\n",
            "Epoch 365/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 14865.7578 - mean_absolute_error: 14865.7578\n",
            "Epoch 00365: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14995.8750 - mean_absolute_error: 14995.8750 - val_loss: 20276.7578 - val_mean_absolute_error: 20276.7578\n",
            "Epoch 366/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 14417.9639 - mean_absolute_error: 14417.9639\n",
            "Epoch 00366: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14437.6768 - mean_absolute_error: 14437.6768 - val_loss: 20997.9941 - val_mean_absolute_error: 20997.9941\n",
            "Epoch 367/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 17829.6855 - mean_absolute_error: 17829.6855\n",
            "Epoch 00367: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17829.6855 - mean_absolute_error: 17829.6855 - val_loss: 25121.1055 - val_mean_absolute_error: 25121.1055\n",
            "Epoch 368/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 16364.6846 - mean_absolute_error: 16364.6846\n",
            "Epoch 00368: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15769.4336 - mean_absolute_error: 15769.4336 - val_loss: 22081.3457 - val_mean_absolute_error: 22081.3457\n",
            "Epoch 369/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 16291.9189 - mean_absolute_error: 16291.9189\n",
            "Epoch 00369: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15948.6631 - mean_absolute_error: 15948.6631 - val_loss: 20140.5410 - val_mean_absolute_error: 20140.5410\n",
            "Epoch 370/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 15245.2246 - mean_absolute_error: 15245.2246\n",
            "Epoch 00370: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14896.4131 - mean_absolute_error: 14896.4131 - val_loss: 23154.6094 - val_mean_absolute_error: 23154.6094\n",
            "Epoch 371/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 14915.1875 - mean_absolute_error: 14915.1875\n",
            "Epoch 00371: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14929.6914 - mean_absolute_error: 14929.6914 - val_loss: 21004.2617 - val_mean_absolute_error: 21004.2617\n",
            "Epoch 372/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 15492.0430 - mean_absolute_error: 15492.0430\n",
            "Epoch 00372: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15273.2656 - mean_absolute_error: 15273.2656 - val_loss: 22923.4883 - val_mean_absolute_error: 22923.4883\n",
            "Epoch 373/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 15658.8477 - mean_absolute_error: 15658.8477\n",
            "Epoch 00373: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15347.7998 - mean_absolute_error: 15347.7998 - val_loss: 21018.1152 - val_mean_absolute_error: 21018.1152\n",
            "Epoch 374/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 14736.2637 - mean_absolute_error: 14736.2637\n",
            "Epoch 00374: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14696.6543 - mean_absolute_error: 14696.6543 - val_loss: 20602.0254 - val_mean_absolute_error: 20602.0254\n",
            "Epoch 375/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 15305.8672 - mean_absolute_error: 15305.8672\n",
            "Epoch 00375: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15238.6143 - mean_absolute_error: 15238.6143 - val_loss: 20753.4043 - val_mean_absolute_error: 20753.4043\n",
            "Epoch 376/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 15608.2422 - mean_absolute_error: 15608.2422\n",
            "Epoch 00376: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15739.8594 - mean_absolute_error: 15739.8594 - val_loss: 20652.4902 - val_mean_absolute_error: 20652.4902\n",
            "Epoch 377/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 14256.7041 - mean_absolute_error: 14256.7041\n",
            "Epoch 00377: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15100.8027 - mean_absolute_error: 15100.8027 - val_loss: 24614.9160 - val_mean_absolute_error: 24614.9160\n",
            "Epoch 378/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 16095.4541 - mean_absolute_error: 16095.4541\n",
            "Epoch 00378: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16133.0225 - mean_absolute_error: 16133.0225 - val_loss: 20834.5000 - val_mean_absolute_error: 20834.5000\n",
            "Epoch 379/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 14452.9824 - mean_absolute_error: 14452.9824\n",
            "Epoch 00379: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 14609.7891 - mean_absolute_error: 14609.7891 - val_loss: 20984.4141 - val_mean_absolute_error: 20984.4141\n",
            "Epoch 380/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 14782.5830 - mean_absolute_error: 14782.5830\n",
            "Epoch 00380: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14996.9473 - mean_absolute_error: 14996.9473 - val_loss: 22929.1367 - val_mean_absolute_error: 22929.1367\n",
            "Epoch 381/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 16511.9961 - mean_absolute_error: 16511.9961\n",
            "Epoch 00381: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16365.9209 - mean_absolute_error: 16365.9209 - val_loss: 22335.8770 - val_mean_absolute_error: 22335.8770\n",
            "Epoch 382/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 15892.5400 - mean_absolute_error: 15892.5400\n",
            "Epoch 00382: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15539.1846 - mean_absolute_error: 15539.1846 - val_loss: 21126.9531 - val_mean_absolute_error: 21126.9531\n",
            "Epoch 383/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 15265.6250 - mean_absolute_error: 15265.6250\n",
            "Epoch 00383: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14910.8506 - mean_absolute_error: 14910.8506 - val_loss: 20912.6875 - val_mean_absolute_error: 20912.6875\n",
            "Epoch 384/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 15447.3760 - mean_absolute_error: 15447.3760\n",
            "Epoch 00384: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15328.4814 - mean_absolute_error: 15328.4814 - val_loss: 20398.7168 - val_mean_absolute_error: 20398.7168\n",
            "Epoch 385/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 15636.9766 - mean_absolute_error: 15636.9766\n",
            "Epoch 00385: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15819.5840 - mean_absolute_error: 15819.5840 - val_loss: 20614.8691 - val_mean_absolute_error: 20614.8691\n",
            "Epoch 386/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 14790.9502 - mean_absolute_error: 14790.9502\n",
            "Epoch 00386: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 14790.9502 - mean_absolute_error: 14790.9502 - val_loss: 20598.7207 - val_mean_absolute_error: 20598.7207\n",
            "Epoch 387/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 14525.7070 - mean_absolute_error: 14525.7070\n",
            "Epoch 00387: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 14454.5156 - mean_absolute_error: 14454.5156 - val_loss: 20461.8242 - val_mean_absolute_error: 20461.8242\n",
            "Epoch 388/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15285.2451 - mean_absolute_error: 15285.2451\n",
            "Epoch 00388: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 15285.2451 - mean_absolute_error: 15285.2451 - val_loss: 20151.2930 - val_mean_absolute_error: 20151.2930\n",
            "Epoch 389/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 15449.2812 - mean_absolute_error: 15449.2812\n",
            "Epoch 00389: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15539.5430 - mean_absolute_error: 15539.5430 - val_loss: 21168.5801 - val_mean_absolute_error: 21168.5801\n",
            "Epoch 390/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 15399.7998 - mean_absolute_error: 15399.7998\n",
            "Epoch 00390: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15100.5254 - mean_absolute_error: 15100.5254 - val_loss: 25232.5488 - val_mean_absolute_error: 25232.5488\n",
            "Epoch 391/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 17540.6270 - mean_absolute_error: 17540.6270\n",
            "Epoch 00391: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17540.6270 - mean_absolute_error: 17540.6270 - val_loss: 24800.8438 - val_mean_absolute_error: 24800.8438\n",
            "Epoch 392/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 14841.1455 - mean_absolute_error: 14841.1455\n",
            "Epoch 00392: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 15527.1338 - mean_absolute_error: 15527.1338 - val_loss: 21739.6680 - val_mean_absolute_error: 21739.6680\n",
            "Epoch 393/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16473.0488 - mean_absolute_error: 16473.0488\n",
            "Epoch 00393: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16473.0488 - mean_absolute_error: 16473.0488 - val_loss: 20508.8906 - val_mean_absolute_error: 20508.8906\n",
            "Epoch 394/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 14934.1934 - mean_absolute_error: 14934.1934\n",
            "Epoch 00394: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14814.1729 - mean_absolute_error: 14814.1729 - val_loss: 20513.1152 - val_mean_absolute_error: 20513.1152\n",
            "Epoch 395/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15741.2812 - mean_absolute_error: 15741.2812\n",
            "Epoch 00395: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15741.2812 - mean_absolute_error: 15741.2812 - val_loss: 21307.3145 - val_mean_absolute_error: 21307.3145\n",
            "Epoch 396/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 14937.3105 - mean_absolute_error: 14937.3105\n",
            "Epoch 00396: val_loss did not improve from 19928.72461\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15537.0732 - mean_absolute_error: 15537.0732 - val_loss: 21956.0957 - val_mean_absolute_error: 21956.0957\n",
            "Epoch 397/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 15171.3662 - mean_absolute_error: 15171.3662\n",
            "Epoch 00397: val_loss improved from 19928.72461 to 19909.55664, saving model to Weights-397--19909.55664.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15046.4688 - mean_absolute_error: 15046.4688 - val_loss: 19909.5566 - val_mean_absolute_error: 19909.5566\n",
            "Epoch 398/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 16316.3945 - mean_absolute_error: 16316.3945\n",
            "Epoch 00398: val_loss did not improve from 19909.55664\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16564.2617 - mean_absolute_error: 16564.2617 - val_loss: 20588.0977 - val_mean_absolute_error: 20588.0977\n",
            "Epoch 399/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 16682.7344 - mean_absolute_error: 16682.7344\n",
            "Epoch 00399: val_loss did not improve from 19909.55664\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 16585.8516 - mean_absolute_error: 16585.8516 - val_loss: 21631.7676 - val_mean_absolute_error: 21631.7676\n",
            "Epoch 400/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 16274.4414 - mean_absolute_error: 16274.4414\n",
            "Epoch 00400: val_loss did not improve from 19909.55664\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16731.5078 - mean_absolute_error: 16731.5078 - val_loss: 25662.2617 - val_mean_absolute_error: 25662.2617\n",
            "Epoch 401/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 15515.0420 - mean_absolute_error: 15515.0420\n",
            "Epoch 00401: val_loss did not improve from 19909.55664\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15403.6592 - mean_absolute_error: 15403.6592 - val_loss: 20007.0078 - val_mean_absolute_error: 20007.0078\n",
            "Epoch 402/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 14595.1025 - mean_absolute_error: 14595.1025\n",
            "Epoch 00402: val_loss did not improve from 19909.55664\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 14632.2812 - mean_absolute_error: 14632.2812 - val_loss: 21237.6465 - val_mean_absolute_error: 21237.6465\n",
            "Epoch 403/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 14536.8418 - mean_absolute_error: 14536.8418\n",
            "Epoch 00403: val_loss did not improve from 19909.55664\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14743.1797 - mean_absolute_error: 14743.1797 - val_loss: 24296.2617 - val_mean_absolute_error: 24296.2617\n",
            "Epoch 404/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 15379.2871 - mean_absolute_error: 15379.2871\n",
            "Epoch 00404: val_loss did not improve from 19909.55664\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 15378.8223 - mean_absolute_error: 15378.8223 - val_loss: 24248.2461 - val_mean_absolute_error: 24248.2461\n",
            "Epoch 405/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 14755.2637 - mean_absolute_error: 14755.2637\n",
            "Epoch 00405: val_loss did not improve from 19909.55664\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14841.1904 - mean_absolute_error: 14841.1904 - val_loss: 20393.1855 - val_mean_absolute_error: 20393.1855\n",
            "Epoch 406/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 13881.7100 - mean_absolute_error: 13881.7100\n",
            "Epoch 00406: val_loss did not improve from 19909.55664\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13776.9912 - mean_absolute_error: 13776.9912 - val_loss: 21253.3125 - val_mean_absolute_error: 21253.3125\n",
            "Epoch 407/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 14805.9844 - mean_absolute_error: 14805.9844\n",
            "Epoch 00407: val_loss did not improve from 19909.55664\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14694.5996 - mean_absolute_error: 14694.5996 - val_loss: 20098.2285 - val_mean_absolute_error: 20098.2285\n",
            "Epoch 408/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 15056.5723 - mean_absolute_error: 15056.5723\n",
            "Epoch 00408: val_loss improved from 19909.55664 to 19769.52930, saving model to Weights-408--19769.52930.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15662.2295 - mean_absolute_error: 15662.2295 - val_loss: 19769.5293 - val_mean_absolute_error: 19769.5293\n",
            "Epoch 409/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 16421.6328 - mean_absolute_error: 16421.6328\n",
            "Epoch 00409: val_loss did not improve from 19769.52930\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 16264.6387 - mean_absolute_error: 16264.6387 - val_loss: 20495.5020 - val_mean_absolute_error: 20495.5020\n",
            "Epoch 410/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 14609.7676 - mean_absolute_error: 14609.7676\n",
            "Epoch 00410: val_loss did not improve from 19769.52930\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14517.5869 - mean_absolute_error: 14517.5869 - val_loss: 20483.2285 - val_mean_absolute_error: 20483.2285\n",
            "Epoch 411/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 13778.8271 - mean_absolute_error: 13778.8271\n",
            "Epoch 00411: val_loss did not improve from 19769.52930\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13837.4053 - mean_absolute_error: 13837.4053 - val_loss: 21235.6328 - val_mean_absolute_error: 21235.6328\n",
            "Epoch 412/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 14567.7686 - mean_absolute_error: 14567.7686\n",
            "Epoch 00412: val_loss did not improve from 19769.52930\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14318.7256 - mean_absolute_error: 14318.7256 - val_loss: 20874.7949 - val_mean_absolute_error: 20874.7949\n",
            "Epoch 413/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 16580.0156 - mean_absolute_error: 16580.0156\n",
            "Epoch 00413: val_loss did not improve from 19769.52930\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16156.8252 - mean_absolute_error: 16156.8252 - val_loss: 21732.1777 - val_mean_absolute_error: 21732.1777\n",
            "Epoch 414/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 15899.7588 - mean_absolute_error: 15899.7588\n",
            "Epoch 00414: val_loss did not improve from 19769.52930\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 15914.3389 - mean_absolute_error: 15914.3389 - val_loss: 21149.6367 - val_mean_absolute_error: 21149.6367\n",
            "Epoch 415/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 17579.1973 - mean_absolute_error: 17579.1973\n",
            "Epoch 00415: val_loss did not improve from 19769.52930\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16849.9121 - mean_absolute_error: 16849.9121 - val_loss: 21699.0762 - val_mean_absolute_error: 21699.0762\n",
            "Epoch 416/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 14818.5791 - mean_absolute_error: 14818.5791\n",
            "Epoch 00416: val_loss did not improve from 19769.52930\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14862.1523 - mean_absolute_error: 14862.1523 - val_loss: 21125.6543 - val_mean_absolute_error: 21125.6543\n",
            "Epoch 417/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15784.1162 - mean_absolute_error: 15784.1162\n",
            "Epoch 00417: val_loss did not improve from 19769.52930\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15784.1162 - mean_absolute_error: 15784.1162 - val_loss: 21097.7578 - val_mean_absolute_error: 21097.7578\n",
            "Epoch 418/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 14372.7402 - mean_absolute_error: 14372.7402\n",
            "Epoch 00418: val_loss improved from 19769.52930 to 19579.85938, saving model to Weights-418--19579.85938.hdf5\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 14158.9180 - mean_absolute_error: 14158.9180 - val_loss: 19579.8594 - val_mean_absolute_error: 19579.8594\n",
            "Epoch 419/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 15585.7549 - mean_absolute_error: 15585.7549\n",
            "Epoch 00419: val_loss did not improve from 19579.85938\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15348.8320 - mean_absolute_error: 15348.8320 - val_loss: 23061.6094 - val_mean_absolute_error: 23061.6094\n",
            "Epoch 420/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16331.3633 - mean_absolute_error: 16331.3633\n",
            "Epoch 00420: val_loss did not improve from 19579.85938\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 16331.3633 - mean_absolute_error: 16331.3633 - val_loss: 23749.4453 - val_mean_absolute_error: 23749.4453\n",
            "Epoch 421/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15462.7119 - mean_absolute_error: 15462.7119\n",
            "Epoch 00421: val_loss did not improve from 19579.85938\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15462.7119 - mean_absolute_error: 15462.7119 - val_loss: 20956.3281 - val_mean_absolute_error: 20956.3281\n",
            "Epoch 422/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 14642.7197 - mean_absolute_error: 14642.7197\n",
            "Epoch 00422: val_loss did not improve from 19579.85938\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 14504.4062 - mean_absolute_error: 14504.4062 - val_loss: 20644.4922 - val_mean_absolute_error: 20644.4922\n",
            "Epoch 423/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 15727.1572 - mean_absolute_error: 15727.1572\n",
            "Epoch 00423: val_loss did not improve from 19579.85938\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15927.3525 - mean_absolute_error: 15927.3525 - val_loss: 20957.6875 - val_mean_absolute_error: 20957.6875\n",
            "Epoch 424/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 14186.1621 - mean_absolute_error: 14186.1621\n",
            "Epoch 00424: val_loss did not improve from 19579.85938\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14255.5078 - mean_absolute_error: 14255.5078 - val_loss: 21379.7031 - val_mean_absolute_error: 21379.7031\n",
            "Epoch 425/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 15076.5986 - mean_absolute_error: 15076.5986\n",
            "Epoch 00425: val_loss did not improve from 19579.85938\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 15140.5518 - mean_absolute_error: 15140.5518 - val_loss: 21591.7461 - val_mean_absolute_error: 21591.7461\n",
            "Epoch 426/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 15333.8848 - mean_absolute_error: 15333.8848\n",
            "Epoch 00426: val_loss did not improve from 19579.85938\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15100.6631 - mean_absolute_error: 15100.6631 - val_loss: 20276.4980 - val_mean_absolute_error: 20276.4980\n",
            "Epoch 427/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 13979.8037 - mean_absolute_error: 13979.8037\n",
            "Epoch 00427: val_loss did not improve from 19579.85938\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14506.1211 - mean_absolute_error: 14506.1211 - val_loss: 21119.6504 - val_mean_absolute_error: 21119.6504\n",
            "Epoch 428/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 15204.5420 - mean_absolute_error: 15204.5420\n",
            "Epoch 00428: val_loss did not improve from 19579.85938\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15279.4961 - mean_absolute_error: 15279.4961 - val_loss: 20642.8281 - val_mean_absolute_error: 20642.8281\n",
            "Epoch 429/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 13575.6309 - mean_absolute_error: 13575.6309\n",
            "Epoch 00429: val_loss improved from 19579.85938 to 19565.63867, saving model to Weights-429--19565.63867.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13582.2969 - mean_absolute_error: 13582.2969 - val_loss: 19565.6387 - val_mean_absolute_error: 19565.6387\n",
            "Epoch 430/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 14815.0967 - mean_absolute_error: 14815.0967\n",
            "Epoch 00430: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14989.3428 - mean_absolute_error: 14989.3428 - val_loss: 28826.5273 - val_mean_absolute_error: 28826.5273\n",
            "Epoch 431/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 15473.4941 - mean_absolute_error: 15473.4941\n",
            "Epoch 00431: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15327.8154 - mean_absolute_error: 15327.8154 - val_loss: 20476.4375 - val_mean_absolute_error: 20476.4375\n",
            "Epoch 432/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 14695.2666 - mean_absolute_error: 14695.2666\n",
            "Epoch 00432: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14516.6387 - mean_absolute_error: 14516.6387 - val_loss: 20269.8008 - val_mean_absolute_error: 20269.8008\n",
            "Epoch 433/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 15776.7637 - mean_absolute_error: 15776.7637\n",
            "Epoch 00433: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15049.3857 - mean_absolute_error: 15049.3857 - val_loss: 20945.9492 - val_mean_absolute_error: 20945.9492\n",
            "Epoch 434/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 15043.8418 - mean_absolute_error: 15043.8418\n",
            "Epoch 00434: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14870.4229 - mean_absolute_error: 14870.4229 - val_loss: 21292.3262 - val_mean_absolute_error: 21292.3262\n",
            "Epoch 435/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 13688.8779 - mean_absolute_error: 13688.8779\n",
            "Epoch 00435: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14377.4639 - mean_absolute_error: 14377.4639 - val_loss: 21915.7344 - val_mean_absolute_error: 21915.7344\n",
            "Epoch 436/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 14433.5908 - mean_absolute_error: 14433.5908\n",
            "Epoch 00436: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14433.5908 - mean_absolute_error: 14433.5908 - val_loss: 20130.8398 - val_mean_absolute_error: 20130.8398\n",
            "Epoch 437/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 14161.5947 - mean_absolute_error: 14161.5947\n",
            "Epoch 00437: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 14232.8662 - mean_absolute_error: 14232.8662 - val_loss: 21235.0430 - val_mean_absolute_error: 21235.0430\n",
            "Epoch 438/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 13908.3057 - mean_absolute_error: 13908.3057\n",
            "Epoch 00438: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13615.6338 - mean_absolute_error: 13615.6338 - val_loss: 19975.4414 - val_mean_absolute_error: 19975.4414\n",
            "Epoch 439/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 13828.0586 - mean_absolute_error: 13828.0586\n",
            "Epoch 00439: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14039.3711 - mean_absolute_error: 14039.3711 - val_loss: 20783.0332 - val_mean_absolute_error: 20783.0332\n",
            "Epoch 440/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 13396.1729 - mean_absolute_error: 13396.1729\n",
            "Epoch 00440: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14149.6074 - mean_absolute_error: 14149.6074 - val_loss: 22791.7246 - val_mean_absolute_error: 22791.7246\n",
            "Epoch 441/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 14962.5381 - mean_absolute_error: 14962.5381\n",
            "Epoch 00441: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14962.5381 - mean_absolute_error: 14962.5381 - val_loss: 21343.2266 - val_mean_absolute_error: 21343.2266\n",
            "Epoch 442/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 14566.7812 - mean_absolute_error: 14566.7812\n",
            "Epoch 00442: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 14566.7812 - mean_absolute_error: 14566.7812 - val_loss: 21862.3672 - val_mean_absolute_error: 21862.3672\n",
            "Epoch 443/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 13668.3125 - mean_absolute_error: 13668.3125\n",
            "Epoch 00443: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14061.8066 - mean_absolute_error: 14061.8066 - val_loss: 20303.0234 - val_mean_absolute_error: 20303.0234\n",
            "Epoch 444/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 14464.2168 - mean_absolute_error: 14464.2168\n",
            "Epoch 00444: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14385.5430 - mean_absolute_error: 14385.5430 - val_loss: 20427.1328 - val_mean_absolute_error: 20427.1328\n",
            "Epoch 445/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 15377.7676 - mean_absolute_error: 15377.7676\n",
            "Epoch 00445: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15277.8613 - mean_absolute_error: 15277.8613 - val_loss: 21333.5273 - val_mean_absolute_error: 21333.5273\n",
            "Epoch 446/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 15570.3760 - mean_absolute_error: 15570.3760\n",
            "Epoch 00446: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15227.5322 - mean_absolute_error: 15227.5322 - val_loss: 23141.2031 - val_mean_absolute_error: 23141.2031\n",
            "Epoch 447/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 14528.4980 - mean_absolute_error: 14528.4980\n",
            "Epoch 00447: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14657.4336 - mean_absolute_error: 14657.4336 - val_loss: 20025.8496 - val_mean_absolute_error: 20025.8496\n",
            "Epoch 448/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 14391.9512 - mean_absolute_error: 14391.9512\n",
            "Epoch 00448: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14287.0869 - mean_absolute_error: 14287.0869 - val_loss: 19631.1602 - val_mean_absolute_error: 19631.1602\n",
            "Epoch 449/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 16850.8398 - mean_absolute_error: 16850.8398\n",
            "Epoch 00449: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 17026.1328 - mean_absolute_error: 17026.1328 - val_loss: 21872.2695 - val_mean_absolute_error: 21872.2695\n",
            "Epoch 450/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 15074.7354 - mean_absolute_error: 15074.7354\n",
            "Epoch 00450: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15347.1455 - mean_absolute_error: 15347.1455 - val_loss: 20781.5293 - val_mean_absolute_error: 20781.5293\n",
            "Epoch 451/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 14005.8135 - mean_absolute_error: 14005.8135\n",
            "Epoch 00451: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14005.8135 - mean_absolute_error: 14005.8135 - val_loss: 19917.1699 - val_mean_absolute_error: 19917.1699\n",
            "Epoch 452/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 14404.2275 - mean_absolute_error: 14404.2275\n",
            "Epoch 00452: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14404.2275 - mean_absolute_error: 14404.2275 - val_loss: 19733.0078 - val_mean_absolute_error: 19733.0078\n",
            "Epoch 453/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 14888.7529 - mean_absolute_error: 14888.7529\n",
            "Epoch 00453: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15616.4111 - mean_absolute_error: 15616.4111 - val_loss: 22603.7266 - val_mean_absolute_error: 22603.7266\n",
            "Epoch 454/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 15360.5176 - mean_absolute_error: 15360.5176\n",
            "Epoch 00454: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 15282.3184 - mean_absolute_error: 15282.3184 - val_loss: 20617.1230 - val_mean_absolute_error: 20617.1230\n",
            "Epoch 455/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 13850.0986 - mean_absolute_error: 13850.0986\n",
            "Epoch 00455: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13778.6914 - mean_absolute_error: 13778.6914 - val_loss: 20790.1934 - val_mean_absolute_error: 20790.1934\n",
            "Epoch 456/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 13256.3740 - mean_absolute_error: 13256.3740\n",
            "Epoch 00456: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 13298.2910 - mean_absolute_error: 13298.2910 - val_loss: 19641.7793 - val_mean_absolute_error: 19641.7793\n",
            "Epoch 457/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 13721.5596 - mean_absolute_error: 13721.5596\n",
            "Epoch 00457: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13929.0039 - mean_absolute_error: 13929.0039 - val_loss: 20957.2129 - val_mean_absolute_error: 20957.2129\n",
            "Epoch 458/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 13237.9883 - mean_absolute_error: 13237.9883\n",
            "Epoch 00458: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 13750.6162 - mean_absolute_error: 13750.6162 - val_loss: 22345.6973 - val_mean_absolute_error: 22345.6973\n",
            "Epoch 459/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 14495.5781 - mean_absolute_error: 14495.5781\n",
            "Epoch 00459: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14403.5088 - mean_absolute_error: 14403.5088 - val_loss: 20294.7773 - val_mean_absolute_error: 20294.7773\n",
            "Epoch 460/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 14954.9355 - mean_absolute_error: 14954.9355\n",
            "Epoch 00460: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15281.3359 - mean_absolute_error: 15281.3359 - val_loss: 21983.7988 - val_mean_absolute_error: 21983.7988\n",
            "Epoch 461/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 14969.4150 - mean_absolute_error: 14969.4150\n",
            "Epoch 00461: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14801.2041 - mean_absolute_error: 14801.2041 - val_loss: 20117.4453 - val_mean_absolute_error: 20117.4453\n",
            "Epoch 462/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 13305.5459 - mean_absolute_error: 13305.5459\n",
            "Epoch 00462: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13384.5049 - mean_absolute_error: 13384.5049 - val_loss: 21444.8203 - val_mean_absolute_error: 21444.8203\n",
            "Epoch 463/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 14805.0352 - mean_absolute_error: 14805.0352\n",
            "Epoch 00463: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 14842.1270 - mean_absolute_error: 14842.1270 - val_loss: 20249.2930 - val_mean_absolute_error: 20249.2930\n",
            "Epoch 464/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 14545.3330 - mean_absolute_error: 14545.3330\n",
            "Epoch 00464: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 14749.5732 - mean_absolute_error: 14749.5732 - val_loss: 20293.7832 - val_mean_absolute_error: 20293.7832\n",
            "Epoch 465/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 14015.5947 - mean_absolute_error: 14015.5947\n",
            "Epoch 00465: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13921.6729 - mean_absolute_error: 13921.6729 - val_loss: 20389.0664 - val_mean_absolute_error: 20389.0664\n",
            "Epoch 466/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 15422.1943 - mean_absolute_error: 15422.1943\n",
            "Epoch 00466: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15423.4980 - mean_absolute_error: 15423.4980 - val_loss: 20503.4746 - val_mean_absolute_error: 20503.4746\n",
            "Epoch 467/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 15547.2725 - mean_absolute_error: 15547.2725\n",
            "Epoch 00467: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15543.5361 - mean_absolute_error: 15543.5361 - val_loss: 21381.8770 - val_mean_absolute_error: 21381.8770\n",
            "Epoch 468/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 15225.3672 - mean_absolute_error: 15225.3672\n",
            "Epoch 00468: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15065.5225 - mean_absolute_error: 15065.5225 - val_loss: 20142.7168 - val_mean_absolute_error: 20142.7168\n",
            "Epoch 469/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 17071.3613 - mean_absolute_error: 17071.3613\n",
            "Epoch 00469: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 17091.6016 - mean_absolute_error: 17091.6016 - val_loss: 22655.8789 - val_mean_absolute_error: 22655.8789\n",
            "Epoch 470/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 15817.3232 - mean_absolute_error: 15817.3232\n",
            "Epoch 00470: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15761.5254 - mean_absolute_error: 15761.5254 - val_loss: 20224.5820 - val_mean_absolute_error: 20224.5820\n",
            "Epoch 471/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 14274.4072 - mean_absolute_error: 14274.4072\n",
            "Epoch 00471: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14356.2393 - mean_absolute_error: 14356.2393 - val_loss: 19803.4219 - val_mean_absolute_error: 19803.4219\n",
            "Epoch 472/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 13791.4883 - mean_absolute_error: 13791.4883\n",
            "Epoch 00472: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13784.4033 - mean_absolute_error: 13784.4033 - val_loss: 20193.7773 - val_mean_absolute_error: 20193.7773\n",
            "Epoch 473/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 14283.5762 - mean_absolute_error: 14283.5762\n",
            "Epoch 00473: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 13987.1357 - mean_absolute_error: 13987.1357 - val_loss: 19680.8105 - val_mean_absolute_error: 19680.8105\n",
            "Epoch 474/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 13654.2344 - mean_absolute_error: 13654.2344\n",
            "Epoch 00474: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 13967.2334 - mean_absolute_error: 13967.2334 - val_loss: 20313.1914 - val_mean_absolute_error: 20313.1914\n",
            "Epoch 475/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 13919.0645 - mean_absolute_error: 13919.0645\n",
            "Epoch 00475: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13967.0039 - mean_absolute_error: 13967.0039 - val_loss: 23074.9707 - val_mean_absolute_error: 23074.9707\n",
            "Epoch 476/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 14876.4258 - mean_absolute_error: 14876.4258\n",
            "Epoch 00476: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15050.4023 - mean_absolute_error: 15050.4023 - val_loss: 20257.8594 - val_mean_absolute_error: 20257.8594\n",
            "Epoch 477/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 17763.4316 - mean_absolute_error: 17763.4316\n",
            "Epoch 00477: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 17649.2695 - mean_absolute_error: 17649.2695 - val_loss: 22991.4746 - val_mean_absolute_error: 22991.4746\n",
            "Epoch 478/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 15013.6768 - mean_absolute_error: 15013.6768\n",
            "Epoch 00478: val_loss did not improve from 19565.63867\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 15028.2705 - mean_absolute_error: 15028.2705 - val_loss: 19902.3691 - val_mean_absolute_error: 19902.3691\n",
            "Epoch 479/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 14057.7832 - mean_absolute_error: 14057.7832\n",
            "Epoch 00479: val_loss improved from 19565.63867 to 19560.40820, saving model to Weights-479--19560.40820.hdf5\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13729.9580 - mean_absolute_error: 13729.9580 - val_loss: 19560.4082 - val_mean_absolute_error: 19560.4082\n",
            "Epoch 480/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 13728.2891 - mean_absolute_error: 13728.2891\n",
            "Epoch 00480: val_loss did not improve from 19560.40820\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13627.0371 - mean_absolute_error: 13627.0371 - val_loss: 19658.8398 - val_mean_absolute_error: 19658.8398\n",
            "Epoch 481/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 13069.4668 - mean_absolute_error: 13069.4668\n",
            "Epoch 00481: val_loss improved from 19560.40820 to 19447.34180, saving model to Weights-481--19447.34180.hdf5\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 13443.7168 - mean_absolute_error: 13443.7168 - val_loss: 19447.3418 - val_mean_absolute_error: 19447.3418\n",
            "Epoch 482/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 14571.9795 - mean_absolute_error: 14571.9795\n",
            "Epoch 00482: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14476.7910 - mean_absolute_error: 14476.7910 - val_loss: 20315.0059 - val_mean_absolute_error: 20315.0059\n",
            "Epoch 483/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 13663.4229 - mean_absolute_error: 13663.4229\n",
            "Epoch 00483: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13705.5020 - mean_absolute_error: 13705.5020 - val_loss: 19488.7285 - val_mean_absolute_error: 19488.7285\n",
            "Epoch 484/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 13393.9746 - mean_absolute_error: 13393.9746\n",
            "Epoch 00484: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 13393.9746 - mean_absolute_error: 13393.9746 - val_loss: 20593.2285 - val_mean_absolute_error: 20593.2285\n",
            "Epoch 485/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 13657.8594 - mean_absolute_error: 13657.8594\n",
            "Epoch 00485: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 13624.1777 - mean_absolute_error: 13624.1777 - val_loss: 20509.8125 - val_mean_absolute_error: 20509.8125\n",
            "Epoch 486/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 14002.1289 - mean_absolute_error: 14002.1289\n",
            "Epoch 00486: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14405.3457 - mean_absolute_error: 14405.3457 - val_loss: 20073.2500 - val_mean_absolute_error: 20073.2500\n",
            "Epoch 487/500\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 15714.2432 - mean_absolute_error: 15714.2432\n",
            "Epoch 00487: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15964.2178 - mean_absolute_error: 15964.2178 - val_loss: 22722.6777 - val_mean_absolute_error: 22722.6777\n",
            "Epoch 488/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 14987.1211 - mean_absolute_error: 14987.1211\n",
            "Epoch 00488: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14957.4336 - mean_absolute_error: 14957.4336 - val_loss: 20343.1953 - val_mean_absolute_error: 20343.1953\n",
            "Epoch 489/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 13876.8379 - mean_absolute_error: 13876.8379\n",
            "Epoch 00489: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 13876.8379 - mean_absolute_error: 13876.8379 - val_loss: 20225.2109 - val_mean_absolute_error: 20225.2109\n",
            "Epoch 490/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 14895.9658 - mean_absolute_error: 14895.9658\n",
            "Epoch 00490: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14895.9658 - mean_absolute_error: 14895.9658 - val_loss: 20919.2305 - val_mean_absolute_error: 20919.2305\n",
            "Epoch 491/500\n",
            "28/37 [=====================>........] - ETA: 0s - loss: 13465.1719 - mean_absolute_error: 13465.1719\n",
            "Epoch 00491: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13906.4873 - mean_absolute_error: 13906.4873 - val_loss: 19542.8691 - val_mean_absolute_error: 19542.8691\n",
            "Epoch 492/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 14854.4648 - mean_absolute_error: 14854.4648\n",
            "Epoch 00492: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 14801.5205 - mean_absolute_error: 14801.5205 - val_loss: 22442.8867 - val_mean_absolute_error: 22442.8867\n",
            "Epoch 493/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 15515.5293 - mean_absolute_error: 15515.5293\n",
            "Epoch 00493: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15296.1299 - mean_absolute_error: 15296.1299 - val_loss: 21151.6934 - val_mean_absolute_error: 21151.6934\n",
            "Epoch 494/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 14570.2842 - mean_absolute_error: 14570.2842\n",
            "Epoch 00494: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 14360.6914 - mean_absolute_error: 14360.6914 - val_loss: 19645.7734 - val_mean_absolute_error: 19645.7734\n",
            "Epoch 495/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 14736.2549 - mean_absolute_error: 14736.2549\n",
            "Epoch 00495: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 14736.2549 - mean_absolute_error: 14736.2549 - val_loss: 22549.9004 - val_mean_absolute_error: 22549.9004\n",
            "Epoch 496/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 14905.5625 - mean_absolute_error: 14905.5625\n",
            "Epoch 00496: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 14890.4336 - mean_absolute_error: 14890.4336 - val_loss: 20559.6816 - val_mean_absolute_error: 20559.6816\n",
            "Epoch 497/500\n",
            "29/37 [======================>.......] - ETA: 0s - loss: 15257.4609 - mean_absolute_error: 15257.4609\n",
            "Epoch 00497: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 15180.2451 - mean_absolute_error: 15180.2451 - val_loss: 19989.7148 - val_mean_absolute_error: 19989.7148\n",
            "Epoch 498/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 13420.3916 - mean_absolute_error: 13420.3916\n",
            "Epoch 00498: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13420.3916 - mean_absolute_error: 13420.3916 - val_loss: 19885.0918 - val_mean_absolute_error: 19885.0918\n",
            "Epoch 499/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 13835.7012 - mean_absolute_error: 13835.7012\n",
            "Epoch 00499: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 13691.6074 - mean_absolute_error: 13691.6074 - val_loss: 20294.7324 - val_mean_absolute_error: 20294.7324\n",
            "Epoch 500/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 13725.7637 - mean_absolute_error: 13725.7637\n",
            "Epoch 00500: val_loss did not improve from 19447.34180\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 13725.7637 - mean_absolute_error: 13725.7637 - val_loss: 20231.8535 - val_mean_absolute_error: 20231.8535\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa5c09714d0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "_7waTYZD9QDL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "a9265f7c-cf6c-441a-e017-4d1c8c33205c"
      },
      "cell_type": "code",
      "source": [
        "# Load wights file of the best model:\n",
        "wights_file = 'Weights-478--18738.19831.hdf5' # choose the best checkpoint \n",
        "NN_model.load_weights(wights_file) # load it\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-ad69f3f4137c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load wights file of the best model:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwights_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Weights-478--18738.19831.hdf5'\u001b[0m \u001b[0;31m# choose the best checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwights_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# load it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_absolute_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[1;32m    425\u001b[0m                                fapl, fcpl=make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[1;32m    426\u001b[0m                                fs_persist=fs_persist, fs_threshold=fs_threshold),\n\u001b[0;32m--> 427\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'Weights-478--18738.19831.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "2ecLK0Zp7WlE"
      },
      "cell_type": "markdown",
      "source": [
        "We see that the validation loss of the best model is 18738.19 "
      ]
    },
    {
      "metadata": {
        "id": "k5Y42dnt8evM"
      },
      "cell_type": "markdown",
      "source": [
        "## Fourth : Test the model\n",
        "We will submit the predictions on the test data to Kaggle and see how good our model is."
      ]
    },
    {
      "metadata": {
        "id": "nn71O0NRK_w-"
      },
      "cell_type": "code",
      "source": [
        "def make_submission(prediction, sub_name):\n",
        "  my_submission = pd.DataFrame({'Id':pd.read_csv('test.csv').Id,'SalePrice':prediction})\n",
        "  my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n",
        "  print('A submission file has been made')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g_XboCvj9Js1"
      },
      "cell_type": "code",
      "source": [
        "predictions = NN_model.predict(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hRUZk-Ir-_dG"
      },
      "cell_type": "code",
      "source": [
        "make_submission(predictions[:,0],'submission(NN).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TBLw2KoPAq3k"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://cdn-images-1.medium.com/max/800/1*mXbUrGB9yB9RrBscwugP9g.png)"
      ]
    },
    {
      "metadata": {
        "id": "L_LWqRyKEEFo"
      },
      "cell_type": "markdown",
      "source": [
        "Not bad at all, with some more preprocessing, and more training, we can do better.\n"
      ]
    },
    {
      "metadata": {
        "id": "vojrVZ9LEkgc"
      },
      "cell_type": "markdown",
      "source": [
        "## Fifth: Try another ML algorithms :\n",
        "Now, let us try another ML algorithm to compare the results.\n",
        "\n",
        "We will use random forest regressor and XGBRegressor."
      ]
    },
    {
      "metadata": {
        "id": "GQwnJ4d-b1WK"
      },
      "cell_type": "markdown",
      "source": [
        "**Split training data to training and validation data**"
      ]
    },
    {
      "metadata": {
        "id": "uIKPQXCbE4fY"
      },
      "cell_type": "code",
      "source": [
        "train_X, val_X, train_y, val_y = train_test_split(train, target, test_size = 0.25, random_state = 14)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6vHJ0gzZb_ri"
      },
      "cell_type": "markdown",
      "source": [
        "**We will try Random forest model first.**"
      ]
    },
    {
      "metadata": {
        "id": "QoegrtWbGGM4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "9c019a28-697c-43fb-a59f-b35269f09d45"
      },
      "cell_type": "code",
      "source": [
        "model = RandomForestRegressor()\n",
        "model.fit(train_X,train_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
              "           max_features='auto', max_leaf_nodes=None,\n",
              "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "           min_samples_leaf=1, min_samples_split=2,\n",
              "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
              "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "id": "v5lsFUEPcGft"
      },
      "cell_type": "markdown",
      "source": [
        "**Get the mean absolute error on the validation data **"
      ]
    },
    {
      "metadata": {
        "id": "DovxifwFFetV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "124d483e-b70f-4d38-da00-08639f9adf2c"
      },
      "cell_type": "code",
      "source": [
        "predicted_prices = model.predict(val_X)\n",
        "MAE = mean_absolute_error(val_y , predicted_prices)\n",
        "print('Random forest validation MAE = ', MAE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random forest validation MAE =  19089.71589041096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d-nsd2pGcUZ-"
      },
      "cell_type": "markdown",
      "source": [
        "**Make a submission file and submit it to Kaggle to see the result.**"
      ]
    },
    {
      "metadata": {
        "id": "Y3IFk9GWHP-B"
      },
      "cell_type": "code",
      "source": [
        "predicted_prices = model.predict(test)\n",
        "make_submission(predicted_prices,'Submission(RF).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rUoyvxR6bx84"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://cdn-images-1.medium.com/max/800/1*wcJqKzBsbLARqMjwvv0Xjw.png)"
      ]
    },
    {
      "metadata": {
        "id": "FgZlYnjWcgqk"
      },
      "cell_type": "markdown",
      "source": [
        "**Now, let us try XGBoost model**"
      ]
    },
    {
      "metadata": {
        "id": "CZi76ZV1GKLV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "ffe0caa1-26d5-4907-b4d7-9c1a45911f35"
      },
      "cell_type": "code",
      "source": [
        "XGBModel = XGBRegressor()\n",
        "XGBModel.fit(train_X,train_y , verbose=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
              "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
              "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
              "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "       silent=True, subsample=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "metadata": {
        "id": "xSIqN9Pdc0Hr"
      },
      "cell_type": "markdown",
      "source": [
        "**Get the mean absolute error on the validation data**"
      ]
    },
    {
      "metadata": {
        "id": "DuL_T225GboJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "808e8dea-9c66-4727-ad5b-c76c3a6ebf4f"
      },
      "cell_type": "code",
      "source": [
        "XGBpredictions = XGBModel.predict(val_X)\n",
        "MAE = mean_absolute_error(val_y , XGBpredictions)\n",
        "print('XGBoost validation MAE = ',MAE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGBoost validation MAE =  17869.75410958904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XMh7f_2pc9rf"
      },
      "cell_type": "markdown",
      "source": [
        "**Make a submission file and submit it to Kaggle to see the result.**"
      ]
    },
    {
      "metadata": {
        "id": "ipV7tGfOMaGI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c012016b-c953-4850-f3c0-5376814154af"
      },
      "cell_type": "code",
      "source": [
        "XGBpredictions = XGBModel.predict(test)\n",
        "make_submission(XGBpredictions,'Submission(XGB).csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A submission file has been made\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4Q2SPfmZbvcz"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://cdn-images-1.medium.com/max/800/1*PO0jxykz1hv-aSN5kkjItg.png)"
      ]
    },
    {
      "metadata": {
        "id": "rysvLk6Yc_wW"
      },
      "cell_type": "markdown",
      "source": [
        "Isn't that a surprise, I really did not think that neural networks will beat random forests and XGBoost algorithms, but let us try not to be too optimistic, remember that we did not configure any hyperparameters on random forest and XGBoost models, I believe if we did so, these two models would outscore neural networks."
      ]
    }
  ]
}